{
  "hash": "5f0742f0ac68acb5f804b58ce223d7df",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multivariable Linear Regression\"\nformat:\n  revealjs:\n    scrollable: true\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: notes/chalkboard_1a.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\neditor: source\n---\n\n\n\n\n\n\n\n\n# Modeling Outcomes\n\n## Explaining Variation\n\n::: fragment\nThis is the process where we try to reduce the variation with the use of other variables.\n:::\n\n::: fragment\nCan be thought of as getting it less wrong when taking an educated guess.\n:::\n\n## Taylor Swift's Songs Danceability\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(taylor_album_songs, aes(danceability)) +\n  geom_density()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 3 rows containing non-finite outside the scale range (`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](6_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Danceability by `mode_name`\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntaylor_album_songs |> \n  drop_na(mode_name) |> \n  ggplot(aes(danceability)) +\n  geom_density() +\n  facet_wrap(~ mode_name)\n```\n\n::: {.cell-output-display}\n![](6_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Danceability by Valence\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(taylor_album_songs, aes(valence, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 3 rows containing non-finite outside the scale range (`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 3 rows containing missing values or values outside the scale range (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](6_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Danceability by Energy\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(taylor_album_songs, aes(energy, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 3 rows containing non-finite outside the scale range (`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 3 rows containing missing values or values outside the scale range (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](6_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Modelling Danceability\n\nHow do we use all the variables to explain danceability?\n\n# Multivariable Linear Regression\n\n## MLR\n\nMultivariable Linear Regression (MLR) is used to model an outcome variable ($Y$) by multiple predictor variables ($X_1, X_2, \\ldots, X_p$).\n\n\n::: fragment\nUsing MLR, you propose that the ouctome variable was constructed from a set of predictors, with their corresponding regression coefficients ($\\beta$), and a bit of error\n:::\n\n\n::: fragment\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n$$\n\n$$\n\\varepsilon \\sim DGP\n$$\n:::\n\n## Model Data\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n$$\n\n$$\n\\varepsilon \\sim DGP\n$$\n\n::: fragment\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n$$\n\n$$\n\\varepsilon_i \\sim DGP\n$$\n\n:::\n\n## Unknown Parameters\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n$$\n\n::: fragment\n$$\n\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p\n$$\n:::\n\n## Estimated Model\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n$$\n\n$$\n\\hat Y_i = \\hat\\beta_0 + \\hat\\beta_1 X_{i1} + \\hat\\beta_2 X_{i2} + \\cdots + \\hat\\beta_p X_{ip} \n$$\n\n## Estimating Prameters\n\n$\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p$ are estimated by minimizing the following function:\n\n\n$$\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n$$\n\n\n## Fitting a Model in R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlm(Y ~ X1 + X2 + ... + Xp, data = DATA)\n```\n:::\n\n\n\n\n\n\n## Modelling Danceability\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = danceability ~ mode_name + valence + energy, data = taylor_album_songs)\n#> \n#> Coefficients:\n#>    (Intercept)  mode_nameminor         valence          energy  \n#>        0.53753         0.07322         0.17872        -0.06349\n```\n\n\n:::\n:::\n\n\n\n\n\n## Estimated Model\n\n$$\ndance = 0.54 + 0.07\\times Minor + 0.18 \\times valence - 0.06 \\times energy \n$$\n\n## $\\hat \\beta_i$ Representation\n\nEach regression coefficient $\\beta_i$ represents how the predictor variable changes the outcome, as it increase by 1 unit.\n\n::: fragment\nFor categorical dummy variables, the $\\beta_i$ represents how the outcome will change when the data point belongs to that value.\n:::\n\n\n## $\\hat \\beta_i$ Interpretation\n\nFor $hat \\beta_i$, which is the regression coefficient (slope) of $X_i$:\n\nAs $X_i$ increases by 1 unit, the outcome ($Y$) will increase/decrease by $\\hat \\beta_i$ units, adjusting for all other predictor variables.\n\n::: fragment\nFor categorical dummy variables $X_i$:\n\nThe outcome $Y_i$ increases/decreases by $\\beta_i$ units for category $X_i$ compared to the reference category, adjusting for all other predictor variables.\n\n:::\n\n\n## Intepreting Minor coefficient\n\n$$\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy \n$$\nMinor song's average danceability score is 0.07 units higher compared to Major song's, adjusting for valence and energy.\n\n## Intepreting valence coefficient\n\n$$\ndance = 0.54 + 0.07Minor + 0.18  valence - 0.06 energy \n$$\n\nAs valence increases by 1 unit, danceability increases by an average of 0.18 units, adjusting for energy and type of song.\n\n\n## Intepreting energy coefficient\n\n$$\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy \n$$\n\nAs energy increases by 1 unit, danceability decreases by an average of 0.06 units, adjusting for valence and type of song.\n\n\n# Adjusted $R^2$\n\n## $R^2$\n\nComputing $R^2$ is done by determining how much the variation in the outcome is explained by model and divided by the variation of the outcome.\n\n$$\nR^2 = \\frac{\\text{variation explained by model}}{\\text{variation from outcome}} \\\\\n= 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\n$$\n\n## $R^2$\n\nProblems arise when multiple predictors are added to the model. As a new predictor is added to the model, new information is added to the model which will always reduce the variation in the residuals. Therefore, the $R^2$ will always increase.\n\n## Problems with $R^2$ in MLR\n\nWhen the number of variables increase, the regular $RÂ²$ will be biased in its prediction capability when new data is obtained.\n\n::: fragment\nTherefore, statisticians uses the adjusted $R^2$, that penalizes the model when more variables are added. This ensures that a variable added will have a significant effect in predicting outcomes.\n:::\n\n## Adjusted $R^2$\n\n$$\nR_a^2 = 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\\times \\frac{n-1}{n-k-1}\n$$\n\n- $n$: Number of data points\n- $k$: Number of predictor variables in the model\n\n## Adjusted $R^2$ in R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nxlm <- lm(Y ~ X1 + X2 + ... + Xp, data = DATA)\nar2(xlm)\n```\n:::\n\n\n\n\n\n\n## Example\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nxlm <- lm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\nar2(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.08106907\n```\n\n\n:::\n:::\n\n\n\n\n\n8.1% of the variation in danceability is explained by the model.\n\n# Model Selection\n\n## Model Selection\n\nModel Selection is the process of obtaining a \"final\" model containing all the necessary predictors, and eliminating any that are not necessary.\n\n## Forward Selection\n\nBegin with the null model ($Y\\sim 1$) and add variables until a final model is chosen.\n\n## Backward Selection\n\nBegin with the full model, and remove variable until the final model is chosen.\n\n## Hybrid Selection\n\nA hybrid approach between the forward and backward building approach.\n\n## About Model Selection\n\nGenerally, it is not a good idea to conduct model selection. The predictor variables in your model should be guided by a literature review that illustrates important predictor variables in a model.\n",
    "supporting": [
      "6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}