[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Spring 2025\nInstructor: Isaac Quintanilla Salinas\nEmail: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours: TBD\nOr by Zoom appointment.\nLecture: TBD\n\n\n\nCritical reasoning using a quantitative and statistical, problem-solving approach to solving real-world problems. Topics include: probability and statistics, sample data, probability and empirical data distributions, sampling techniques, estimation and hypothesis testing, ANOVA, and correlation and regression analysis. Students will use standard statistical software to analyze real-world and simulated data. GenEd: B4\n\n\n\n\nPrepare students for advanced courses in data-management and statistics\nReason both inductively and deductively with quantitative information and data\nUse statistical software for complex statistical analysis of real-world and simulated data\nEmpower students to apply computational and inferential thinking to address real-world problems\nWrite the results of a statistical study and draw conclusion in reports\n\n\n\n\nIntroduction to Modern Statistics\n\n\n\nTBD\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nParticipation\n5%\n\n\nReading Assignments\n10%\n\n\nVideo Assignments\n10%\n\n\nHomework\n15%\n\n\nNotebook Assignments\n15%\n\n\nExam 1\n15%\n\n\nExam 2\n15%\n\n\nExam 3\n15%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nParticipation is based on short writing assignments conducted in class. There will be no make ups for these writing prompts.\n\n\n\nNotebook assignments are designed to expand your statistical knowledge. These will be completed in Google Colab which can be accessed from Canvas. There is one notebook assignments every week that you can be completed during class time. The notebook assignment will be due on Wednesday at 11:59 PM every week.\n\n\n\nReading assignments are designed to teach you different statistical concepts and R Programming. As the course progresses, many of the concepts build on each other. Therefore, the assignments encourage you to read each chapter in an appropriate amount of time. You must read the chapter and answer the questions by the Sunday night at 11:59 PM. The 3 lowest reading assignments will be dropped.\n\n\n\nVideos are used to teach statistical concepts related to the course. Students are expected to watch at least one video a week. The videos are implemented using PlayPosit, where questions are embedded in the video to check for understanding. The videos may require students to program and/or read research articles.\n\n\n\nThere will be three in exams. Exam #1 will be on Sept 30, Exam #2 will be on Nov 13, and Exam #3 will be on Dec 11 at 8-10 AM (Sec 01) or Dec 9 1-3 PM (Sec 02). While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by your median average exam grade. This course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the University’s academic integrity policy.\n\n\n\nThere will be 3 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date. Information on the extra credit can be found here.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nReadings\nNB Due\nVideo Lecture Due\n\n\n\n\n8/26-8/30\nWelcome to College/Course\n\n1\n\n\n\n9/2-9/6\nHello Data and Study Design\nCh 1 and 2\n2\n1 and 2\n\n\n9/9-9/13\nExploring Categorical Data\nCh 4\n3\n3\n\n\n9/16-9/20\nExploring Numerical Data\nCh 5\n4\n4\n\n\n9/23-9/27\nSimple Linear Regression\nCh 7\n5\n5\n\n\n9/30-10/4\nExam #1/Multivariable Linear Regression\n\n\n\n\n\n10/7-10/11\nMultivariable Linear Regression\nCh 8\n6\n6\n\n\n10/14-10/18\nLogistic Regression\nCh 9\n7\n7\n\n\n10/21-10/25\nHypothesis Testing\nCh 11\n8\n8\n\n\n10/28-11/1\nBootstrap-based Confidence Intervals\nCh 12\n9\n9\n\n\n11/4-11/9\nMathematical Models\nCh 13\n10\n10\n\n\n11/11-11/15\nExam #2\n\n\n\n\n\n11/18-11/22\nDecision Errors\nCh 14\n11\n11\n\n\n11/25-11/29\nLinear Regression Inference\nCh 24\n\n\n\n\n12/2-12/6\nLogistic Regression Inference\nCh 26\n12\n12\n\n\n12/9-12/13\nExam #3\n\n\n\n\n\n\n\n\n\nThe use of generative artificial intelligence (AI) will be prohibited in class. This includes, but not limited to, ChatGPT, Meta AI, and Google Gemini.\n\n\n\n\nAcademic Honesty:\nConduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Spring 2023 Semester website or they may be subject to removal from the classroom."
  },
  {
    "objectID": "syllabus.html#math-201-elementary-statistics",
    "href": "syllabus.html#math-201-elementary-statistics",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Spring 2025\nInstructor: Isaac Quintanilla Salinas\nEmail: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours: TBD\nOr by Zoom appointment.\nLecture: TBD\n\n\n\nCritical reasoning using a quantitative and statistical, problem-solving approach to solving real-world problems. Topics include: probability and statistics, sample data, probability and empirical data distributions, sampling techniques, estimation and hypothesis testing, ANOVA, and correlation and regression analysis. Students will use standard statistical software to analyze real-world and simulated data. GenEd: B4\n\n\n\n\nPrepare students for advanced courses in data-management and statistics\nReason both inductively and deductively with quantitative information and data\nUse statistical software for complex statistical analysis of real-world and simulated data\nEmpower students to apply computational and inferential thinking to address real-world problems\nWrite the results of a statistical study and draw conclusion in reports\n\n\n\n\nIntroduction to Modern Statistics\n\n\n\nTBD\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nParticipation\n5%\n\n\nReading Assignments\n10%\n\n\nVideo Assignments\n10%\n\n\nHomework\n15%\n\n\nNotebook Assignments\n15%\n\n\nExam 1\n15%\n\n\nExam 2\n15%\n\n\nExam 3\n15%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nParticipation is based on short writing assignments conducted in class. There will be no make ups for these writing prompts.\n\n\n\nNotebook assignments are designed to expand your statistical knowledge. These will be completed in Google Colab which can be accessed from Canvas. There is one notebook assignments every week that you can be completed during class time. The notebook assignment will be due on Wednesday at 11:59 PM every week.\n\n\n\nReading assignments are designed to teach you different statistical concepts and R Programming. As the course progresses, many of the concepts build on each other. Therefore, the assignments encourage you to read each chapter in an appropriate amount of time. You must read the chapter and answer the questions by the Sunday night at 11:59 PM. The 3 lowest reading assignments will be dropped.\n\n\n\nVideos are used to teach statistical concepts related to the course. Students are expected to watch at least one video a week. The videos are implemented using PlayPosit, where questions are embedded in the video to check for understanding. The videos may require students to program and/or read research articles.\n\n\n\nThere will be three in exams. Exam #1 will be on Sept 30, Exam #2 will be on Nov 13, and Exam #3 will be on Dec 11 at 8-10 AM (Sec 01) or Dec 9 1-3 PM (Sec 02). While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by your median average exam grade. This course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the University’s academic integrity policy.\n\n\n\nThere will be 3 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date. Information on the extra credit can be found here.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nReadings\nNB Due\nVideo Lecture Due\n\n\n\n\n8/26-8/30\nWelcome to College/Course\n\n1\n\n\n\n9/2-9/6\nHello Data and Study Design\nCh 1 and 2\n2\n1 and 2\n\n\n9/9-9/13\nExploring Categorical Data\nCh 4\n3\n3\n\n\n9/16-9/20\nExploring Numerical Data\nCh 5\n4\n4\n\n\n9/23-9/27\nSimple Linear Regression\nCh 7\n5\n5\n\n\n9/30-10/4\nExam #1/Multivariable Linear Regression\n\n\n\n\n\n10/7-10/11\nMultivariable Linear Regression\nCh 8\n6\n6\n\n\n10/14-10/18\nLogistic Regression\nCh 9\n7\n7\n\n\n10/21-10/25\nHypothesis Testing\nCh 11\n8\n8\n\n\n10/28-11/1\nBootstrap-based Confidence Intervals\nCh 12\n9\n9\n\n\n11/4-11/9\nMathematical Models\nCh 13\n10\n10\n\n\n11/11-11/15\nExam #2\n\n\n\n\n\n11/18-11/22\nDecision Errors\nCh 14\n11\n11\n\n\n11/25-11/29\nLinear Regression Inference\nCh 24\n\n\n\n\n12/2-12/6\nLogistic Regression Inference\nCh 26\n12\n12\n\n\n12/9-12/13\nExam #3\n\n\n\n\n\n\n\n\n\nThe use of generative artificial intelligence (AI) will be prohibited in class. This includes, but not limited to, ChatGPT, Meta AI, and Google Gemini.\n\n\n\n\nAcademic Honesty:\nConduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Spring 2023 Semester website or they may be subject to removal from the classroom."
  },
  {
    "objectID": "lectures/9.html#motivation-1",
    "href": "lectures/9.html#motivation-1",
    "title": "The Bootstrap Method",
    "section": "Motivation",
    "text": "Motivation\nThe bacteria data set contians information on whether bacteria (y: y or n) is present after utilizing treatments (ap: active or placebo).\n\nWe are interesting in determine the proportion of having bacteria present is different for those taking an “active” or “placebo”."
  },
  {
    "objectID": "lectures/9.html#comparing-proportions",
    "href": "lectures/9.html#comparing-proportions",
    "title": "The Bootstrap Method",
    "section": "Comparing Proportions",
    "text": "Comparing Proportions\nWe are interesting in determining if different groups see different proportions of a binary outcome.\nWe compute the proportions of observing the binary outcome in Group 1 and Group 2 and see if they are fundamentally different from each other."
  },
  {
    "objectID": "lectures/9.html#by-2-cross-tabulations",
    "href": "lectures/9.html#by-2-cross-tabulations",
    "title": "The Bootstrap Method",
    "section": "2 by 2 Cross Tabulations",
    "text": "2 by 2 Cross Tabulations\n\n\n\nGroups\nOutcome 1\nOutcome 2\n\n\nGroup 1\n\\(p_{11}\\)\n\\(p_{21}\\)\n\n\nGroup 2\n\\(p_{12}\\)\n\\(p_{22}\\)\n\n\n\n\nWe want to compare \\(p_{11}\\) and \\(p_{12}\\), to determine if the probability of outcome 1 are the same for both groups."
  },
  {
    "objectID": "lectures/9.html#test-statistic",
    "href": "lectures/9.html#test-statistic",
    "title": "The Bootstrap Method",
    "section": "Test Statistic",
    "text": "Test Statistic\nWe can use both \\(p_{11}\\) and \\(p_{12}\\) to determine if there is a fundamental difference.\n\nHowever, it will be more beneficial to utilize one statistic to contruct the sampling distribution.\n\n\n\\[\nT =  \\hat p_{11} - \\hat p_{12}\n\\]"
  },
  {
    "objectID": "lectures/9.html#obtain-difference-in-r",
    "href": "lectures/9.html#obtain-difference-in-r",
    "title": "The Bootstrap Method",
    "section": "Obtain Difference in R",
    "text": "Obtain Difference in R\n\n\nCode\nprops_df(DATA, GROUP, OUTCOME, VAL, diff = TRUE)"
  },
  {
    "objectID": "lectures/9.html#bacteria-example",
    "href": "lectures/9.html#bacteria-example",
    "title": "The Bootstrap Method",
    "section": "Bacteria Example",
    "text": "Bacteria Example\n\n\nCode\nprops_df(bacteria, ap, y, \"y\")\n\n\n#&gt;    p1    p2 \n#&gt; 0.750 0.875"
  },
  {
    "objectID": "lectures/9.html#bacteria-example-1",
    "href": "lectures/9.html#bacteria-example-1",
    "title": "The Bootstrap Method",
    "section": "Bacteria Example",
    "text": "Bacteria Example\n\n\nCode\nprops_df(bacteria, ap, y, \"y\", TRUE)\n\n\n#&gt; [1] 0.125"
  },
  {
    "objectID": "lectures/9.html#hypothesis-test-1",
    "href": "lectures/9.html#hypothesis-test-1",
    "title": "The Bootstrap Method",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nIs \\(T = .125\\) of a difference large enough to indicate that an active drug is effective against the bacteria, or seeing this can be due to random chance."
  },
  {
    "objectID": "lectures/9.html#hypotheis-test",
    "href": "lectures/9.html#hypotheis-test",
    "title": "The Bootstrap Method",
    "section": "Hypotheis Test",
    "text": "Hypotheis Test\nWe will test the following hypothesis:\n\\[\nH_0:\\ \\Delta =  p_1-p_2 = 0\n\\]\n\\[\nH_a:\\ \\Delta = p_1 - p_2 \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/9.html#hypothesis-test-2",
    "href": "lectures/9.html#hypothesis-test-2",
    "title": "The Bootstrap Method",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\n\nCode\nsim_stats &lt;- replicate(1000, props(bacteria$ap, shuffle(bacteria$y), \"y\", T))\ntest_stat &lt;- props(bacteria$ap, bacteria$y, \"y\", T)\nsum(abs(sim_stats) &gt; abs(test_stat)) / 1001\n\n\n#&gt; [1] 0.01498501\n\n\nCode\nggplot(data.frame(x = sim_stats), aes(x)) + \n  geom_density() +\n  geom_vline(xintercept = test_stat)"
  },
  {
    "objectID": "lectures/9.html#hypothesis-test-3",
    "href": "lectures/9.html#hypothesis-test-3",
    "title": "The Bootstrap Method",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\n\nWhat if \\(\\Delta = - 0.5\\)?\nWhat if \\(\\Delta = 0.5\\)?\nWhat if \\(\\Delta = - 0.25\\)?\nWhat if \\(\\Delta = 0.25\\)?\n\n\n\nCan we determine the plausible values of \\(\\Delta\\) that may have produced our data set?"
  },
  {
    "objectID": "lectures/9.html#standard-error",
    "href": "lectures/9.html#standard-error",
    "title": "The Bootstrap Method",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error is the standard deviation of a statistic.\n\nIt can give us a margin of error for the statistic that we compute from the data."
  },
  {
    "objectID": "lectures/9.html#confidence-intervals",
    "href": "lectures/9.html#confidence-intervals",
    "title": "The Bootstrap Method",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nConfidence Intervals give you a collection of values that may contain the true parameter of interest.\n\nThe probability of capturing the true parameter is known as the confidence. This value can take any number between 0% to 100%"
  },
  {
    "objectID": "lectures/9.html#confidence-interval",
    "href": "lectures/9.html#confidence-interval",
    "title": "The Bootstrap Method",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nThe confidence interval were displayed as: X%: (L, U)"
  },
  {
    "objectID": "lectures/9.html#confidence-interval-intepretation",
    "href": "lectures/9.html#confidence-interval-intepretation",
    "title": "The Bootstrap Method",
    "section": "Confidence Interval Intepretation",
    "text": "Confidence Interval Intepretation"
  },
  {
    "objectID": "lectures/9.html#confidence-intervals-1",
    "href": "lectures/9.html#confidence-intervals-1",
    "title": "The Bootstrap Method",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nThese are a collection of \\(\\Delta\\) values that will fail to reject the Null Hypothesis.\n\nThe \\(\\Delta\\) values that are not different from our current estimate \\(T\\)"
  },
  {
    "objectID": "lectures/9.html#confidence-intervals-2",
    "href": "lectures/9.html#confidence-intervals-2",
    "title": "The Bootstrap Method",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nCode\ndata.frame(x = rnorm(1000, 0.15, 0.05)) |&gt; \n  ggplot(aes(x)) + \n  geom_density() +\n  geom_vline(xintercept = test_stat)"
  },
  {
    "objectID": "lectures/9.html#confidence-intervals-3",
    "href": "lectures/9.html#confidence-intervals-3",
    "title": "The Bootstrap Method",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nCode\ndata.frame(x = rnorm(1000, 0.075, 0.05)) |&gt; \n  ggplot(aes(x)) + \n  geom_density() +\n  geom_vline(xintercept = test_stat)"
  },
  {
    "objectID": "lectures/9.html#how-to-construct-the-dgp-centered-around-delta",
    "href": "lectures/9.html#how-to-construct-the-dgp-centered-around-delta",
    "title": "The Bootstrap Method",
    "section": "How to construct the DGP centered around \\(\\Delta\\)?",
    "text": "How to construct the DGP centered around \\(\\Delta\\)?\n\nBootstrapping Techniques\n\n\nMathematical Models"
  },
  {
    "objectID": "lectures/9.html#bootstrapping-techniques-1",
    "href": "lectures/9.html#bootstrapping-techniques-1",
    "title": "The Bootstrap Method",
    "section": "Bootstrapping Techniques",
    "text": "Bootstrapping Techniques\nBootstrapping is a technique to determine which \\(\\Delta = p_1-p_2\\) values, or any other statistic, will not be significantly different than \\(T\\).\n\nBootstrapping will simulate several fake data sets from the DGP of the data. Afterwards, each data set will produce a statistic to construct the sampling distribution."
  },
  {
    "objectID": "lectures/9.html#bootstrap-theory",
    "href": "lectures/9.html#bootstrap-theory",
    "title": "The Bootstrap Method",
    "section": "Bootstrap Theory",
    "text": "Bootstrap Theory\n\nThe data set was generated from a distribution called \\(f\\).\n\n\n\\(f\\) is unknown, we will sample from \\(\\hat f\\) generated from the data.\n\n\nWhen \\(n \\rightarrow \\infty\\), then \\(\\hat f\\rightarrow f\\)\n\n\nSo long as \\(n\\) is sufficiently large, the DGP of a statistic will be generated."
  },
  {
    "objectID": "lectures/9.html#bootstrapping-algorithm",
    "href": "lectures/9.html#bootstrapping-algorithm",
    "title": "The Bootstrap Method",
    "section": "Bootstrapping Algorithm",
    "text": "Bootstrapping Algorithm\n\nResample the data set using the resample() function\nCompute the statistic of interest\nStore the statistic\nRepeat steps 1-3, until you a high number of simulations\nCompute the confidence interval or standard errors from the bootstrap estimates"
  },
  {
    "objectID": "lectures/9.html#bootstrapping-in-r",
    "href": "lectures/9.html#bootstrapping-in-r",
    "title": "The Bootstrap Method",
    "section": "Bootstrapping in R",
    "text": "Bootstrapping in R\nThe resample() will create a new data set by sampling the old data set with replacement.\n\n\nCode\nboot1 &lt;- resample(bacteria)\nprops_df(boot1, ap, y, yval = \"y\", TRUE)\n\n\n#&gt; [1] 0.130291"
  },
  {
    "objectID": "lectures/9.html#repeat-process",
    "href": "lectures/9.html#repeat-process",
    "title": "The Bootstrap Method",
    "section": "Repeat Process",
    "text": "Repeat Process\nUse the replicate() function to repeat this process a high number of times.\n\n\nCode\nboot_samples &lt;-replicate(1000, \n                         props_df(resample(bacteria), ap, y, yval = \"y\", TRUE))"
  },
  {
    "objectID": "lectures/9.html#bootstrap-distribution",
    "href": "lectures/9.html#bootstrap-distribution",
    "title": "The Bootstrap Method",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\n\n\nCode\nggplot(data.frame(x = boot_samples), aes(x)) +\n  geom_histogram() + \n  geom_vline(xintercept = test_stat)"
  },
  {
    "objectID": "lectures/9.html#bootstrap-based-standard-error",
    "href": "lectures/9.html#bootstrap-based-standard-error",
    "title": "The Bootstrap Method",
    "section": "Bootstrap-Based Standard Error",
    "text": "Bootstrap-Based Standard Error\n\n\nCode\nsd(boot_samples)\n\n\n#&gt; [1] 0.05044719"
  },
  {
    "objectID": "lectures/9.html#interpreting-the-se",
    "href": "lectures/9.html#interpreting-the-se",
    "title": "The Bootstrap Method",
    "section": "Interpreting the SE",
    "text": "Interpreting the SE\n\\[\n0.125 \\pm 0.05\n\\]"
  },
  {
    "objectID": "lectures/9.html#confidence-interval-1",
    "href": "lectures/9.html#confidence-interval-1",
    "title": "The Bootstrap Method",
    "section": "95% Confidence Interval",
    "text": "95% Confidence Interval\n\n\nCode\nggplot(data.frame(x = boot_samples), aes(x)) +\n  geom_histogram(aes(fill = middle(x, .95))) + \n  geom_vline(xintercept = test_stat)"
  },
  {
    "objectID": "lectures/9.html#confidence-interval-2",
    "href": "lectures/9.html#confidence-interval-2",
    "title": "The Bootstrap Method",
    "section": "95% Confidence Interval",
    "text": "95% Confidence Interval\n\n\nCode\nquantile(boot_samples, probs = c(0.025, 0.975))\n\n\n#&gt;       2.5%      97.5% \n#&gt; 0.02499504 0.22086102"
  },
  {
    "objectID": "lectures/9.html#interpretation",
    "href": "lectures/9.html#interpretation",
    "title": "The Bootstrap Method",
    "section": "Interpretation",
    "text": "Interpretation\nWe are 95% confident that the true difference in proportion (\\(\\Delta\\)) is captured between the ranges (0.025, 0.224)."
  },
  {
    "objectID": "lectures/7.html#melanoma",
    "href": "lectures/7.html#melanoma",
    "title": "Logistic Regression",
    "section": "Melanoma",
    "text": "Melanoma\nMelanoma is a type of skin cancer that causes the cells that produce melanin to grow out of control. What makes Melanoma so dangerous is that it can metastasize to other parts of the body."
  },
  {
    "objectID": "lectures/7.html#outcome-of-interest",
    "href": "lectures/7.html#outcome-of-interest",
    "title": "Logistic Regression",
    "section": "Outcome of Interest",
    "text": "Outcome of Interest\nWe are interested in learning how do different factors affect and individual’s chances of survival. Therefore, we are measuring patients and if they lived or died during a study period."
  },
  {
    "objectID": "lectures/7.html#data",
    "href": "lectures/7.html#data",
    "title": "Logistic Regression",
    "section": "Data",
    "text": "Data\nWe will be using the Melanoma data set with the following variables: dead (died by Melanoma, 1=yes, 0= no), sex (1 = male, 0 = female), age (in years), thickness (tumour thickness in mm), ulcer (1 = presence, 0 absent)."
  },
  {
    "objectID": "lectures/7.html#plot",
    "href": "lectures/7.html#plot",
    "title": "Logistic Regression",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nMelanoma |&gt; \n  ggplot(aes(thickness, dead)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/7.html#construct-model",
    "href": "lectures/7.html#construct-model",
    "title": "Logistic Regression",
    "section": "Construct Model",
    "text": "Construct Model\n\\[\n\\left(\\begin{array}{c}\nDead \\\\\nAlive\n\\end{array}\\right) = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#let",
    "href": "lectures/7.html#let",
    "title": "Logistic Regression",
    "section": "Let …",
    "text": "Let …\n\\[\nY = \\left\\{\\begin{array}{cc}\n1 & Dead \\\\\n0 & Alive\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "lectures/7.html#construct-a-model",
    "href": "lectures/7.html#construct-a-model",
    "title": "Logistic Regression",
    "section": "Construct a Model",
    "text": "Construct a Model\n\\[\nP\\left(Y = 1\\right) = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#construct-a-model-1",
    "href": "lectures/7.html#construct-a-model-1",
    "title": "Logistic Regression",
    "section": "Construct a Model",
    "text": "Construct a Model\n\\[\nP\\left(Y = 1\\right) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\n\\]"
  },
  {
    "objectID": "lectures/7.html#construct-a-model-2",
    "href": "lectures/7.html#construct-a-model-2",
    "title": "Logistic Regression",
    "section": "Construct a Model",
    "text": "Construct a Model\n\\[\n\\frac{P(Y = 1)}{P(Y = 0)} = e^{\\beta_0 + \\beta_1X}\n\\]\nwhere \\(\\frac{P(Y = 1)}{P(Y = 0)}\\) are considered the odds of observing \\(Y = 1\\)."
  },
  {
    "objectID": "lectures/7.html#the-logistic-model",
    "href": "lectures/7.html#the-logistic-model",
    "title": "Logistic Regression",
    "section": "The Logistic Model",
    "text": "The Logistic Model\n\\[\n\\log\\left\\{\\frac{P(Y = 1)}{P(Y = 0)}\\right\\} = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#logistic-regression-1",
    "href": "lectures/7.html#logistic-regression-1",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression is used to model the association between a set of predictors and a binary outcome variable.\n\nThis is similar Linear Regression which models the association between a set of predictors and a numerical outcome variable."
  },
  {
    "objectID": "lectures/7.html#logistic-regression-2",
    "href": "lectures/7.html#logistic-regression-2",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression uses the logistic model to formulate the relationship between the predictors and the outcome.\n\nMore specifically, for an outcome of Y:\n\\[\nY = \\left\\{\\begin{array}{cc}\n1 & \\text{Category 1} \\\\\n0 & \\text{Category 2}\n\\end{array}\\right.\n\\]\nThe Predictors variable will model the probability of observing category 1 (\\(P(Y=1)\\))"
  },
  {
    "objectID": "lectures/7.html#logistic-model",
    "href": "lectures/7.html#logistic-model",
    "title": "Logistic Regression",
    "section": "Logistic Model",
    "text": "Logistic Model\n\\[\n\\log\\left\\{\\frac{P(Y = 1)}{P(Y = 0)}\\right\\} = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#regression-coefficients-beta",
    "href": "lectures/7.html#regression-coefficients-beta",
    "title": "Logistic Regression",
    "section": "Regression Coefficients \\(\\beta\\)",
    "text": "Regression Coefficients \\(\\beta\\)\nThe regression coefficients quantify how a specific predictor changes the odds of observing the first category of the outcome (\\(Y = 1\\))"
  },
  {
    "objectID": "lectures/7.html#estimating-beta",
    "href": "lectures/7.html#estimating-beta",
    "title": "Logistic Regression",
    "section": "Estimating \\(\\beta\\)",
    "text": "Estimating \\(\\beta\\)\nTo obtain the numerical value for \\(\\beta\\), denoted as \\(\\hat \\beta\\), we will be finding the values of \\(\\hat \\beta\\) that maximizes the likelihood function:\n\\[\nL(\\boldsymbol \\beta) = \\prod_{i=1}^n \\left(\\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\right)^{Y_i}\\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1X}}\\right)^{1-Y_i}\n\\]\nThe likelihood function can be thought as the probability of observing the entire data set. Therefore, we want to choose the values the \\(\\beta_0\\) and \\(\\beta_1\\) that will result in the highest probability of observing the data."
  },
  {
    "objectID": "lectures/7.html#estimated-parameters",
    "href": "lectures/7.html#estimated-parameters",
    "title": "Logistic Regression",
    "section": "Estimated Parameters",
    "text": "Estimated Parameters\nThe values you obtain (\\(\\hat \\beta\\)) tell you the relationship between the a predictor variable and the log odds of observing the first category of the outcome \\(Y=1\\).\n\nExponentiating the estimate (\\(e^{\\hat \\beta}\\)) will give you the relationship between a predictor variable and the odds of observing the first category of the outcome \\(Y=1\\)."
  },
  {
    "objectID": "lectures/7.html#interpreting-hat-beta",
    "href": "lectures/7.html#interpreting-hat-beta",
    "title": "Logistic Regression",
    "section": "Interpreting \\(\\hat \\beta\\)",
    "text": "Interpreting \\(\\hat \\beta\\)\nFor a continuous predictor variable:\nAs X increases by 1 unit, the odds of observing the first category (\\(Y = 1\\)) increases by a factor of \\(e^{\\hat\\beta}\\).\nFor a categorical predictor variable (first dummy variable):\nThe odds of observing the first category (\\(Y = 1\\)) in the indicated category (\\(D=1\\)) is \\(e^{\\hat\\beta}\\) times higher/lower compared to the reference category (\\(D=0\\))."
  },
  {
    "objectID": "lectures/7.html#logistic-regression-in-r",
    "href": "lectures/7.html#logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Logistic Regression in R",
    "text": "Logistic Regression in R\n\n\nCode\nglm(Y ~ X1 + X2 + ... + Xp,\n    data = DATA,\n    family = binomial())"
  },
  {
    "objectID": "lectures/7.html#example",
    "href": "lectures/7.html#example",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\nModelling dead by sex, age, thickness, and ulcer:\n\n\nCode\nglm(dead ~ sex + age + thickness + ulcer,\n    data = Melanoma,\n    family = binomial())\n\n\n#&gt; \n#&gt; Call:  glm(formula = dead ~ sex + age + thickness + ulcer, family = binomial(), \n#&gt;     data = Melanoma)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          sex          age    thickness        ulcer  \n#&gt;    -2.39860      0.40767      0.00402      0.11253      1.31314  \n#&gt; \n#&gt; Degrees of Freedom: 204 Total (i.e. Null);  200 Residual\n#&gt; Null Deviance:       242.4 \n#&gt; Residual Deviance: 210.3     AIC: 220.3\n\n\n\\[\n\\log(odds) = -2.40 + 0.41 M + 0.004 age + 0.11 thick + 1.31ulc\n\\]"
  },
  {
    "objectID": "lectures/7.html#interpretation",
    "href": "lectures/7.html#interpretation",
    "title": "Logistic Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\n\\log(odds) = -2.40 + 0.41 M + 0.004 age + 0.11 thick + 1.31ulc\n\\]\n\nThe odds of experiencing death in males is 1.5 times larger than the odds for females, adjusting for age, tumor thickness and the presence of an ulcer.\n\n\nAs age increases by 1 year, the odds of experiencing death increases by a factor of 1.004, adjusting for sex, tumor thickness and presence of an ulcer."
  },
  {
    "objectID": "lectures/7.html#working-with-probabilities",
    "href": "lectures/7.html#working-with-probabilities",
    "title": "Logistic Regression",
    "section": "Working with probabilities",
    "text": "Working with probabilities\nAs you can see, working with odds may be unintuitive for the average person. It will be better to predict the probability and display those results to individuals."
  },
  {
    "objectID": "lectures/7.html#predicting-probability",
    "href": "lectures/7.html#predicting-probability",
    "title": "Logistic Regression",
    "section": "Predicting Probability",
    "text": "Predicting Probability\n\\[\n\\hat P\\left(Y = 1\\right) = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1X_1 + \\cdots  + \\hat \\beta_pX_p}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1X_1 + \\cdots  + \\hat \\beta_pX_p}}\n\\]"
  },
  {
    "objectID": "lectures/7.html#prediction-in-r",
    "href": "lectures/7.html#prediction-in-r",
    "title": "Logistic Regression",
    "section": "Prediction in R",
    "text": "Prediction in R\n\n\nCode\nxglm &lt;- glm(Y ~ X1 + X2 + ... + Xp,\n            data = DATA,\n            family = binomial())\npredict_df &lt;- data.frame(X1 = VAL,  X2 = VAL, ..., Xp = VAL)\npredict(xglm,\n        predict_df,\n        type = \"response\")"
  },
  {
    "objectID": "lectures/7.html#example-1",
    "href": "lectures/7.html#example-1",
    "title": "Logistic Regression",
    "section": "Example 1",
    "text": "Example 1\nPredict the probability of observing death for a patient who is male, 75 years old, an tumor thickness of 2.9, and with ulcer presence.\n\n\nCode\nxglm &lt;- glm(dead ~ sex + age + thickness + ulcer,\n    data = Melanoma,\n    family = binomial())\npredict_df &lt;- data.frame(sex = 1, age = 75, thickness = 2.9, ulcer = 1)\npredict(xglm, predict_df, type = \"response\")\n\n\n#&gt;         1 \n#&gt; 0.4875223"
  },
  {
    "objectID": "lectures/7.html#example-1-1",
    "href": "lectures/7.html#example-1-1",
    "title": "Logistic Regression",
    "section": "Example 1",
    "text": "Example 1\nPredict the probability of observing death for a patient who is male, 75 years old, an tumor thickness of 2.9, and without ulcer presence.\n\n\nCode\nxglm &lt;- glm(dead ~ sex + age + thickness + ulcer,\n    data = Melanoma,\n    family = binomial())\npredict_df &lt;- data.frame(sex = 1, age = 75, thickness = 2.9, ulcer = 0)\npredict(xglm, predict_df, type = \"response\")\n\n\n#&gt;         1 \n#&gt; 0.2037438"
  },
  {
    "objectID": "lectures/7.html#example-1-2",
    "href": "lectures/7.html#example-1-2",
    "title": "Logistic Regression",
    "section": "Example 1",
    "text": "Example 1\n\n\n\n\nUlcer Present\nUlcer not Present\n\n\n\n\nProbabiltiy\n48.7%\n20.3%\n\n\n\nFor a patient who is 75 years old, male, and a tumor thickness of 2.9, when an ulcer is present, the probability a patient dies increases to 48% compared to a patient who does not have an ulcer with a probability of 20%."
  },
  {
    "objectID": "lectures/7.html#example-2",
    "href": "lectures/7.html#example-2",
    "title": "Logistic Regression",
    "section": "Example 2",
    "text": "Example 2\nPredict the probability of observing death for a patient who is female, 75 years old, an tumor thickness of 2.9, and with ulcer presence.\n\n\nCode\nxglm &lt;- glm(dead ~ sex + age + thickness + ulcer,\n    data = Melanoma,\n    family = binomial())\npredict_df &lt;- data.frame(sex = 0, age = 75, thickness = 2.9, ulcer = 1)\npredict(xglm, predict_df, type = \"response\")\n\n\n#&gt;         1 \n#&gt; 0.3875575"
  },
  {
    "objectID": "lectures/7.html#example-2-1",
    "href": "lectures/7.html#example-2-1",
    "title": "Logistic Regression",
    "section": "Example 2",
    "text": "Example 2\nPredict the probability of observing death for a patient who is female, 55 years old, an tumor thickness of 2.9, and with ulcer presence.\n\n\nCode\nxglm &lt;- glm(dead ~ sex + age + thickness + ulcer,\n    data = Melanoma,\n    family = binomial())\npredict_df &lt;- data.frame(sex = 0, age = 55, thickness = 2.9, ulcer = 1)\npredict(xglm, predict_df, type = \"response\")\n\n\n#&gt;         1 \n#&gt; 0.3686531"
  },
  {
    "objectID": "lectures/7.html#example-2-2",
    "href": "lectures/7.html#example-2-2",
    "title": "Logistic Regression",
    "section": "Example 2",
    "text": "Example 2\n\n\n\n\n55\n75\n\n\n\n\nProbabiltiy\n36.8%\n38.7%\n\n\n\nFor a patient who is female, a tumor thickness of 2.9, and has an ulcer, the probability a 75 year old patient dies increases to 38.7% compared to a 55 year old patient with a probability of 36.8%."
  },
  {
    "objectID": "lectures/5.html#explaining-variation",
    "href": "lectures/5.html#explaining-variation",
    "title": "Linear Regression",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\nThis is the process where we try to reduce the variation with the use of other variables.\n\n\nCan be thought of as getting it less wrong when taking an educated guess."
  },
  {
    "objectID": "lectures/5.html#explaining-variation-1",
    "href": "lectures/5.html#explaining-variation-1",
    "title": "Linear Regression",
    "section": "Explaining Variation",
    "text": "Explaining Variation"
  },
  {
    "objectID": "lectures/5.html#explaining-variation-with-one-variable",
    "href": "lectures/5.html#explaining-variation-with-one-variable",
    "title": "Linear Regression",
    "section": "Explaining Variation with One Variable",
    "text": "Explaining Variation with One Variable"
  },
  {
    "objectID": "lectures/5.html#generated-model",
    "href": "lectures/5.html#generated-model",
    "title": "Linear Regression",
    "section": "Generated Model",
    "text": "Generated Model\n\\[\nY \\sim DGP_1\n\\]"
  },
  {
    "objectID": "lectures/5.html#a-simple-model-1",
    "href": "lectures/5.html#a-simple-model-1",
    "title": "Linear Regression",
    "section": "A Simple Model",
    "text": "A Simple Model"
  },
  {
    "objectID": "lectures/5.html#a-simple-model-2",
    "href": "lectures/5.html#a-simple-model-2",
    "title": "Linear Regression",
    "section": "A Simple Model",
    "text": "A Simple Model\n\\[\nY = \\_\\_\\_ + error\n\\]"
  },
  {
    "objectID": "lectures/5.html#notation",
    "href": "lectures/5.html#notation",
    "title": "Linear Regression",
    "section": "Notation",
    "text": "Notation\n\\[\nY = \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\varepsilon\n\\]"
  },
  {
    "objectID": "lectures/5.html#the-simple-generated-model",
    "href": "lectures/5.html#the-simple-generated-model",
    "title": "Linear Regression",
    "section": "The Simple Generated Model",
    "text": "The Simple Generated Model\n\\[\nY \\sim \\mu + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP_2\n\\]\n\n\\(DGP_2\\) is not the same as the \\(DGP_1\\), it is transformed due \\(\\beta_0\\). Consider this the NULL \\(DGP\\)."
  },
  {
    "objectID": "lectures/5.html#observing-data",
    "href": "lectures/5.html#observing-data",
    "title": "Linear Regression",
    "section": "Observing Data",
    "text": "Observing Data\n\\[\nY = \\beta_0 + \\varepsilon\n\\]"
  },
  {
    "objectID": "lectures/5.html#estimated-line",
    "href": "lectures/5.html#estimated-line",
    "title": "Linear Regression",
    "section": "Estimated Line",
    "text": "Estimated Line\n\\[\n\\hat Y=\\hat\\beta_0\n\\]"
  },
  {
    "objectID": "lectures/5.html#notation-1",
    "href": "lectures/5.html#notation-1",
    "title": "Linear Regression",
    "section": "Notation",
    "text": "Notation\n\n\nObserved\n\\[\nY = \\beta_0 + \\varepsilon\n\\]\n\nEstimated\n\\[\n\\hat Y = \\hat \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/5.html#indexing-data",
    "href": "lectures/5.html#indexing-data",
    "title": "Linear Regression",
    "section": "Indexing Data",
    "text": "Indexing Data\nThe data in a data set can be indexed by a number.\n\npenguins[1,-c(1:2)]\n\n#&gt; # A tibble: 1 × 6\n#&gt;   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n#&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n#&gt; 1           39.1          18.7               181        3750 male   2007\n\n\n\nMaking the variable “body_mass_g” be represented by \\(Y\\) and “flipper_length_mm” as \\(X\\):\n\\[\nY_1 = 3750 \\ \\ X_1=181\n\\]"
  },
  {
    "objectID": "lectures/5.html#indexing-data-1",
    "href": "lectures/5.html#indexing-data-1",
    "title": "Linear Regression",
    "section": "Indexing Data",
    "text": "Indexing Data\n\\[\nY_i, X_i\n\\]"
  },
  {
    "objectID": "lectures/5.html#data",
    "href": "lectures/5.html#data",
    "title": "Linear Regression",
    "section": "Data",
    "text": "Data\nWith the data that we collect from a sample, we hypothesize how the data was generated.\n\nUsing a simple model:\n\\[\nY_i = \\beta_0 + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/5.html#estimated-value",
    "href": "lectures/5.html#estimated-value",
    "title": "Linear Regression",
    "section": "Estimated Value",
    "text": "Estimated Value\n\\[\n\\hat Y_i = \\hat \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/5.html#estimation",
    "href": "lectures/5.html#estimation",
    "title": "Linear Regression",
    "section": "Estimation",
    "text": "Estimation\nTo estimate \\(\\hat \\beta_0\\), we minimize the follow function:\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]\n\nThis is known as the sum squared errors, SSE"
  },
  {
    "objectID": "lectures/5.html#residuals",
    "href": "lectures/5.html#residuals",
    "title": "Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals are known as the observed errors from the data in the model:\n\\[\nr_i = Y_i - \\hat Y_i\n\\]"
  },
  {
    "objectID": "lectures/5.html#estimation-in-r",
    "href": "lectures/5.html#estimation-in-r",
    "title": "Linear Regression",
    "section": "Estimation in R",
    "text": "Estimation in R\n\nlm(Y ~ 1, data = DATA)"
  },
  {
    "objectID": "lectures/5.html#modeling-body-mass-in-penguins",
    "href": "lectures/5.html#modeling-body-mass-in-penguins",
    "title": "Linear Regression",
    "section": "Modeling Body Mass in Penguins",
    "text": "Modeling Body Mass in Penguins\n\nlm(body_mass_g ~ 1, data = penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ 1, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)  \n#&gt;        4207"
  },
  {
    "objectID": "lectures/5.html#linear-model-1",
    "href": "lectures/5.html#linear-model-1",
    "title": "Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\nThe goal of Statistics is to develop models the have a better explanation of the outcome \\(Y\\).\n\nIn particularly, reduce the sum of squared errors.\n\n\nBy utilizing a bit more of information, \\(X\\), we can increase the predicting capabilities of the model.\n\n\nThus, the linear model is born."
  },
  {
    "objectID": "lectures/5.html#linear-model-2",
    "href": "lectures/5.html#linear-model-2",
    "title": "Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP_3\n\\]"
  },
  {
    "objectID": "lectures/5.html#scatter-plot",
    "href": "lectures/5.html#scatter-plot",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  geom_point() +\n  theme(axis.text.x = element_text(size = 24),\n                   axis.title.x = element_text(size = 30),\n                   plot.title = element_text(size = 48),\n                   strip.text = element_text(size = 20),\n                   legend.title = element_blank(),\n                   legend.text = element_text(size = 24))"
  },
  {
    "objectID": "lectures/5.html#imposing-a-line",
    "href": "lectures/5.html#imposing-a-line",
    "title": "Linear Regression",
    "section": "Imposing a Line",
    "text": "Imposing a Line\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  geom_point() +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(axis.text.x = element_text(size = 24),\n                   axis.title.x = element_text(size = 30),\n                   plot.title = element_text(size = 48),\n                   strip.text = element_text(size = 20),\n                   legend.title = element_blank(),\n                   legend.text = element_text(size = 24))"
  },
  {
    "objectID": "lectures/5.html#modelling-the-data",
    "href": "lectures/5.html#modelling-the-data",
    "title": "Linear Regression",
    "section": "Modelling the Data",
    "text": "Modelling the Data\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/5.html#linear-model-3",
    "href": "lectures/5.html#linear-model-3",
    "title": "Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\n\\]\n\nGoal is to obtain numerical values for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) that will minimize the SSE."
  },
  {
    "objectID": "lectures/5.html#sse",
    "href": "lectures/5.html#sse",
    "title": "Linear Regression",
    "section": "SSE",
    "text": "SSE\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\n\\]"
  },
  {
    "objectID": "lectures/5.html#fitting-a-model-in-r",
    "href": "lectures/5.html#fitting-a-model-in-r",
    "title": "Linear Regression",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\nlm(Y ~ X, data = DATA)"
  },
  {
    "objectID": "lectures/5.html#example",
    "href": "lectures/5.html#example",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nY: “body_mass_g”; X: “flipper_length_mm”\n\nlm(body_mass_g ~ flipper_length_mm, data = penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm  \n#&gt;          -5872.09              50.15\n\n\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]"
  },
  {
    "objectID": "lectures/5.html#interpretation-of-hatbeta_0",
    "href": "lectures/5.html#interpretation-of-hatbeta_0",
    "title": "Linear Regression",
    "section": "Interpretation of \\(\\hat\\beta_0\\)",
    "text": "Interpretation of \\(\\hat\\beta_0\\)\nThe intercept \\(\\hat \\beta_0\\) can be interpreted as the base value when \\(X\\) is set to 0.\n\nSome times the intercept can be interpretable to real world scenarios.\n\n\nOther times it cannot."
  },
  {
    "objectID": "lectures/5.html#interpreting-example",
    "href": "lectures/5.html#interpreting-example",
    "title": "Linear Regression",
    "section": "Interpreting Example",
    "text": "Interpreting Example\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\] When flipper length is 0 mm, the body mass is -5872 grams."
  },
  {
    "objectID": "lectures/5.html#interpretation-of-hat-beta_1",
    "href": "lectures/5.html#interpretation-of-hat-beta_1",
    "title": "Linear Regression",
    "section": "Interpretation of \\(\\hat \\beta_1\\)",
    "text": "Interpretation of \\(\\hat \\beta_1\\)\nThe slope \\(\\hat \\beta_1\\) indicates how will y change when x increases by 1 unit.\n\nIt will demonstrate if there is, on average, a positive or negative relationship based on the sign provided."
  },
  {
    "objectID": "lectures/5.html#interpreting-example-1",
    "href": "lectures/5.html#interpreting-example-1",
    "title": "Linear Regression",
    "section": "Interpreting Example",
    "text": "Interpreting Example\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]\nWhen flipper length increases by 1 mm, the body mass will increase by 50.15 grams."
  },
  {
    "objectID": "lectures/5.html#body-mass-with-species",
    "href": "lectures/5.html#body-mass-with-species",
    "title": "Linear Regression",
    "section": "Body Mass with Species",
    "text": "Body Mass with Species\n\n\nCode\npenguins |&gt; \n  ggplot(aes(body_mass_g)) +\n    geom_boxplot() +\n    theme(axis.text.x = element_text(size = 24),\n          axis.title.x = element_text(size = 30),\n          plot.title = element_text(size = 48),\n          strip.text = element_text(size = 20),\n          legend.title = element_blank(),\n          legend.text = element_text(size = 24))"
  },
  {
    "objectID": "lectures/5.html#body-mass-with-species-1",
    "href": "lectures/5.html#body-mass-with-species-1",
    "title": "Linear Regression",
    "section": "Body Mass with Species",
    "text": "Body Mass with Species\n\n\nCode\npenguins |&gt; \n  ggplot(aes(body_mass_g, fill = species)) +\n    geom_boxplot() +\n    theme(axis.text.x = element_text(size = 24),\n          axis.title.x = element_text(size = 30),\n          plot.title = element_text(size = 48),\n          strip.text = element_text(size = 20),\n          legend.title = element_blank(),\n          legend.text = element_text(size = 24))"
  },
  {
    "objectID": "lectures/5.html#group-statistics",
    "href": "lectures/5.html#group-statistics",
    "title": "Linear Regression",
    "section": "Group Statistics",
    "text": "Group Statistics\nWe can use statistics to explain a continuous variable by the categories.\n\nCompute statistics for each group.\n\n\n\nnum_by_cat_stats(DATA, NUM, CAT)"
  },
  {
    "objectID": "lectures/5.html#compute-group-statistics",
    "href": "lectures/5.html#compute-group-statistics",
    "title": "Linear Regression",
    "section": "Compute Group Statistics",
    "text": "Compute Group Statistics\n\nnum_by_cat_stats(penguins, body_mass_g, species)\n\n#&gt; # A tibble: 3 × 11\n#&gt;   species     min   q25  mean median   q75   max    sd     var   iqr missing\n#&gt;   &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n#&gt; 1 Adelie     2850 3362. 3706.   3700  4000  4775  459. 210332.  638.       0\n#&gt; 2 Chinstrap  2700 3488. 3733.   3700  3950  4800  384. 147713.  462.       0\n#&gt; 3 Gentoo     3950 4700  5092.   5050  5500  6300  501. 251478.  800        0"
  },
  {
    "objectID": "lectures/5.html#linear-models-with-categorical-variables",
    "href": "lectures/5.html#linear-models-with-categorical-variables",
    "title": "Linear Regression",
    "section": "Linear Models with Categorical Variables",
    "text": "Linear Models with Categorical Variables\nA line is normally used to model 2 continuous variables.\n\nHowever, the predictor variable \\(X\\) can be restricted to a set a variables that can symbolize categories.\n\n\nA category will be used as a reference for a model."
  },
  {
    "objectID": "lectures/5.html#binary-dummy-variables",
    "href": "lectures/5.html#binary-dummy-variables",
    "title": "Linear Regression",
    "section": "Binary (Dummy) Variables",
    "text": "Binary (Dummy) Variables\nBinary variables are variable that can only take on the value 0 or 1.\n\\[\nD_i = \\left\\{\n\\begin{array}{cc}\n1 & Category\\\\\n0 & Other\n\\end{array}\n\\right.\n\\]"
  },
  {
    "objectID": "lectures/5.html#binary-dummy-variables-1",
    "href": "lectures/5.html#binary-dummy-variables-1",
    "title": "Linear Regression",
    "section": "Binary (Dummy) Variables",
    "text": "Binary (Dummy) Variables\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use \\(C-1\\) dummy variables where \\(C\\) indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables."
  },
  {
    "objectID": "lectures/5.html#example-1",
    "href": "lectures/5.html#example-1",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nIf we have 4 categories, we will need 3 dummy variables:\n\n\n\n\nCat 1\nCat 2\nCat 3\nCat 4\n\n\n\n\nDummy 1\n1\n0\n0\n0\n\n\nDummy 2\n0\n1\n0\n0\n\n\nDummy 2\n0\n0\n1\n0"
  },
  {
    "objectID": "lectures/5.html#species-dummy-variables",
    "href": "lectures/5.html#species-dummy-variables",
    "title": "Linear Regression",
    "section": "Species Dummy Variables",
    "text": "Species Dummy Variables\n\n\n\n\nChinstrap\nGentoo\nAdelie\n\n\n\n\n\\(D_1\\)\n1\n0\n0\n\n\n\\(D_2\\)\n0\n1\n0"
  },
  {
    "objectID": "lectures/5.html#linear-model-4",
    "href": "lectures/5.html#linear-model-4",
    "title": "Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat\\beta_1 D_{1i} + \\hat\\beta_2 D_{2i}\n\\]\n\n\\(\\hat \\beta_1\\) indicates how body mass changes from Adelie to Chinstrap.\n\n\n\\(\\hat \\beta_2\\) indicates how body mass changes from Adelie to Gentoo.\n\n\n\\(\\hat \\beta_0\\) represents the baseline level, in this case the body mass of Adelie."
  },
  {
    "objectID": "lectures/5.html#fitting-a-model-in-r-1",
    "href": "lectures/5.html#fitting-a-model-in-r-1",
    "title": "Linear Regression",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\nlm(Y ~ X, data = DATA)"
  },
  {
    "objectID": "lectures/5.html#example-2",
    "href": "lectures/5.html#example-2",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\n\nlm(body_mass_g ~ species, penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept)  speciesChinstrap     speciesGentoo  \n#&gt;          3706.16             26.92           1386.27\n\n\n\\[\n\\hat Y_i = 3706 + 26.92 D_{1i} + 1386.27 D_{2i}\n\\]"
  },
  {
    "objectID": "lectures/5.html#finding-the-adelie-mass",
    "href": "lectures/5.html#finding-the-adelie-mass",
    "title": "Linear Regression",
    "section": "Finding the Adelie MASS",
    "text": "Finding the Adelie MASS\n\\[\n\\hat Y_i = 3706 + 26.92 (0) + 1386.27 (0)\n\\]"
  },
  {
    "objectID": "lectures/5.html#finding-the-chinstrap-mass",
    "href": "lectures/5.html#finding-the-chinstrap-mass",
    "title": "Linear Regression",
    "section": "Finding the Chinstrap MASS",
    "text": "Finding the Chinstrap MASS\n\\[\n\\hat Y_i = 3706 + 26.92 (1) + 1386.27 (0)\n\\]"
  },
  {
    "objectID": "lectures/5.html#finding-the-gentoo-mass",
    "href": "lectures/5.html#finding-the-gentoo-mass",
    "title": "Linear Regression",
    "section": "Finding the Gentoo MASS",
    "text": "Finding the Gentoo MASS\n\\[\n\\hat Y_i = 3706 + 26.92 (0) + 1386.27 (1)\n\\]"
  },
  {
    "objectID": "lectures/5.html#intepreting-hat-beta_1",
    "href": "lectures/5.html#intepreting-hat-beta_1",
    "title": "Linear Regression",
    "section": "Intepreting \\(\\hat \\beta_1\\)",
    "text": "Intepreting \\(\\hat \\beta_1\\)\nOn average, Chinstrap has a larger mass than Adelie by about 26.92 grams."
  },
  {
    "objectID": "lectures/5.html#intepreting-hat-beta_2",
    "href": "lectures/5.html#intepreting-hat-beta_2",
    "title": "Linear Regression",
    "section": "Intepreting \\(\\hat \\beta_2\\)",
    "text": "Intepreting \\(\\hat \\beta_2\\)\nOn average, Gentoo has a larger mass than Adelie by about 1386.27 grams."
  },
  {
    "objectID": "lectures/5.html#correlation",
    "href": "lectures/5.html#correlation",
    "title": "Linear Regression",
    "section": "Correlation",
    "text": "Correlation\nCorrelation is a statistics that can be used to describe the strength of the relationship between 2 continuous variables.\n\n\\[\nr = \\frac{1}{n-1}\\sum^n_{i=1}\\frac{x_i - \\bar x}{s_x}\\frac{y_i - \\bar y}{s_y}\n\\]\n\n\\(\\bar x\\), \\(\\bar y\\): sample means\n\\(s_x\\), \\(s_y\\): sample standard deviations\n\n\n\n\\[\n-1 \\leq r \\leq 1\n\\]"
  },
  {
    "objectID": "lectures/5.html#correlations",
    "href": "lectures/5.html#correlations",
    "title": "Linear Regression",
    "section": "Correlations",
    "text": "Correlations\n\nFrom IMS 2e"
  },
  {
    "objectID": "lectures/5.html#coefficient-of-determination",
    "href": "lectures/5.html#coefficient-of-determination",
    "title": "Linear Regression",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\nThe coefficient of determination evaluates the strength between an outcome \\(Y\\) and the linear model, which includes \\(X\\).\n\n\\[\nR^2 = r^2\n\\]\n\n\n\\[\n0 \\leq R^2 \\leq 1\n\\]\n\n\nThe coefficient of determination measures the total variation explained by the linear model. The closer to 1, the better the linear model."
  },
  {
    "objectID": "lectures/5.html#correlation-in-r",
    "href": "lectures/5.html#correlation-in-r",
    "title": "Linear Regression",
    "section": "Correlation in R",
    "text": "Correlation in R\n\ncor(DATA$Y, DATA$X)"
  },
  {
    "objectID": "lectures/5.html#example-3",
    "href": "lectures/5.html#example-3",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\n\ncor(penguins$body_mass_g, penguins$flipper_length_mm)\n\n#&gt; [1] 0.8729789"
  },
  {
    "objectID": "lectures/5.html#coefficient-of-determination-in-r",
    "href": "lectures/5.html#coefficient-of-determination-in-r",
    "title": "Linear Regression",
    "section": "Coefficient of Determination in R",
    "text": "Coefficient of Determination in R\n\nxlm &lt;- lm(Y ~ X, data = DATA)\nr2(xlm)"
  },
  {
    "objectID": "lectures/5.html#example-4",
    "href": "lectures/5.html#example-4",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\n\nxlm &lt;- lm(body_mass_g ~ species, penguins)\nr2(xlm)\n\n#&gt; [1] 0.6744887"
  },
  {
    "objectID": "lectures/3.html#r-packages",
    "href": "lectures/3.html#r-packages",
    "title": "Categorical Data",
    "section": "R Packages",
    "text": "R Packages\n\ncsucistats\ntidyverse\nggtricks\nggmosaic\nwaffle"
  },
  {
    "objectID": "lectures/3.html#background",
    "href": "lectures/3.html#background",
    "title": "Categorical Data",
    "section": "Background",
    "text": "Background\nIn October 2023, James Hoffman and Cometeer held the “Great American Coffee Taste Test” on YouTube, asking viewers to fill out a survey and coffee ordered from Cometeer."
  },
  {
    "objectID": "lectures/3.html#data",
    "href": "lectures/3.html#data",
    "title": "Categorical Data",
    "section": "Data",
    "text": "Data\nThe data is part of DSLC Tidy Tuesday program where data sets are provided to help data science learners how to create graphics.\nInformation on the data sets variables (columns) can be found here."
  },
  {
    "objectID": "lectures/3.html#data-1",
    "href": "lectures/3.html#data-1",
    "title": "Categorical Data",
    "section": "Data",
    "text": "Data\n\n\nCode\nlibrary(csucistats)\nlibrary(ggtricks)\nlibrary(waffle)\nlibrary(ggmosaic)\nlibrary(tidyverse)\nlibrary(ThemePark)\nlibrary(DT)\ncoffee &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-05-14/coffee_survey.csv\")\ndatatable(slice_sample(coffee, n = 10), options = list(dom = \"p\", pageLength = 5))"
  },
  {
    "objectID": "lectures/3.html#categorical-data-1",
    "href": "lectures/3.html#categorical-data-1",
    "title": "Categorical Data",
    "section": "Categorical Data",
    "text": "Categorical Data\nCategorical data are data recordings that represented a category.\n\nData may be recorded as a “character” or “string” data.\n\n\nData may be recorded as a whole number, with an attached code book indicating the categories each number belongs to."
  },
  {
    "objectID": "lectures/3.html#examples-of-categorical-data",
    "href": "lectures/3.html#examples-of-categorical-data",
    "title": "Categorical Data",
    "section": "Examples of Categorical Data",
    "text": "Examples of Categorical Data\n\n\nAre you a student?\nWhat city do you live in?\nWhat is your major?"
  },
  {
    "objectID": "lectures/3.html#likert-scale",
    "href": "lectures/3.html#likert-scale",
    "title": "Categorical Data",
    "section": "Likert Scale",
    "text": "Likert Scale\nLikert scales are the rating systems you may have answered in surveys.\n\n\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree"
  },
  {
    "objectID": "lectures/3.html#likert-scales",
    "href": "lectures/3.html#likert-scales",
    "title": "Categorical Data",
    "section": "Likert Scales",
    "text": "Likert Scales\nLikert scales may be treated as numerical data if the jumps between scales are equal."
  },
  {
    "objectID": "lectures/3.html#summarizing-categorical-data",
    "href": "lectures/3.html#summarizing-categorical-data",
    "title": "Categorical Data",
    "section": "Summarizing Categorical Data",
    "text": "Summarizing Categorical Data\nOnce we have the data, how do we summarize it to other people."
  },
  {
    "objectID": "lectures/3.html#continguency-tables-1",
    "href": "lectures/3.html#continguency-tables-1",
    "title": "Categorical Data",
    "section": "Continguency Tables",
    "text": "Continguency Tables\nContinguency tables display how often a category is seen in the data.\n\nThere are two types of statistics that are reported in a table, the frequency and proportion."
  },
  {
    "objectID": "lectures/3.html#frequencey",
    "href": "lectures/3.html#frequencey",
    "title": "Categorical Data",
    "section": "Frequencey",
    "text": "Frequencey\nFrequency represents the count of observing a specific category in your sample.\n\n\n#&gt;  [1] \"2\" \"1\" \"2\" \"1\" \"1\" \"1\" \"2\" \"1\" \"2\" \"1\""
  },
  {
    "objectID": "lectures/3.html#proportions-relative-frequencey",
    "href": "lectures/3.html#proportions-relative-frequencey",
    "title": "Categorical Data",
    "section": "Proportions (relative frequencey)",
    "text": "Proportions (relative frequencey)\nProportions represent the percentage that the category represents the sample.\n\nThis allows you to generalize your sample to the population, regardless of sample size."
  },
  {
    "objectID": "lectures/3.html#continguency-tables-in-r",
    "href": "lectures/3.html#continguency-tables-in-r",
    "title": "Categorical Data",
    "section": "Continguency Tables in R",
    "text": "Continguency Tables in R\nThe variable caffeine indicates how much caffeine a participant prefers.\n\ncat_stats(coffee$caffeine)"
  },
  {
    "objectID": "lectures/3.html#plotting-in-r",
    "href": "lectures/3.html#plotting-in-r",
    "title": "Categorical Data",
    "section": "Plotting in R",
    "text": "Plotting in R\nPlotting in R can be done via the ggplot2, a powerful library based on the Grammar of Graphics."
  },
  {
    "objectID": "lectures/3.html#plotting-in-r-1",
    "href": "lectures/3.html#plotting-in-r-1",
    "title": "Categorical Data",
    "section": "Plotting in R",
    "text": "Plotting in R\n\nYou need to create a base plot using the ggplot()\nUse the + to change the look of the base plot\nIndicate how to transform the base plot to the desired plot\ngeom_*\nstat_*\nChange the look of the plot with other functions\nUse a theme_* function to add a theme to the plot"
  },
  {
    "objectID": "lectures/3.html#bar-plots-1",
    "href": "lectures/3.html#bar-plots-1",
    "title": "Categorical Data",
    "section": "Bar Plots",
    "text": "Bar Plots\nBar Plots can be used to display the frequency or proportions on the data."
  },
  {
    "objectID": "lectures/3.html#frequency-bar-plots-in-r",
    "href": "lectures/3.html#frequency-bar-plots-in-r",
    "title": "Categorical Data",
    "section": "Frequency Bar Plots in R",
    "text": "Frequency Bar Plots in R\n\nggplot(data = DATA, aes(x = VARIABLE)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#frequency-bar-plots-in-r-1",
    "href": "lectures/3.html#frequency-bar-plots-in-r-1",
    "title": "Categorical Data",
    "section": "Frequency Bar Plots in R",
    "text": "Frequency Bar Plots in R\n\n\nCode\nggplot(coffee, aes(caffeine)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#relative-frequency-bar-plots-in-r",
    "href": "lectures/3.html#relative-frequency-bar-plots-in-r",
    "title": "Categorical Data",
    "section": "Relative Frequency Bar Plots in R",
    "text": "Relative Frequency Bar Plots in R\n\nggplot(data = DATA, aes(x = VARIABLE, y = after_stat(prop), group = 1)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#relative-frequency-bar-plots-in-r-1",
    "href": "lectures/3.html#relative-frequency-bar-plots-in-r-1",
    "title": "Categorical Data",
    "section": "Relative Frequency Bar Plots in R",
    "text": "Relative Frequency Bar Plots in R\n\n\nCode\nggplot(coffee, aes(caffeine, after_stat(prop), group = 1)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#cross-tabulation-1",
    "href": "lectures/3.html#cross-tabulation-1",
    "title": "Categorical Data",
    "section": "Cross-Tabulation",
    "text": "Cross-Tabulation\nCross Tabulations allow you to see how data points are divided when looking at 2 different categorical variables."
  },
  {
    "objectID": "lectures/3.html#data-2",
    "href": "lectures/3.html#data-2",
    "title": "Categorical Data",
    "section": "Data",
    "text": "Data\nThe variable taste indicates if the participants like the taste of coffee.\n\n\n#&gt;    [1] NA              NA              NA              NA              NA              NA              NA              NA              NA              NA              NA              NA              NA             \n#&gt;   [14] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;   [27] \"Full caffeine\" NA              NA              NA              \"Full caffeine\" NA              NA              \"Full caffeine\" \"Full caffeine\" NA              NA              \"Full caffeine\" \"Full caffeine\"\n#&gt;   [40] \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"    \n#&gt;   [53] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     NA              \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;   [66] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;   [79] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt;   [92] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [105] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"        \n#&gt;  [118] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt;  [131] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [144] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [157] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [170] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [183] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [196] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [209] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [222] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [235] NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [248] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [261] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [274] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [287] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [300] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [313] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Half caff\"     \"Full caffeine\"\n#&gt;  [326] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [339] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [352] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [365] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [378] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [391] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [404] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [417] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [430] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [443] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [456] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" NA              \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [469] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [482] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [495] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [508] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [521] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [534] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [547] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt;  [560] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [573] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [586] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [599] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [612] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [625] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [638] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [651] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [664] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA             \n#&gt;  [677] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [690] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [703] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [716] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [729] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt;  [742] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [755] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt;  [768] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [781] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\"\n#&gt;  [794] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt;  [807] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [820] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [833] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [846] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [859] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt;  [872] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [885] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\"\n#&gt;  [898] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [911] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [924] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [937] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [950] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [963] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt;  [976] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt;  [989] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1002] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1015] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1028] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1041] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1054] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1067] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1080] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1093] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [1106] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1119] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt; [1132] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1145] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1158] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1171] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1184] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1197] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1210] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1223] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1236] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1249] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1262] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [1275] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1288] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1301] \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Half caff\"     \"Full caffeine\" NA              \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1314] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1327] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1340] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1353] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [1366] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1379] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt; [1392] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1405] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Decaf\"         \"Full caffeine\"\n#&gt; [1418] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1431] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1444] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1457] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1470] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1483] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1496] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1509] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1522] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1535] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1548] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt; [1561] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Half caff\"    \n#&gt; [1574] \"Full caffeine\" \"Full caffeine\" NA              NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\"\n#&gt; [1587] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\"\n#&gt; [1600] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt; [1613] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1626] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1639] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\"\n#&gt; [1652] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt; [1665] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1678] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1691] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1704] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\"\n#&gt; [1717] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1730] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [1743] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Decaf\"        \n#&gt; [1756] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1769] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1782] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1795] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\"\n#&gt; [1808] \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1821] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"        \n#&gt; [1834] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1847] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1860] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1873] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1886] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt; [1899] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1912] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1925] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"        \n#&gt; [1938] \"Full caffeine\" \"Decaf\"         \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1951] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1964] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1977] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [1990] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2003] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2016] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2029] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2042] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2055] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2068] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\"\n#&gt; [2081] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2094] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt; [2107] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2120] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2133] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2146] NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2159] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2172] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Decaf\"        \n#&gt; [2185] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"        \n#&gt; [2198] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2211] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2224] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2237] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2250] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2263] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2276] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2289] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2302] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2315] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2328] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2341] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2354] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2367] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2380] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\"\n#&gt; [2393] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"        \n#&gt; [2406] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              NA              \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt; [2419] \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" NA             \n#&gt; [2432] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2445] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2458] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt; [2471] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\"\n#&gt; [2484] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt; [2497] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2510] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2523] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2536] \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Half caff\"     NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2549] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2562] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" NA              \"Full caffeine\" NA              \"Full caffeine\"\n#&gt; [2575] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2588] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2601] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\"\n#&gt; [2614] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2627] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2640] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA             \n#&gt; [2653] NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\"\n#&gt; [2666] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" NA              \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"        \n#&gt; [2679] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2692] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2705] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2718] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [2731] \"Half caff\"     \"Decaf\"         \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2744] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              NA              NA              \"Full caffeine\" \"Full caffeine\"\n#&gt; [2757] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2770] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2783] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2796] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2809] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2822] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2835] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2848] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2861] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2874] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2887] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2900] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA             \n#&gt; [2913] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2926] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2939] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2952] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2965] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2978] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [2991] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3004] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3017] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"    \n#&gt; [3030] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3043] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3056] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3069] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3082] \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3095] NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3108] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3121] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3134] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3147] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3160] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt; [3173] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3186] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3199] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3212] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3225] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\"\n#&gt; [3238] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3251] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3264] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3277] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\"\n#&gt; [3290] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Decaf\"        \n#&gt; [3303] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3316] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3329] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3342] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\"\n#&gt; [3355] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3368] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3381] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3394] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3407] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"    \n#&gt; [3420] \"Full caffeine\" \"Half caff\"     NA              \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3433] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3446] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3459] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3472] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\"\n#&gt; [3485] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" NA              \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3498] \"Half caff\"     \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3511] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [3524] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Decaf\"         \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3537] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3550] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3563] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3576] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3589] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA             \n#&gt; [3602] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3615] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3628] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [3641] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     NA              \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3654] \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Full caffeine\"\n#&gt; [3667] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3680] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [3693] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Decaf\"         NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3706] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA             \n#&gt; [3719] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"    \n#&gt; [3732] \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\"\n#&gt; [3745] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3758] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3771] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3784] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3797] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3810] \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3823] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3836] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3849] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3862] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3875] NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\"\n#&gt; [3888] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3901] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3914] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3927] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3940] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3953] NA              NA              \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3966] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [3979] \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Half caff\"    \n#&gt; [3992] \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Decaf\"         \"Full caffeine\" \"Full caffeine\" \"Half caff\"     NA             \n#&gt; [4005] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" NA              \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [4018] \"Full caffeine\" \"Full caffeine\" \"Decaf\"         \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n#&gt; [4031] \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Half caff\"     \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\" \"Full caffeine\"\n\n\n#&gt;    [1] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    \"Yes\"\n#&gt;   [36] NA    NA    NA    NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA   \n#&gt;   [71] \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [106] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [141] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\"\n#&gt;  [176] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [211] \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    NA    \"Yes\" NA    NA    NA    NA    NA    \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt;  [246] NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [281] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [316] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [351] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [386] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [421] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [456] \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"No\"  NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [491] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [526] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\"\n#&gt;  [561] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [596] \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [631] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [666] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"No\" \n#&gt;  [701] \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [736] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [771] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [806] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [841] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [876] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt;  [911] \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [946] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt;  [981] \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1016] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1051] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\"\n#&gt; [1086] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1121] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1156] \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1191] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\"\n#&gt; [1226] \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1261] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1296] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1331] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1366] \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1401] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1436] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1471] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1506] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1541] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1576] NA    NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1611] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1646] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1681] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\" \n#&gt; [1716] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"No\"  \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\" \n#&gt; [1751] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1786] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" NA    NA    \"Yes\" \"No\"  \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1821] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1856] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt; [1891] \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1926] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1961] NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [1996] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt; [2031] \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2066] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2101] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2136] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"No\"  \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt; [2171] \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"No\" \n#&gt; [2206] \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2241] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2276] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA   \n#&gt; [2311] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2346] \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2381] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA   \n#&gt; [2416] \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"No\"  NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2451] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2486] \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA   \n#&gt; [2521] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"No\"  NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\"\n#&gt; [2556] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA   \n#&gt; [2591] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2626] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"No\"  NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2661] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2696] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2731] \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    NA    NA    \"Yes\" \"Yes\" NA    NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2766] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2801] \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2836] \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\"\n#&gt; [2871] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2906] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2941] \"Yes\" NA    \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [2976] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt; [3011] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3046] \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3081] \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3116] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3151] \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3186] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\"\n#&gt; [3221] \"Yes\" NA    \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3256] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3291] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\"\n#&gt; [3326] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\" \n#&gt; [3361] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA   \n#&gt; [3396] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" NA    NA    NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3431] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3466] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3501] \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3536] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3571] \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3606] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA   \n#&gt; [3641] \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3676] \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\"\n#&gt; [3711] NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    NA    \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\"\n#&gt; [3746] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3781] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\"\n#&gt; [3816] \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\"\n#&gt; [3851] \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\"\n#&gt; [3886] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" NA   \n#&gt; [3921] NA    \"No\"  NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"No\"  \"Yes\" \"Yes\" NA    \"Yes\" NA    \"Yes\"\n#&gt; [3956] NA    \"Yes\" \"Yes\" NA    NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [3991] \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    NA    NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\"\n#&gt; [4026] \"Yes\" \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\" NA    \"No\"  \"Yes\" NA    \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"Yes\""
  },
  {
    "objectID": "lectures/3.html#cross-tabulations",
    "href": "lectures/3.html#cross-tabulations",
    "title": "Categorical Data",
    "section": "Cross Tabulations",
    "text": "Cross Tabulations\n\ncat_stats(coffee$caffeine, coffee$taste)"
  },
  {
    "objectID": "lectures/3.html#table-proportions",
    "href": "lectures/3.html#table-proportions",
    "title": "Categorical Data",
    "section": "Table Proportions",
    "text": "Table Proportions\n\ncat_stats(coffee$caffeine, coffee$taste, prop = \"table\")"
  },
  {
    "objectID": "lectures/3.html#row-proportions",
    "href": "lectures/3.html#row-proportions",
    "title": "Categorical Data",
    "section": "Row Proportions",
    "text": "Row Proportions\n\ncat_stats(coffee$caffeine, coffee$taste, prop = \"row\")"
  },
  {
    "objectID": "lectures/3.html#column-proportions",
    "href": "lectures/3.html#column-proportions",
    "title": "Categorical Data",
    "section": "Column Proportions",
    "text": "Column Proportions\n\ncat_stats(coffee$caffeine, coffee$taste, prop = \"col\")"
  },
  {
    "objectID": "lectures/3.html#stacked-bar-plot-in-r",
    "href": "lectures/3.html#stacked-bar-plot-in-r",
    "title": "Categorical Data",
    "section": "Stacked Bar Plot in R",
    "text": "Stacked Bar Plot in R\n\nggplot(DATA, aes(x = VAR1, y = after_stat(count), fill = VAR2)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#stacked-bar-plot-in-r-1",
    "href": "lectures/3.html#stacked-bar-plot-in-r-1",
    "title": "Categorical Data",
    "section": "Stacked Bar Plot in R",
    "text": "Stacked Bar Plot in R\n\nggplot(coffee, aes(x = caffeine, y = after_stat(count), fill = taste)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#stacked-bar-plot-in-r-2",
    "href": "lectures/3.html#stacked-bar-plot-in-r-2",
    "title": "Categorical Data",
    "section": "Stacked Bar Plot in R",
    "text": "Stacked Bar Plot in R\n\nggplot(coffee, aes(y = caffeine, x = after_stat(count), fill = taste)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#pie-charts",
    "href": "lectures/3.html#pie-charts",
    "title": "Categorical Data",
    "section": "Pie Charts",
    "text": "Pie Charts\nPie charts are useful methods to show how a categories are related to each other in a sample."
  },
  {
    "objectID": "lectures/3.html#pie-chart-in-r",
    "href": "lectures/3.html#pie-chart-in-r",
    "title": "Categorical Data",
    "section": "Pie Chart in R",
    "text": "Pie Chart in R\n\ndf_pie &lt;- cat_stats(DATA$VARIABLE, pie = TRUE)\nggplot(df_pie, aes(cat = Category, val = n, fill = Category)) +\n  geom_pie()"
  },
  {
    "objectID": "lectures/3.html#pie-chart-in-r-1",
    "href": "lectures/3.html#pie-chart-in-r-1",
    "title": "Categorical Data",
    "section": "Pie Chart in R",
    "text": "Pie Chart in R\n\ncoffee_pie &lt;- cat_stats(coffee$caffeine, pie = TRUE)\nggplot(coffee_pie, aes(cat = Category, val = n, fill = Category)) +\n  geom_pie()"
  },
  {
    "objectID": "lectures/3.html#mosiac-plots",
    "href": "lectures/3.html#mosiac-plots",
    "title": "Categorical Data",
    "section": "Mosiac Plots",
    "text": "Mosiac Plots\nMosiac Plots are usful in visualizing two categorical variables with its relative proportions.\n\n\n#&gt; Warning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\n\n\n#&gt; Warning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\n#&gt; ℹ Please use the `transform` argument instead.\n\n\n#&gt; Warning: `unite_()` was deprecated in tidyr 1.2.0.\n#&gt; ℹ Please use `unite()` instead.\n#&gt; ℹ The deprecated feature was likely used in the ggmosaic package.\n#&gt;   Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;."
  },
  {
    "objectID": "lectures/3.html#mosiac-plots-in-r",
    "href": "lectures/3.html#mosiac-plots-in-r",
    "title": "Categorical Data",
    "section": "Mosiac Plots in R",
    "text": "Mosiac Plots in R\n\nggplot(DATA) +\n  geom_mosaic(aes(x = product(VARIABLE), fill = VARIABLE))"
  },
  {
    "objectID": "lectures/3.html#mosiac-plots-in-r-1",
    "href": "lectures/3.html#mosiac-plots-in-r-1",
    "title": "Categorical Data",
    "section": "Mosiac Plots in R",
    "text": "Mosiac Plots in R\n\nggplot(coffee) +\n  geom_mosaic(aes(x = product(caffeine, taste), fill = taste))"
  },
  {
    "objectID": "lectures/3.html#waffle-charts",
    "href": "lectures/3.html#waffle-charts",
    "title": "Categorical Data",
    "section": "Waffle Charts",
    "text": "Waffle Charts"
  },
  {
    "objectID": "lectures/3.html#waffle-charts-1",
    "href": "lectures/3.html#waffle-charts-1",
    "title": "Categorical Data",
    "section": "Waffle Charts",
    "text": "Waffle Charts\nWaffle Charts is similar to the pie charts where you can visualize the proportions."
  },
  {
    "objectID": "lectures/3.html#waffle-charts-in-r",
    "href": "lectures/3.html#waffle-charts-in-r",
    "title": "Categorical Data",
    "section": "Waffle Charts in R",
    "text": "Waffle Charts in R\n\ndf_pie &lt;- cat_stats(DATA$VARIABLE, pie = TRUE)\nggplot(df_pie, aes(cat = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE)"
  },
  {
    "objectID": "lectures/3.html#waffle-charts-in-r-1",
    "href": "lectures/3.html#waffle-charts-in-r-1",
    "title": "Categorical Data",
    "section": "Waffle Charts in R",
    "text": "Waffle Charts in R\n\ncoffee_pie &lt;- cat_stats(coffee$caffeine, pie = TRUE)\nggplot(coffee_pie, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE)"
  },
  {
    "objectID": "lectures/3.html#themes",
    "href": "lectures/3.html#themes",
    "title": "Categorical Data",
    "section": "Themes",
    "text": "Themes\nThe R packages ThemePark and ggthemes allows you to change the overall look of a plot.\n\nAll you need to do is add the theme to the plot."
  },
  {
    "objectID": "lectures/2.html#research-question",
    "href": "lectures/2.html#research-question",
    "title": "Research Study",
    "section": "Research Question",
    "text": "Research Question\nA research question is designed to create new knowledge of certain phenomenons observed in the world.\n\nThis is designed by conducting a research projects that systematically answers the question.\n\n\nThe study design is the procedure in which data is collected to answer the question, while reducing any potential bias while conducting the study."
  },
  {
    "objectID": "lectures/2.html#case-study-epidemiological-model-in-politics",
    "href": "lectures/2.html#case-study-epidemiological-model-in-politics",
    "title": "Research Study",
    "section": "Case Study: Epidemiological Model in Politics",
    "text": "Case Study: Epidemiological Model in Politics\nAn Epidemiological Math Model Approach to a Political System with Three Parties"
  },
  {
    "objectID": "lectures/2.html#study-design-1",
    "href": "lectures/2.html#study-design-1",
    "title": "Research Study",
    "section": "Study Design",
    "text": "Study Design\nThis is the procedure designed to answer a research question.\n\nEntails procedures to collect data that answers a research question.\n\n\nDictates how the data will be analyzed.\n\n\nDetermines how the data is quanitified and what is the experimental unit."
  },
  {
    "objectID": "lectures/2.html#sampling-1",
    "href": "lectures/2.html#sampling-1",
    "title": "Research Study",
    "section": "Sampling",
    "text": "Sampling\nSampling is the process of selecting units from a population interest to collect data from.\n\nImage Provided by Simply Psychology."
  },
  {
    "objectID": "lectures/2.html#sampling-2",
    "href": "lectures/2.html#sampling-2",
    "title": "Research Study",
    "section": "Sampling",
    "text": "Sampling\nWhen Sampling you want to maintain these properties:\n\nRepresentative sample\nLarge enough sample size"
  },
  {
    "objectID": "lectures/2.html#sampling-example",
    "href": "lectures/2.html#sampling-example",
    "title": "Research Study",
    "section": "Sampling Example",
    "text": "Sampling Example\nWe want to answer the question, are people happy?"
  },
  {
    "objectID": "lectures/2.html#random-sampling",
    "href": "lectures/2.html#random-sampling",
    "title": "Research Study",
    "section": "Random Sampling",
    "text": "Random Sampling\nWhen sampling, we strive for random sampling\n\nEach unit in the population of interest must have an equal probability of being selected for the study.\nThis ensures a representative population"
  },
  {
    "objectID": "lectures/2.html#independent-sampling",
    "href": "lectures/2.html#independent-sampling",
    "title": "Research Study",
    "section": "Independent Sampling",
    "text": "Independent Sampling\nIn addition to random sampling, we strive to make sure each unit is independent from each other.\n\nThe probability of UNIT A being sampled will not affect the probability of UNIT B to be sampled."
  },
  {
    "objectID": "lectures/2.html#sampling-variation",
    "href": "lectures/2.html#sampling-variation",
    "title": "Research Study",
    "section": "Sampling Variation",
    "text": "Sampling Variation\n\nRandom samples may vary from the population of interest.\nDue to randomness, samples many not look the or biased.\nHowever, this is to be expected as because the sample will not be biased in one way or another.\nSamples are then considered unbiased as long as experimental units were collected randomly."
  },
  {
    "objectID": "lectures/2.html#data",
    "href": "lectures/2.html#data",
    "title": "Research Study",
    "section": "Data",
    "text": "Data\nThe measurement collected from an experimental unit."
  },
  {
    "objectID": "lectures/2.html#data-distribution-1",
    "href": "lectures/2.html#data-distribution-1",
    "title": "Research Study",
    "section": "Data Distribution",
    "text": "Data Distribution\nWhen thinking about data, we know an attribute is allowed to vary. With this variation, some numbers are more likely to be observed than others."
  },
  {
    "objectID": "lectures/2.html#forest-from-the-trees",
    "href": "lectures/2.html#forest-from-the-trees",
    "title": "Research Study",
    "section": "Forest from the trees",
    "text": "Forest from the trees\n\nImamge Provided by Fine Art America."
  },
  {
    "objectID": "lectures/2.html#sample",
    "href": "lectures/2.html#sample",
    "title": "Research Study",
    "section": "Sample",
    "text": "Sample\n\nWhen inspecting data:\n\nDo not focus on one individual data point.\nSee how data points interact with each other\nSee what is common\nSee what is rare"
  },
  {
    "objectID": "lectures/2.html#data-generation-process-dgp",
    "href": "lectures/2.html#data-generation-process-dgp",
    "title": "Research Study",
    "section": "Data Generation Process (DGP)",
    "text": "Data Generation Process (DGP)\nThe data generation process is understanding how variation from the population is transferred to the data collected.\n\nA population has a mechanism to produce data, understanding this mechanism is essential understanding the data."
  },
  {
    "objectID": "lectures/2.html#dgp",
    "href": "lectures/2.html#dgp",
    "title": "Research Study",
    "section": "DGP",
    "text": "DGP\nA populations DGP can be defined with the following characteristics:\n\nThe potential outcomes that can be observed when measuring\nEach potential outcome will have a probability of being observed\n\nThe probability must be between 0 and 1\n\nSum of all the probabilities of each outcome will add up to 1"
  },
  {
    "objectID": "lectures/2.html#flipping-a-coin",
    "href": "lectures/2.html#flipping-a-coin",
    "title": "Research Study",
    "section": "Flipping a Coin",
    "text": "Flipping a Coin\nFlipping a coin results in either heads or tail. The probability for heads is 50%\n\nThe DGP of flipping a coin is the process of selecting an outcome, given the probability of both options are 50%."
  },
  {
    "objectID": "lectures/2.html#rolling-a-die",
    "href": "lectures/2.html#rolling-a-die",
    "title": "Research Study",
    "section": "Rolling a die",
    "text": "Rolling a die"
  },
  {
    "objectID": "lectures/2.html#measuring-body-temperature",
    "href": "lectures/2.html#measuring-body-temperature",
    "title": "Research Study",
    "section": "Measuring Body Temperature",
    "text": "Measuring Body Temperature\n\n\n\n\n\nImage Provided by Thermofisher Scientific."
  },
  {
    "objectID": "lectures/2.html#inference",
    "href": "lectures/2.html#inference",
    "title": "Research Study",
    "section": "Inference",
    "text": "Inference\nHow do we use DGP and sampling to understand the world?\n\nWe can use a sample to understand the DGP.\n\n\nWe can use the DGP to understand the sample."
  },
  {
    "objectID": "lectures/2.html#example",
    "href": "lectures/2.html#example",
    "title": "Research Study",
    "section": "Example",
    "text": "Example\n\n\n#&gt; Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(density)` instead."
  },
  {
    "objectID": "lectures/12.html#motivating-example-1",
    "href": "lectures/12.html#motivating-example-1",
    "title": "Inference with Linear Regression",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\np1 &lt;- penguins |&gt; ggplot(aes(x=species, y = body_mass_g)) +\n  geom_jitter() + \n  geom_boxplot() + \n  labs(x = \"Species\", y = \"Body Mass\")\n  \np2 &lt;- penguins |&gt; ggplot(aes(x=flipper_length_mm, y = body_mass_g)) +\n  geom_point() + \n  labs(x = \"Flipper Length\", y = \"Body Mass\")  \n\np1 + p2"
  },
  {
    "objectID": "lectures/12.html#mathematical-models-1",
    "href": "lectures/12.html#mathematical-models-1",
    "title": "Inference with Linear Regression",
    "section": "Mathematical Models",
    "text": "Mathematical Models"
  },
  {
    "objectID": "lectures/12.html#standard-normal-distribution",
    "href": "lectures/12.html#standard-normal-distribution",
    "title": "Inference with Linear Regression",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\n\n\\[\n{\\frac{1}{\\sqrt{2 \\pi}}} e^{-\\frac{1}{2}x^2}\n\\]\n\n\n\nCode\ndata.frame(x = seq(-5,5, length.out = 100), \n           y1 = dt(seq(-5,5, length.out = 100), 1),\n           y2 = dt(seq(-5,5, length.out = 100), 10),\n           y3 = dt(seq(-5,5, length.out = 100), 30),\n           y4 = dt(seq(-5,5, length.out = 100), 100),\n           y5 = dnorm((seq(-5,5, length.out = 100)))) |&gt; \n  ggplot() +\n  # geom_line(aes(x, y1, color = \"1\")) +\n  # geom_line(aes(x, y2, color = \"10\")) +\n  # geom_line(aes(x, y3, color = \"30\")) +\n  # geom_line(aes(x, y4, color = \"100\")) +\n  geom_line(aes(x, y5)) +\n  ylab(\"y\")"
  },
  {
    "objectID": "lectures/12.html#t-distribution",
    "href": "lectures/12.html#t-distribution",
    "title": "Inference with Linear Regression",
    "section": "t Distribution",
    "text": "t Distribution\n\n\n\\[\n\\frac{\\Gamma \\left(\\frac{v+1}{2}\\right)}{\\sqrt{\\pi v}\\Gamma\\left(\\frac{v}{2}\\right)} \\left(1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\n\\]\n\n\n\nCode\ndata.frame(x = seq(-5,5, length.out = 100), \n           y1 = dt(seq(-5,5, length.out = 100), 1),\n           y2 = dt(seq(-5,5, length.out = 100), 10),\n           y3 = dt(seq(-5,5, length.out = 100), 30),\n           y4 = dt(seq(-5,5, length.out = 100), 100),\n           y5 = dnorm((seq(-5,5, length.out = 100)))) |&gt; \n  ggplot() +\n  geom_line(aes(x, y1, color = \"1\")) +\n  geom_line(aes(x, y2, color = \"10\")) +\n  geom_line(aes(x, y3, color = \"30\")) +\n  geom_line(aes(x, y4, color = \"100\")) +\n  geom_line(aes(x, y5, color = \"Normal\")) +\n  ylab(\"y\")"
  },
  {
    "objectID": "lectures/12.html#hypothesis",
    "href": "lectures/12.html#hypothesis",
    "title": "Inference with Linear Regression",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/12.html#testing-beta_j",
    "href": "lectures/12.html#testing-beta_j",
    "title": "Inference with Linear Regression",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\nT = \\frac{\\hat\\beta_j-\\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]\n\n\\(n\\): sample size\n\\(p^\\prime\\): number of \\(\\beta\\)s"
  },
  {
    "objectID": "lectures/12.html#p-value",
    "href": "lectures/12.html#p-value",
    "title": "Inference with Linear Regression",
    "section": "P-Value",
    "text": "P-Value\n\n\n\nAlternative Hypothesis\np-value\n\n\n\n\n\\(\\beta&gt;\\theta\\)\n\\(P(\\hat\\beta &gt;T)=p\\)\n\n\n\\(\\beta&lt;\\theta\\)\n\\(P(\\hat\\beta &lt; T)=p\\)\n\n\n\\(\\beta\\ne\\theta\\)\n\\(2\\times P(\\hat\\beta &gt;|T|)=p\\)"
  },
  {
    "objectID": "lectures/12.html#confidence-intervals",
    "href": "lectures/12.html#confidence-intervals",
    "title": "Inference with Linear Regression",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\n\\hat \\beta_j \\pm CV \\times se(\\hat\\beta_j)\n\\]\n\n\\(CV\\): Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\n\\(se\\): Standard Error Function"
  },
  {
    "objectID": "lectures/12.html#conducting-ht-of-beta_j",
    "href": "lectures/12.html#conducting-ht-of-beta_j",
    "title": "Inference with Linear Regression",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)"
  },
  {
    "objectID": "lectures/12.html#example",
    "href": "lectures/12.html#example",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass_g ~ species + flipper_length_mm, penguins)\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -898.8 -252.0  -24.8  229.8 1191.6 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -4013.18     586.25  -6.846 3.74e-11 ***\n#&gt; speciesChinstrap   -205.38      57.57  -3.568 0.000414 ***\n#&gt; speciesGentoo       284.52      95.43   2.981 0.003083 ** \n#&gt; flipper_length_mm    40.61       3.08  13.186  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 373.3 on 329 degrees of freedom\n#&gt; Multiple R-squared:  0.787,  Adjusted R-squared:  0.7851 \n#&gt; F-statistic: 405.3 on 3 and 329 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/12.html#confidence-interval",
    "href": "lectures/12.html#confidence-interval",
    "title": "Inference with Linear Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/12.html#example-1",
    "href": "lectures/12.html#example-1",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.90)\n\n\n#&gt;                           5 %        95 %\n#&gt; (Intercept)       -4980.19064 -3046.16714\n#&gt; speciesChinstrap   -300.33123  -110.41973\n#&gt; speciesGentoo       127.11143   441.93578\n#&gt; flipper_length_mm    35.52645    45.68588"
  },
  {
    "objectID": "lectures/12.html#model-inference",
    "href": "lectures/12.html#model-inference",
    "title": "Inference with Linear Regression",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(\\hat Y=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/12.html#model-inference-1",
    "href": "lectures/12.html#model-inference-1",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven 2 models:\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\n\\hat Y = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/12.html#anova",
    "href": "lectures/12.html#anova",
    "title": "Inference with Linear Regression",
    "section": "ANOVA",
    "text": "ANOVA"
  },
  {
    "objectID": "lectures/12.html#anova-table",
    "href": "lectures/12.html#anova-table",
    "title": "Inference with Linear Regression",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nModel\n\\(DFR=k-1\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSM}{DFR}\\)\n\\(\\hat F=\\frac{MSR}{MSE}\\)\n\n\nError\n\\(DFE=n-k\\)\n\\(SSE\\)\n\\(MSE=\\frac{SSE}{DFE}\\)\n\n\n\nTotal\n\\(TDF=n-1\\)\n\\(TSS=SSR+SSE\\)\n\n\n\n\n\n\\[\n\\hat F \\sim F(DFR, DFE)\n\\]"
  },
  {
    "objectID": "lectures/12.html#conducting-an-anova-in-r",
    "href": "lectures/12.html#conducting-an-anova-in-r",
    "title": "Inference with Linear Regression",
    "section": "Conducting an ANOVA in R",
    "text": "Conducting an ANOVA in R\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)"
  },
  {
    "objectID": "lectures/12.html#example-2",
    "href": "lectures/12.html#example-2",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -898.8 -252.0  -24.8  229.8 1191.6 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -4013.18     586.25  -6.846 3.74e-11 ***\n#&gt; speciesChinstrap   -205.38      57.57  -3.568 0.000414 ***\n#&gt; speciesGentoo       284.52      95.43   2.981 0.003083 ** \n#&gt; flipper_length_mm    40.61       3.08  13.186  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 373.3 on 329 degrees of freedom\n#&gt; Multiple R-squared:  0.787,  Adjusted R-squared:  0.7851 \n#&gt; F-statistic: 405.3 on 3 and 329 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/12.html#model-inference-2",
    "href": "lectures/12.html#model-inference-2",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/12.html#model-inference-3",
    "href": "lectures/12.html#model-inference-3",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ \\hat y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ \\hat y = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/12.html#model-inference-4",
    "href": "lectures/12.html#model-inference-4",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/12.html#test-statistic",
    "href": "lectures/12.html#test-statistic",
    "title": "Inference with Linear Regression",
    "section": "Test Statistic",
    "text": "Test Statistic\n\\[\n\\hat F = \\frac{[SSE(RED) - SSE(FULL)]/[DFE(RED)-DFE(FULL)]}{MSE(FULL)}\n\\]\n\\[\n\\hat F \\sim F[DFE(RED) - DFE(FULL), DFE(FULL)]\n\\]"
  },
  {
    "objectID": "lectures/12.html#anova-in-r",
    "href": "lectures/12.html#anova-in-r",
    "title": "Inference with Linear Regression",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\n\nCode\nfull &lt;- lm(Y  ~  X1 + X2 + X3 + X4)\nred &lt;- lm(Y ~ X1 + X2)\nanova(red, full)"
  },
  {
    "objectID": "lectures/12.html#example-3",
    "href": "lectures/12.html#example-3",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass_g ~ species + island + flipper_length_mm, penguins)\nm2 &lt;- lm(body_mass_g ~ island + flipper_length_mm, penguins)\nanova(m2, m1)\n\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: body_mass_g ~ island + flipper_length_mm\n#&gt; Model 2: body_mass_g ~ species + island + flipper_length_mm\n#&gt;   Res.Df      RSS Df Sum of Sq      F    Pr(&gt;F)    \n#&gt; 1    329 47774435                                  \n#&gt; 2    327 45552857  2   2221579 7.9738 0.0004157 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/12.html#model-assumptions-1",
    "href": "lectures/12.html#model-assumptions-1",
    "title": "Inference with Linear Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nWhen we are conducting inference linear regression, we will have to check the following conditions:\n\nLinearity\nIndependence\nNormality\nEqual Variances\nMulticollinearity (for MLR)"
  },
  {
    "objectID": "lectures/12.html#linearity",
    "href": "lectures/12.html#linearity",
    "title": "Inference with Linear Regression",
    "section": "Linearity",
    "text": "Linearity\nProbably considered the most important assumption, but there must be a linear relationship between both the outcome variable (y) and a set of predictors (\\(x_1\\), \\(x_2\\), …)."
  },
  {
    "objectID": "lectures/12.html#independence",
    "href": "lectures/12.html#independence",
    "title": "Inference with Linear Regression",
    "section": "Independence",
    "text": "Independence\nThe data points must not influence each other."
  },
  {
    "objectID": "lectures/12.html#normality",
    "href": "lectures/12.html#normality",
    "title": "Inference with Linear Regression",
    "section": "Normality",
    "text": "Normality\nThe model errors (also known as residuals) must follow a normal distribution."
  },
  {
    "objectID": "lectures/12.html#equal-variances",
    "href": "lectures/12.html#equal-variances",
    "title": "Inference with Linear Regression",
    "section": "Equal Variances",
    "text": "Equal Variances\nThe variability of the data points must be the same for all predictor values."
  },
  {
    "objectID": "lectures/12.html#residuals",
    "href": "lectures/12.html#residuals",
    "title": "Inference with Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nResiduals are the errors between the observed value and the estimated model. Common residuals include\n\nRaw Residual\nStandardized Residual\nJackknife (studentized) Residuals"
  },
  {
    "objectID": "lectures/12.html#influential-measurements",
    "href": "lectures/12.html#influential-measurements",
    "title": "Inference with Linear Regression",
    "section": "Influential Measurements",
    "text": "Influential Measurements\nInfluential measures are statistics that determine how much a data point affects the model. Common influential measures are\n\nLeverages\nCook’s Distance"
  },
  {
    "objectID": "lectures/12.html#raw-residuals",
    "href": "lectures/12.html#raw-residuals",
    "title": "Inference with Linear Regression",
    "section": "Raw Residuals",
    "text": "Raw Residuals\n\\[\n\\hat r_i = y_i - \\hat y_i\n\\]"
  },
  {
    "objectID": "lectures/12.html#residual-analysis",
    "href": "lectures/12.html#residual-analysis",
    "title": "Inference with Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to test the assumptions of linear regression."
  },
  {
    "objectID": "lectures/12.html#qq-plot",
    "href": "lectures/12.html#qq-plot",
    "title": "Inference with Linear Regression",
    "section": "QQ Plot",
    "text": "QQ Plot\nA qq (quantile-quantile) plot will plot the estimated quantiles of the residuals against the theoretical quantiles from a normal distribution function. If the points from the qq-plot lie on the \\(y=x\\) line, it is said that the residuals follow a normal distribution."
  },
  {
    "objectID": "lectures/12.html#residual-vs-fitted-plot",
    "href": "lectures/12.html#residual-vs-fitted-plot",
    "title": "Inference with Linear Regression",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\nThis plot allows you to assess the linearity, constant variance, and identify potential outliers. Create a scatter plot between the fitted values (x-axis) and the raw/standardized residuals (y-axis)."
  },
  {
    "objectID": "lectures/12.html#variance-inflation-factor",
    "href": "lectures/12.html#variance-inflation-factor",
    "title": "Inference with Linear Regression",
    "section": "Variance Inflation Factor",
    "text": "Variance Inflation Factor\nThe variance inflation factor is a measurement on how much variables are collinear with each other. A value greater than 10 is a cause for concern and action should be taken."
  },
  {
    "objectID": "lectures/12.html#residual-analysis-in-r",
    "href": "lectures/12.html#residual-analysis-in-r",
    "title": "Inference with Linear Regression",
    "section": "Residual Analysis in R",
    "text": "Residual Analysis in R\nUse the resid_df function to obtain the residuals of a model.\n\n\nCode\nrdf &lt;- resid_df(LM_OBJECT)"
  },
  {
    "objectID": "lectures/12.html#residual-vs-fitted-plot-1",
    "href": "lectures/12.html#residual-vs-fitted-plot-1",
    "title": "Inference with Linear Regression",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\n\n\nCode\nggplot(RDF, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")"
  },
  {
    "objectID": "lectures/12.html#qq-plot-1",
    "href": "lectures/12.html#qq-plot-1",
    "title": "Inference with Linear Regression",
    "section": "QQ Plot",
    "text": "QQ Plot\n\n\nCode\nggplot(RDF, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "lectures/12.html#example-4",
    "href": "lectures/12.html#example-4",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nxlm &lt;- lm(body_mass_g ~   island + species + flipper_length_mm,\n          penguins)\ndfxlm &lt;- resid_df(xlm)\n\nggplot(dfxlm, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n\nggplot(dfxlm, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "lectures/10.html#motivation-1",
    "href": "lectures/10.html#motivation-1",
    "title": "Mathematical Models",
    "section": "Motivation",
    "text": "Motivation\nThe bacteria data set contians information on whether bacteria (y: y or n) is present after utilizing treatments (ap: active or placebo).\n\nWe are interesting in determine the proportion of having bacteria present is different for those taking an “active” or “placebo”."
  },
  {
    "objectID": "lectures/10.html#comparing-proportions",
    "href": "lectures/10.html#comparing-proportions",
    "title": "Mathematical Models",
    "section": "Comparing Proportions",
    "text": "Comparing Proportions\nWe are interesting in determining if different groups see different proportions of a binary outcome.\nWe compute the proportions of observing the binary outcome in Group 1 and Group 2 and see if they are fundamentally different from each other."
  },
  {
    "objectID": "lectures/10.html#by-2-cross-tabulations",
    "href": "lectures/10.html#by-2-cross-tabulations",
    "title": "Mathematical Models",
    "section": "2 by 2 Cross Tabulations",
    "text": "2 by 2 Cross Tabulations\n\n\n\nGroups\nOutcome 1\nOutcome 2\n\n\nGroup 1\n\\(p_{11}\\)\n\\(p_{21}\\)\n\n\nGroup 2\n\\(p_{12}\\)\n\\(p_{22}\\)\n\n\n\n\nWe want to compare \\(p_{11}\\) and \\(p_{12}\\), to determine if the probability of outcome 1 are the same for both groups."
  },
  {
    "objectID": "lectures/10.html#test-statistic",
    "href": "lectures/10.html#test-statistic",
    "title": "Mathematical Models",
    "section": "Test Statistic",
    "text": "Test Statistic\nWe can use both \\(p_{11}\\) and \\(p_{12}\\) to determine if there is a fundamental difference.\n\nHowever, it will be more beneficial to utilize one statistic to contruct the sampling distribution.\n\n\n\\[\nT =  \\hat p_{11} - \\hat p_{12}\n\\]"
  },
  {
    "objectID": "lectures/10.html#obtain-proportions-in-r",
    "href": "lectures/10.html#obtain-proportions-in-r",
    "title": "Mathematical Models",
    "section": "Obtain Proportions in R",
    "text": "Obtain Proportions in R\n\n\nCode\nprops_df(DATA, GROUP, OUTCOME, VAL)"
  },
  {
    "objectID": "lectures/10.html#obtain-difference-in-r",
    "href": "lectures/10.html#obtain-difference-in-r",
    "title": "Mathematical Models",
    "section": "Obtain Difference in R",
    "text": "Obtain Difference in R\n\n\nCode\nprops_df(DATA, GROUP, OUTCOME, VAL, diff = TRUE)"
  },
  {
    "objectID": "lectures/10.html#bacteria-example",
    "href": "lectures/10.html#bacteria-example",
    "title": "Mathematical Models",
    "section": "Bacteria Example",
    "text": "Bacteria Example\n\n\nCode\nprops_df(bacteria, ap, y, \"y\")\n\n\n#&gt;    p1    p2 \n#&gt; 0.750 0.875"
  },
  {
    "objectID": "lectures/10.html#bacteria-example-1",
    "href": "lectures/10.html#bacteria-example-1",
    "title": "Mathematical Models",
    "section": "Bacteria Example",
    "text": "Bacteria Example\n\n\nCode\nprops_df(bacteria, ap, y, \"y\", TRUE)\n\n\n#&gt; [1] 0.125"
  },
  {
    "objectID": "lectures/10.html#hypotheis-test",
    "href": "lectures/10.html#hypotheis-test",
    "title": "Mathematical Models",
    "section": "Hypotheis Test",
    "text": "Hypotheis Test\nWe will test the following hypothesis:\n\\[\nH_0:\\ \\Delta =  p_1-p_2 = 0\n\\]\n\\[\nH_a:\\ \\Delta = p_1 - p_2 \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/10.html#normal-distribution-1",
    "href": "lectures/10.html#normal-distribution-1",
    "title": "Mathematical Models",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe Normal Distribution is a probability distribution that is symmetric, with most of the data points clustering around the mean.\n\nIt’s bell-shaped and is defined mathematically by two parameters:\n\nMean (\\(\\mu\\)): The center or peak of the distribution.\nStandard Deviation (\\(\\sigma\\)): Controls the spread of the distribution."
  },
  {
    "objectID": "lectures/10.html#normal-distribution-2",
    "href": "lectures/10.html#normal-distribution-2",
    "title": "Mathematical Models",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\ndata.frame(x = x, y = dnorm(x)) |&gt; \n  ggplot(aes(x, y)) +\n  geom_line()"
  },
  {
    "objectID": "lectures/10.html#properties-of-the-normal-distribution",
    "href": "lectures/10.html#properties-of-the-normal-distribution",
    "title": "Mathematical Models",
    "section": "Properties of the Normal Distribution",
    "text": "Properties of the Normal Distribution\n\n\nSymmetry: It is perfectly symmetric about the mean, meaning the left side is a mirror image of the right.\nUnimodal: There is a single peak at the mean.\nMean, Median, and Mode are Equal: In a normal distribution, these three measures of central tendency are located at the same point.\n68-95-99.7 Rule (Empirical Rule)"
  },
  {
    "objectID": "lectures/10.html#standard-normal-distribution",
    "href": "lectures/10.html#standard-normal-distribution",
    "title": "Mathematical Models",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\nThe Standard Normal Distribution is a special type of normal distribution with a mean of 0 and a standard deviation of 1. It’s often used as a reference to convert any normal distribution to a standard form."
  },
  {
    "objectID": "lectures/10.html#z-scores",
    "href": "lectures/10.html#z-scores",
    "title": "Mathematical Models",
    "section": "Z-Scores",
    "text": "Z-Scores\nA Z-score (or standard score) tells us how many standard deviations an individual data point is from the mean. It’s calculated as: \\(Z = \\frac{X - \\mu}{\\sigma}\\)\n\nIf \\(Z\\) is positive, the data point is above the mean.\nIf \\(Z\\) is negative, the data point is below the mean.\nUsing Z-scores, we can compare values across different normal distributions or find the probability associated with a particular score."
  },
  {
    "objectID": "lectures/10.html#empirical-rule",
    "href": "lectures/10.html#empirical-rule",
    "title": "Mathematical Models",
    "section": "Empirical Rule",
    "text": "Empirical Rule\nThe Empirical Rule provides a way to understand the spread of data in a normal distribution by describing how data points cluster around the mean. According to this rule:\n\nApproximately 68% of data points fall within one standard deviation of the mean.\nApproximately 95% of data points fall within two standard deviations of the mean.\nApproximately 99.7% of data points fall within three standard deviations of the mean."
  },
  {
    "objectID": "lectures/10.html#empirical-rule-and-normal-distribution",
    "href": "lectures/10.html#empirical-rule-and-normal-distribution",
    "title": "Mathematical Models",
    "section": "Empirical Rule and Normal Distribution",
    "text": "Empirical Rule and Normal Distribution\nIn a normal distribution:\n\n68% of data lies between \\((\\mu - \\sigma)\\) and \\((\\mu + \\sigma)\\).\n95% of data lies between \\((\\mu - 2\\sigma)\\) and \\((\\mu + 2\\sigma)\\).\n99.7% of data lies between \\((\\mu - 3\\sigma)\\) and \\((\\mu + 3\\sigma)\\).\n\nThese intervals allow us to estimate probabilities for data within each range without needing to calculate exact probabilities."
  },
  {
    "objectID": "lectures/10.html#visualizing-the-empirical-rule",
    "href": "lectures/10.html#visualizing-the-empirical-rule",
    "title": "Mathematical Models",
    "section": "Visualizing the Empirical Rule",
    "text": "Visualizing the Empirical Rule\n\nThe 68% region represents the middle of the curve, starting one standard deviation left of the mean and ending one standard deviation right.\nThe 95% region stretches further out, covering almost the entire curve except for the outer tails.\nThe 99.7% region includes nearly all data points, covering the entire curve except for a tiny fraction at each extreme."
  },
  {
    "objectID": "lectures/10.html#empirical-rule-1",
    "href": "lectures/10.html#empirical-rule-1",
    "title": "Mathematical Models",
    "section": "Empirical Rule",
    "text": "Empirical Rule"
  },
  {
    "objectID": "lectures/10.html#central-limit-theorem-1",
    "href": "lectures/10.html#central-limit-theorem-1",
    "title": "Mathematical Models",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nWhat does the Central Limit Theorem (CLT) actually tell us?\n\nThe CLT states that:\n\n\nIf you take a sufficiently large number of samples from any population, the distribution of the sample means will be approximately normal.\nThis approximation holds no matter the shape of the original population distribution (it could be normal, skewed, bimodal, etc.).\nThe mean of this sampling distribution will be equal to the population mean.\nThe standard deviation of this sampling distribution (often called the standard error) will be the population standard deviation divided by the square root of the sample size, ( _{} = )."
  },
  {
    "objectID": "lectures/10.html#why-the-central-limit-theorem-is-important",
    "href": "lectures/10.html#why-the-central-limit-theorem-is-important",
    "title": "Mathematical Models",
    "section": "Why the Central Limit Theorem is Important",
    "text": "Why the Central Limit Theorem is Important\nThe CLT is powerful for several reasons:\n\n\nPredicting Sample Outcomes: Since we know the distribution of sample means will be approximately normal, we can make predictions about future samples.\nConfidence Intervals and Hypothesis Testing: The normal distribution of sample means allows us to estimate population parameters (like the mean) with confidence and test hypotheses even if the population itself isn’t normally distributed.\nPractical Applications in Various Fields: From quality control in manufacturing to political polling and medicine, the CLT lets us make inferences about population characteristics based on sample data."
  },
  {
    "objectID": "lectures/10.html#using-mathematical-models",
    "href": "lectures/10.html#using-mathematical-models",
    "title": "Mathematical Models",
    "section": "Using Mathematical Models",
    "text": "Using Mathematical Models\nThe inferential procedures, such as computing the p-value or confidence interval, can be constructed with mathematical models."
  },
  {
    "objectID": "lectures/10.html#advantages",
    "href": "lectures/10.html#advantages",
    "title": "Mathematical Models",
    "section": "Advantages",
    "text": "Advantages\n\n\nMathematical Models have been widely studied and implemented in several software packages.\nThese are not computationally intensive\nCan work with smaller data set and provide more reliable results"
  },
  {
    "objectID": "lectures/10.html#disadvantages",
    "href": "lectures/10.html#disadvantages",
    "title": "Mathematical Models",
    "section": "Disadvantages",
    "text": "Disadvantages\n\n\nIf the mathematical model is wrong, results may be invalid.\nDifficult to know what are the true mathematical models."
  },
  {
    "objectID": "lectures/10.html#inference-in-r-1",
    "href": "lectures/10.html#inference-in-r-1",
    "title": "Mathematical Models",
    "section": "Inference in R",
    "text": "Inference in R\n\\[\nH_0:\\ \\Delta =  p_1-p_2 = 0\n\\]\n\\[\nH_a:\\ \\Delta = p_1 - p_2 \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/10.html#proportions-test",
    "href": "lectures/10.html#proportions-test",
    "title": "Mathematical Models",
    "section": "Proportions Test",
    "text": "Proportions Test\n\n\nCode\nprop.test(x = c(CAT_1, CAT_2), n = c(N_1, N_2))"
  },
  {
    "objectID": "lectures/10.html#descriptives",
    "href": "lectures/10.html#descriptives",
    "title": "Mathematical Models",
    "section": "Descriptives",
    "text": "Descriptives\n\n\nCode\ncat_stats(bacteria$ap, bacteria$y)"
  },
  {
    "objectID": "lectures/10.html#proportions-test-in-r",
    "href": "lectures/10.html#proportions-test-in-r",
    "title": "Mathematical Models",
    "section": "Proportions Test in R",
    "text": "Proportions Test in R\n\n\nCode\nprop.test(x = c(93, 84), n = c(124, 96))\n\n\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  c(93, 84) out of c(124, 96)\n#&gt; X-squared = 4.6109, df = 1, p-value = 0.03177\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.23516294 -0.01483706\n#&gt; sample estimates:\n#&gt; prop 1 prop 2 \n#&gt;  0.750  0.875"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Categorical Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariable Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumbers & Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumbers & Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandomization Tests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bootstrap Method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ec/ec_4.html",
    "href": "ec/ec_4.html",
    "title": "Extra Credit 4",
    "section": "",
    "text": "Provide a summary of one book below, an write a brief analysis in supporting or opposing the book, and connect key elements of the book to your every day life. Some of these books are available through the Broome Library.\nBooks:\n\nThe Book of Why\n\nJudea Pearl\n\nAlgorithms of Oppression\n\nSafiya Umoja Noble\n\nData Feminism\n\nCatherine D’Ignazio and Lauren Klein\n\nWeapons of Math Destruction\n\nCathy O’Niel\n\nInvisible women : data bias in a world designed for men\n\nCaroline Criado Perez\n\nFactfulness: Ten Reasons We’re Wrong About the World… and Why Things are Better Than You Think\n\nHans Rosling\n\nArtificial Unintelligence: How Computers Misunderstand the World\n\nMerideth Broussard\n\nTechnically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech\n\nSara Wachter-Boettcher\n\nOR ANY Book Approved By Me (Deadline for Approval by NOV 15, 2024)\n\nReport guidelines\n\n4-5 Pages\nMust include a title page\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 5/13/2024\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "ec/ec_2.html",
    "href": "ec/ec_2.html",
    "title": "Extra Credit 2",
    "section": "",
    "text": "Provide a summary of one book below, an write a brief analysis in supporting or opposing the book, and connect key elements of the book to your every day life. All books should be available through the Broome Library.\nBooks:\n\nMake it Stick: The Science of Successful Learning\n\nPeter Brown\n\nTeach Yourself How to Learn: Strategies you can use to ace any course\n\nSaundra McGuire\n\n\nReport guidelines f - 4-5 Pages\n\nMust include a title page\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 5/13/2024\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "c/1a.html#slides",
    "href": "c/1a.html#slides",
    "title": "Welcome!",
    "section": "Slides",
    "text": "Slides\nm201.inqs.info/c/1a"
  },
  {
    "objectID": "c/1a.html#learning-outcomes",
    "href": "c/1a.html#learning-outcomes",
    "title": "Welcome!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIntroductions\nEmbedded Peer Educator\nSyllabus\nImportant Announcements\nStatistics Activity\nAccess to Coursekata"
  },
  {
    "objectID": "c/1a.html#introductions-1",
    "href": "c/1a.html#introductions-1",
    "title": "Welcome!",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "c/1a.html#introductions-2",
    "href": "c/1a.html#introductions-2",
    "title": "Welcome!",
    "section": "Introductions",
    "text": "Introductions\n\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "c/1a.html#epe---louis-heer",
    "href": "c/1a.html#epe---louis-heer",
    "title": "Welcome!",
    "section": "EPE - Louis Heer",
    "text": "EPE - Louis Heer\n\n\n\n\n\n3rd Year\nPolitical Science Major\nCourse\nElementary Statistics\nStatistics for Biologists\nStatistics Application for Political Science"
  },
  {
    "objectID": "c/1a.html#early-succes-module-esm",
    "href": "c/1a.html#early-succes-module-esm",
    "title": "Welcome!",
    "section": "Early Succes Module (ESM)",
    "text": "Early Succes Module (ESM)"
  },
  {
    "objectID": "c/1a.html#assignments",
    "href": "c/1a.html#assignments",
    "title": "Welcome!",
    "section": "Assignments",
    "text": "Assignments\n\n\n\nAttendance\nSelfie at profesor’s door and Introduction\nSelfie at the LRC\nLearner Info Survey\nEvidence of a Study Group\n\n\n\nBloom’s Taxonomy\nUsing the Study Cycle Efficiently\nSelfie at favorite place\nSelfie with support staff\nThe Feynman Technique"
  },
  {
    "objectID": "c/1a.html#class-setup-1",
    "href": "c/1a.html#class-setup-1",
    "title": "Welcome!",
    "section": "Class Setup",
    "text": "Class Setup\n\nParticipation\nReading Assignments\nNotebook Assignments\nVideo Assignments\nExams\nExtra Credit"
  },
  {
    "objectID": "c/1a.html#study-expectations.",
    "href": "c/1a.html#study-expectations.",
    "title": "Welcome!",
    "section": "Study Expectations.",
    "text": "Study Expectations.\nFor every 1 unit of class you take, you are expected to spend ~3 hours outside of class for the course."
  },
  {
    "objectID": "c/1a.html#syllabus",
    "href": "c/1a.html#syllabus",
    "title": "Welcome!",
    "section": "Syllabus",
    "text": "Syllabus\nm201.inqs.info/syllabus"
  },
  {
    "objectID": "c/1a.html#r-programming",
    "href": "c/1a.html#r-programming",
    "title": "Welcome!",
    "section": "R Programming",
    "text": "R Programming\nThis class you will use R programming to conduct data analysis.\nR is a powerful tool that allows you to program complex tasks.\nLearning R is difficult, but it is the best teacher to teach you how to overcome challenges. Don’t give up when it gets hard and seek out help when needed."
  },
  {
    "objectID": "c/1a.html#when-do-you-use-statistics",
    "href": "c/1a.html#when-do-you-use-statistics",
    "title": "Welcome!",
    "section": "When do you use statistics?",
    "text": "When do you use statistics?"
  },
  {
    "objectID": "c/1a.html#what-is-the-difference-between-math-and-stats",
    "href": "c/1a.html#what-is-the-difference-between-math-and-stats",
    "title": "Welcome!",
    "section": "What is the difference between math and stats?",
    "text": "What is the difference between math and stats?"
  },
  {
    "objectID": "c/1a.html#what-is-variation",
    "href": "c/1a.html#what-is-variation",
    "title": "Welcome!",
    "section": "What is variation?",
    "text": "What is variation?"
  },
  {
    "objectID": "c/1a.html#generative-ai-1",
    "href": "c/1a.html#generative-ai-1",
    "title": "Welcome!",
    "section": "Generative AI",
    "text": "Generative AI\nThe use of generative artificial intelligence (AI) will be prohibited in class. This includes, but not limited to, ChatGPT, Meta AI, and Google Gemini."
  },
  {
    "objectID": "c/1a.html#educational-mislearning",
    "href": "c/1a.html#educational-mislearning",
    "title": "Welcome!",
    "section": "Educational Mislearning",
    "text": "Educational Mislearning\nThe purpose of this class, and college, is for you to learn about critical thinking skills and perseverance. Using AI will only teach you how to get an answer, which may or may not be correct.\n\nYou will not develop the skills needed to problem solve a challenge. Additionally, developing grit is essential to become successful in college and life. There is no easy way out and AI is an illusion to your success in life.\n\n\nTo learn something, it requires hours of work!"
  },
  {
    "objectID": "c/1a.html#stolen-work",
    "href": "c/1a.html#stolen-work",
    "title": "Welcome!",
    "section": "Stolen Work",
    "text": "Stolen Work\nGenerative AI models were developed with the data from the works by authors, artists, researchers, and many more, who did not consent to have their work to be used this way.\n\nAdditionally, all these individuals are not receiving any royalties for the work to be used in creating generative AI models.\n\n\nInside Higher Ed and The New Yorker highlight individual’s concern of their work being used to train AI models."
  },
  {
    "objectID": "c/1a.html#privacy-concerns",
    "href": "c/1a.html#privacy-concerns",
    "title": "Welcome!",
    "section": "Privacy Concerns",
    "text": "Privacy Concerns\nThe use of generative AI raises concerns of what data is being harvested from us, possibly without informed consent.\n\nWhen you used any large language models, you do not know what information is being harvested from you.\n\n\nDo you want to upload your thoughts and ideas to a company that can monetize, and possibly exploit.\n\n\nDoes your Professors consent with you uploading their assignments to large language models.\n\n\nStanford provided a report highlighting the risks of our personal data use in large language models."
  },
  {
    "objectID": "c/1a.html#math-201-consent",
    "href": "c/1a.html#math-201-consent",
    "title": "Welcome!",
    "section": "Math 201 Consent",
    "text": "Math 201 Consent\n\nDr. Isaac Quintanilla Salinas consents in you sharing course material to your friends and family for learning purposes. Use the material to educate and uplift your community.\n\n\nDr. Isaac Quintanilla Salinas does not consent in you sharing course material to companies/corporations. Additionally, I do not consent in the use of course material for academic dishonesty. Exceptions are allowed for DASS-approved services."
  },
  {
    "objectID": "c/1a.html#negative-environmental-impact",
    "href": "c/1a.html#negative-environmental-impact",
    "title": "Welcome!",
    "section": "Negative Environmental Impact",
    "text": "Negative Environmental Impact\nIn order to run these large language models, companies need to use a large amounts of energy. This is because large servers are needed to both train and execute a model.\n\nThe LA Times reports the potential impact that running AI models in California.\n\n\nAdditionally, Time reports that a ChatGPT query uses ten times more energy than a Google search query, and global AI demands can consume of 1 trillion gallons of water by 2027.\n\n\nThere are also environmental justice questions about where these data centers are constructed."
  },
  {
    "objectID": "c/1a.html#worker-exploitation",
    "href": "c/1a.html#worker-exploitation",
    "title": "Welcome!",
    "section": "Worker Exploitation",
    "text": "Worker Exploitation\nThe Washington Post and Time (Article 1 and Article 2) reported that AI companies utilize “digital sweatshops” to classify data points for model training.\n\nThere is a human cost from the Global South, both financially and mentally, to develop the AI models for users in the United States and Europe.\n\n\nWe must be conscious consumers and demand more from these companies to provide safe working conditions and livable wages."
  },
  {
    "objectID": "c/1a.html#how-will-the-profesor-enforce-the-policy",
    "href": "c/1a.html#how-will-the-profesor-enforce-the-policy",
    "title": "Welcome!",
    "section": "How will the Profesor enforce the policy?",
    "text": "How will the Profesor enforce the policy?\n\nI am not! Unless it is extremely obvious, it is extremely difficult to identify AI use.\n\n\nCell has an article indicating that the use of GPT detectors harm non-native English speakers."
  },
  {
    "objectID": "c/1a.html#is-using-ai-bad",
    "href": "c/1a.html#is-using-ai-bad",
    "title": "Welcome!",
    "section": "Is Using AI Bad?",
    "text": "Is Using AI Bad?\n\nNO\n\n\nWhat we should do is demand companies to develop these tools in an ethical manner!"
  },
  {
    "objectID": "c/1a.html#complete-survey",
    "href": "c/1a.html#complete-survey",
    "title": "Welcome!",
    "section": "Complete Survey",
    "text": "Complete Survey\nhttps://forms.gle/EdYK1sGZpDEibaHK7"
  },
  {
    "objectID": "c/1a.html#field-trip",
    "href": "c/1a.html#field-trip",
    "title": "Welcome!",
    "section": "Field Trip",
    "text": "Field Trip"
  },
  {
    "objectID": "c/15b.html#traditional-statistics",
    "href": "c/15b.html#traditional-statistics",
    "title": "Installing R and Rstudio",
    "section": "Traditional Statistics",
    "text": "Traditional Statistics\nTraditional Statistics methods can conducted using either linear or logistic regression.\nVisit Here for more information."
  },
  {
    "objectID": "c/15b.html#what-is-statistics",
    "href": "c/15b.html#what-is-statistics",
    "title": "Installing R and Rstudio",
    "section": "What is Statistics?",
    "text": "What is Statistics?\nIt is the study of variation and randomness!"
  },
  {
    "objectID": "c/15b.html#whats-the-goal-of-statistics",
    "href": "c/15b.html#whats-the-goal-of-statistics",
    "title": "Installing R and Rstudio",
    "section": "What’s the goal of Statistics?",
    "text": "What’s the goal of Statistics?\n\nINFERENCE\n\n\nUse our sample data to understand the larger population.\n\n\nThe data will tell us how the population generally behaves.\n\n\nThe data will guide us in the differences in units.\n\n\nData will tell us if there is a signal or just noise."
  },
  {
    "objectID": "c/15b.html#final-thoughts",
    "href": "c/15b.html#final-thoughts",
    "title": "Installing R and Rstudio",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nWhen conducting a study, literature review and study design are as equally important as statistics.\n\n\nIf you don’t see variability in the data, something is wrong.\n\n\nFocus on consistency in the methodology, not consistency in data.\n\n\nUnderstand that you can be wrong, and that is okay.\n\n\nDon’t let data influence the methodology during a study/experiment."
  },
  {
    "objectID": "c/15b.html#statistics-mantra",
    "href": "c/15b.html#statistics-mantra",
    "title": "Installing R and Rstudio",
    "section": "Statistics Mantra",
    "text": "Statistics Mantra\n\n\n\n\n\n\n\n\nAll models are wrong,\nsome are useful!"
  },
  {
    "objectID": "c/15b.html#final-survey-1",
    "href": "c/15b.html#final-survey-1",
    "title": "Installing R and Rstudio",
    "section": "Final Survey",
    "text": "Final Survey\nSurvery"
  },
  {
    "objectID": "c/15b.html#installing-r",
    "href": "c/15b.html#installing-r",
    "title": "Installing R and Rstudio",
    "section": "Installing R",
    "text": "Installing R\nhttps://cloud.r-project.org/"
  },
  {
    "objectID": "c/15b.html#installing-rstudio",
    "href": "c/15b.html#installing-rstudio",
    "title": "Installing R and Rstudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nhttps://posit.co/download/rstudio-desktop/"
  },
  {
    "objectID": "c/15b.html#installing-rtools-windows",
    "href": "c/15b.html#installing-rtools-windows",
    "title": "Installing R and Rstudio",
    "section": "Installing RTools (Windows)",
    "text": "Installing RTools (Windows)\nhttps://cran.r-project.org/bin/windows/Rtools/"
  },
  {
    "objectID": "c/15b.html#tidyverse",
    "href": "c/15b.html#tidyverse",
    "title": "Installing R and Rstudio",
    "section": "Tidyverse",
    "text": "Tidyverse\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "c/15b.html#csucistats",
    "href": "c/15b.html#csucistats",
    "title": "Installing R and Rstudio",
    "section": "csucistats",
    "text": "csucistats\n\ninstall.packages('csucistats', \n                 repos = c('https://inqs909.r-universe.dev', \n                           'https://cloud.r-project.org'))"
  },
  {
    "objectID": "c/15b.html#other-packages",
    "href": "c/15b.html#other-packages",
    "title": "Installing R and Rstudio",
    "section": "Other Packages",
    "text": "Other Packages\n\ninstall.packages(c(\"ggthemes\", \"statmod\", \"car\", \"ggpubr\", \"lmtest\", \"rms\", \"palmerpenguins\"))\ncsucistats::install_plots()\ncsucistats::install_themes()"
  },
  {
    "objectID": "c/15b.html#load-packages",
    "href": "c/15b.html#load-packages",
    "title": "Installing R and Rstudio",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(csucistats)\nlibrary(ggthemes)\nlibrary(waffle)\nlibrary(ggmosaic)\nlibrary(ggtricks)\nlibrary(ggtext)\nlibrary(ggpubr)\nlibrary(ThemePark)\nlibrary(car)\nlibrary(lmtest)\nlibrary(rms)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "c/15b.html#rstudio-layout-1",
    "href": "c/15b.html#rstudio-layout-1",
    "title": "Installing R and Rstudio",
    "section": "RStudio Layout",
    "text": "RStudio Layout\n\nScripts\nConsole\nEnvironment\nFiles\nEverything else"
  },
  {
    "objectID": "c/15b.html#downloading-data-data",
    "href": "c/15b.html#downloading-data-data",
    "title": "Installing R and Rstudio",
    "section": "Downloading Data Data",
    "text": "Downloading Data Data\n\nu1 &lt;- \"https://www.inqs.info/p/plotathon/owenWilsonWows.csv\"\nu2 &lt;- \"https://www.inqs.info/p/plotathon/owenWilsonWows.xlsx\"\n\ndownload.file(u1,\n              file.path(getwd(), basename(u1)))\ndownload.file(u2,\n              file.path(getwd(), basename(u2)))"
  },
  {
    "objectID": "c/15b.html#loading-data-1",
    "href": "c/15b.html#loading-data-1",
    "title": "Installing R and Rstudio",
    "section": "Loading Data",
    "text": "Loading Data"
  },
  {
    "objectID": "ec.html",
    "href": "ec.html",
    "title": "Extra Credit",
    "section": "",
    "text": "Extra Credit is designed to expand on different topics that related to Probability and Statistics, but are not necessarily required for the course. Additionally, these opportunities provide students relief when unexpected situations occur during the semester. While it is not required, I encourage everyone to attempt each opportunity.\nBelow is more information on each assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 2\n\n\n\n\n\nInstructions for extra credit two.\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 3\n\n\n\n\n\nInstructions for extra credit three.\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 4\n\n\n\n\n\nInstructions for extra credit four.\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ec/ec_3.html",
    "href": "ec/ec_3.html",
    "title": "Extra Credit 3",
    "section": "",
    "text": "Provide a summary of one book below, an write a brief analysis in supporting or opposing the book, and connect key elements of the book to your every day life. The majority of these books are available through the Broome Library.\nBooks:\n\nHappiness: A Guide to Developing Life’s\n\nMatthieu Ricard\n\nGrit: The Power of Passion and Perserverance\n\nAngela Duckworth\n\nMindset: The New Psychology of Success\n\nCarol Dweck\n\n\nReport guidelines\n\n4-5 Pages\nMust include a title page\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 5/13/2024\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Elementary Statistics!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWelcome to the course! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu."
  },
  {
    "objectID": "lectures/1.html#documents",
    "href": "lectures/1.html#documents",
    "title": "Numbers & Data",
    "section": "Documents",
    "text": "Documents\n\nVideo_1A_1.ipynb\nVideo_1A_2.ipynb"
  },
  {
    "objectID": "lectures/1.html#penguin",
    "href": "lectures/1.html#penguin",
    "title": "Numbers & Data",
    "section": "Penguin",
    "text": "Penguin\n\nPicture from: World Wildlife Fund\n\nWhat is the current knowledge of penguins?\nHow do we gain new knowledge of penguins?\nCan anything we learn about this penguin be applied to other penguins?"
  },
  {
    "objectID": "lectures/1.html#penguin-1",
    "href": "lectures/1.html#penguin-1",
    "title": "Numbers & Data",
    "section": "Penguin",
    "text": "Penguin\n\n\n\n\n\nPicture from: World Wildlife Fund\n\n\n\n\n\nHow long is its flipper?\nHow long is its beak?\nWhat is its mass?\nWhat its species?\nWhere was it found?\nWhen was it found?"
  },
  {
    "objectID": "lectures/1.html#penguins",
    "href": "lectures/1.html#penguins",
    "title": "Numbers & Data",
    "section": "Penguins",
    "text": "Penguins\n\nPicture from: Australian Antarctic Program"
  },
  {
    "objectID": "lectures/1.html#palmer-penguins",
    "href": "lectures/1.html#palmer-penguins",
    "title": "Numbers & Data",
    "section": "Palmer Penguins",
    "text": "Palmer Penguins\nPalmer Penguins is a study to determine if"
  },
  {
    "objectID": "lectures/1.html#observations",
    "href": "lectures/1.html#observations",
    "title": "Numbers & Data",
    "section": "Observations",
    "text": "Observations\nAn observation is the unit which contains information to be obtained.\n\n\nAlso known as experimental unit."
  },
  {
    "objectID": "lectures/1.html#data-1",
    "href": "lectures/1.html#data-1",
    "title": "Numbers & Data",
    "section": "Data",
    "text": "Data\nData is information from a particular experimental unit. There can be more that one piece of information per experimental unit."
  },
  {
    "objectID": "lectures/1.html#data-structure",
    "href": "lectures/1.html#data-structure",
    "title": "Numbers & Data",
    "section": "Data Structure",
    "text": "Data Structure\nThe structure of the data can be represented in various forms:\n\nA list of long numbers\nTable"
  },
  {
    "objectID": "lectures/1.html#tabular-form",
    "href": "lectures/1.html#tabular-form",
    "title": "Numbers & Data",
    "section": "Tabular Form",
    "text": "Tabular Form"
  },
  {
    "objectID": "lectures/1.html#measurements-1",
    "href": "lectures/1.html#measurements-1",
    "title": "Numbers & Data",
    "section": "Measurements",
    "text": "Measurements\nMeasurements are the processes where we represent an attribute of an experimental unit as either a number or category."
  },
  {
    "objectID": "lectures/1.html#quantitative-measurements",
    "href": "lectures/1.html#quantitative-measurements",
    "title": "Numbers & Data",
    "section": "Quantitative Measurements",
    "text": "Quantitative Measurements\nQuantitative measurements are data measurements that take a numeric form."
  },
  {
    "objectID": "lectures/1.html#qualitative-measurements",
    "href": "lectures/1.html#qualitative-measurements",
    "title": "Numbers & Data",
    "section": "Qualitative Measurements",
    "text": "Qualitative Measurements\nQualitative measurements are data measurements that take a certain category."
  },
  {
    "objectID": "lectures/1.html#value",
    "href": "lectures/1.html#value",
    "title": "Numbers & Data",
    "section": "Value",
    "text": "Value\nA value is a description (number or category) of a specific attribute of an experimental unit."
  },
  {
    "objectID": "lectures/1.html#variable",
    "href": "lectures/1.html#variable",
    "title": "Numbers & Data",
    "section": "Variable",
    "text": "Variable\nA variable is the descriptive attribute that we want to obtain from an experimental unit. In terms of a data set, the variable contains all the values of specific attribute in a sample."
  },
  {
    "objectID": "lectures/11.html#motivation-1",
    "href": "lectures/11.html#motivation-1",
    "title": "Hypothesis Testing",
    "section": "Motivation",
    "text": "Motivation\nThe ncbirths data set provides information on whether different predictors. The outcome of interest will be premie (“full term” or “premie”)."
  },
  {
    "objectID": "lectures/11.html#motivation-1-1",
    "href": "lectures/11.html#motivation-1-1",
    "title": "Hypothesis Testing",
    "section": "Motivation 1",
    "text": "Motivation 1\nModel the relationship between the outcome premie and habit (“smoker” and “nonsmoker”)."
  },
  {
    "objectID": "lectures/11.html#motivation-2",
    "href": "lectures/11.html#motivation-2",
    "title": "Hypothesis Testing",
    "section": "Motivation 2",
    "text": "Motivation 2\nModel the relationship between the outcome premie and visits (“smoker” and “nonsmoker”)."
  },
  {
    "objectID": "lectures/11.html#logistic-model-1",
    "href": "lectures/11.html#logistic-model-1",
    "title": "Hypothesis Testing",
    "section": "Logistic Model 1",
    "text": "Logistic Model 1\nUse a logistic regression to model the outcome premie and habit.\n\n\nCode\nglm(premie ~ habit, data = ncbirths, family = binomial())"
  },
  {
    "objectID": "lectures/11.html#odds-ratio-1",
    "href": "lectures/11.html#odds-ratio-1",
    "title": "Hypothesis Testing",
    "section": "Odds Ratio 1",
    "text": "Odds Ratio 1\n\n\nCode\nb(glm(premie ~ habit, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ habit, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/11.html#odds-ratio-1-1",
    "href": "lectures/11.html#odds-ratio-1-1",
    "title": "Hypothesis Testing",
    "section": "Odds Ratio 1",
    "text": "Odds Ratio 1\n\n\nCode\nb(glm(premie ~ habit, data = ncbirths, family = binomial()), 1)\n\n\n#&gt; [1] -0.01344105\n\n\nCode\nexp(b(glm(premie ~ habit, data = ncbirths, family = binomial()), 1))\n\n\n#&gt; [1] 0.9866489\n\n\nThe odds of a mother who smokes having a premature infant is 0.98 times lower than the odds of a mother who does not smoke."
  },
  {
    "objectID": "lectures/11.html#logistic-model-2",
    "href": "lectures/11.html#logistic-model-2",
    "title": "Hypothesis Testing",
    "section": "Logistic Model 2",
    "text": "Logistic Model 2\nUse a logistic regression to model the outcome premie and visits.\n\n\nCode\nglm(premie ~ visits, data = ncbirths, family = binomial())"
  },
  {
    "objectID": "lectures/11.html#odds-ratio-2",
    "href": "lectures/11.html#odds-ratio-2",
    "title": "Hypothesis Testing",
    "section": "Odds Ratio 2",
    "text": "Odds Ratio 2\n\n\nCode\nb(glm(premie ~ visits, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ visits, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/11.html#odds-ratio-2-1",
    "href": "lectures/11.html#odds-ratio-2-1",
    "title": "Hypothesis Testing",
    "section": "Odds Ratio 2",
    "text": "Odds Ratio 2\n\n\nCode\nb(glm(premie ~ visits, data = ncbirths, family = binomial()), 1)\n\n\n#&gt; [1] -0.1063097\n\n\nCode\nexp(b(glm(premie ~ visits, data = ncbirths, family = binomial()), 1))\n\n\n#&gt; [1] 0.8991461\n\n\nAs the number of hospital visits increases by 1, the odds of having a premature infant decreases by a factor 0.89."
  },
  {
    "objectID": "lectures/11.html#real-or-random",
    "href": "lectures/11.html#real-or-random",
    "title": "Hypothesis Testing",
    "section": "Real or Random",
    "text": "Real or Random\n\nDoes smoking have a protective effect on having a premature infant?\n\n\nDoes the number of hospital visits have a protective effect?\n\n\nWhat is real and what is random?"
  },
  {
    "objectID": "lectures/11.html#hypothesis-tests",
    "href": "lectures/11.html#hypothesis-tests",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/11.html#null-hypothesis-h_0",
    "href": "lectures/11.html#null-hypothesis-h_0",
    "title": "Hypothesis Testing",
    "section": "Null Hypothesis \\(H_0\\)",
    "text": "Null Hypothesis \\(H_0\\)\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value."
  },
  {
    "objectID": "lectures/11.html#alternative-hypothesis-h_a",
    "href": "lectures/11.html#alternative-hypothesis-h_a",
    "title": "Hypothesis Testing",
    "section": "Alternative Hypothesis \\(H_a\\)",
    "text": "Alternative Hypothesis \\(H_a\\)\nThe alternative hypothesis contradicts the null hypothesis."
  },
  {
    "objectID": "lectures/11.html#example-of-null-and-alternative-hypothesis",
    "href": "lectures/11.html#example-of-null-and-alternative-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Example of Null and Alternative Hypothesis",
    "text": "Example of Null and Alternative Hypothesis\nWe want to see if \\(\\beta\\) is different from \\(\\beta^*\\)\n\n\n\nNull Hypothesis\nAlternative Hypothesis\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)"
  },
  {
    "objectID": "lectures/11.html#one-side-vs-two-side-hypothesis-tests",
    "href": "lectures/11.html#one-side-vs-two-side-hypothesis-tests",
    "title": "Hypothesis Testing",
    "section": "One-Side vs Two-Side Hypothesis Tests",
    "text": "One-Side vs Two-Side Hypothesis Tests\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis (\\(H_a:\\beta\\ne\\beta^*\\)) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n\n\nNull Hypothesis\nAlternative Hypothesis\nSide\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n2-Sided\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n1-Sided\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)\n1-Sided"
  },
  {
    "objectID": "lectures/11.html#decision-making",
    "href": "lectures/11.html#decision-making",
    "title": "Hypothesis Testing",
    "section": "Decision Making",
    "text": "Decision Making\nHypothesis Testing will force you to make a decision: Reject \\(H_0\\) OR Fail to Reject \\(H_0\\)\n\nReject \\(H_0\\): The effect seen is not due to random chance, there is a process making contributing to the effect.\n\n\nFail to Reject \\(H_0\\): The effect seen is due to random chance. Random sampling is the reason why an effect is displayed, not an underlying process."
  },
  {
    "objectID": "lectures/11.html#decision-making-p-value",
    "href": "lectures/11.html#decision-making-p-value",
    "title": "Hypothesis Testing",
    "section": "Decision Making: P-Value",
    "text": "Decision Making: P-Value\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true.\n\nIf \\(p &lt; \\alpha\\), then you reject \\(H_0\\); otherwise, you will fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/11.html#decision-making-confidence-interval-approach",
    "href": "lectures/11.html#decision-making-confidence-interval-approach",
    "title": "Hypothesis Testing",
    "section": "Decision Making: Confidence Interval Approach",
    "text": "Decision Making: Confidence Interval Approach\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is \\(\\beta\\ne\\beta^*\\). The bootstrapping approach will result in a lower and upper bound denoted as: \\((LB, UB)\\).\n\nIf \\(\\beta^*\\) is in \\((LB, UB)\\), then you fail to reject \\(H_0\\). If \\(\\beta^*\\) is not in \\((LB,UB)\\), then you reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/11.html#significance-level-alpha",
    "href": "lectures/11.html#significance-level-alpha",
    "title": "Hypothesis Testing",
    "section": "Significance Level \\(\\alpha\\)",
    "text": "Significance Level \\(\\alpha\\)\nThe significance level \\(\\alpha\\) is the probability you will reject the null hypothesis given that it was true.\n\nIn other words, \\(\\alpha\\) is the error rate that a research controls.\n\n\nTypically, we want this error rate to be small (\\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "lectures/11.html#hypothesis-test-process",
    "href": "lectures/11.html#hypothesis-test-process",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Test Process",
    "text": "Hypothesis Test Process\n\n\nSet up the Null and Alternative Hypothesis.\n\n\\(H_0: \\beta = 0\\) and \\(H_a: \\beta \\ne 0\\)\n\nCompute p-value or confidence interval\nMake a decision\nInterpret the results"
  },
  {
    "objectID": "lectures/11.html#example",
    "href": "lectures/11.html#example",
    "title": "Hypothesis Testing",
    "section": "Example",
    "text": "Example\nUse a logistic regression to model the outcome premie and habit.\n\n\nCode\nglm(premie ~ habit, data = ncbirths, family = binomial())\nb(glm(premie ~ habit, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ habit, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/11.html#example-3",
    "href": "lectures/11.html#example-3",
    "title": "Hypothesis Testing",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "lectures/11.html#logistic-model-2-1",
    "href": "lectures/11.html#logistic-model-2-1",
    "title": "Hypothesis Testing",
    "section": "Logistic Model 2",
    "text": "Logistic Model 2\n\n\nCode\nglm(premie ~ visits, data = ncbirths, family = binomial())\n\nb(glm(premie ~ visits, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ visits, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/13.html#motivating-example-1",
    "href": "lectures/13.html#motivating-example-1",
    "title": "Inference with Logistic Regression",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\nbladder1 |&gt; ggplot(aes(number, color = death2)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/13.html#hypothesis",
    "href": "lectures/13.html#hypothesis",
    "title": "Inference with Logistic Regression",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/13.html#testing-beta_j",
    "href": "lectures/13.html#testing-beta_j",
    "title": "Inference with Logistic Regression",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\n\\frac{\\hat\\beta_j - \\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/13.html#confidence-intervals",
    "href": "lectures/13.html#confidence-intervals",
    "title": "Inference with Logistic Regression",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\nPE \\pm CV \\times SE\n\\]\n\nPE: Point Estimate\nCV: Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\nSE: Standard Error"
  },
  {
    "objectID": "lectures/13.html#conducting-ht-of-beta_j",
    "href": "lectures/13.html#conducting-ht-of-beta_j",
    "title": "Inference with Logistic Regression",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial())\nsummary(xlm)"
  },
  {
    "objectID": "lectures/13.html#example",
    "href": "lectures/13.html#example",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ recur + number + size, bladder1, family = binomial())\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = death ~ recur + number + size, family = binomial(), \n#&gt;     data = bladder1)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.8525259  0.4462559  -1.910 0.056082 .  \n#&gt; recur       -0.3897480  0.1062848  -3.667 0.000245 ***\n#&gt; number       0.0008451  0.1124503   0.008 0.994004    \n#&gt; size        -0.2240419  0.1626749  -1.377 0.168439    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 189.38  on 293  degrees of freedom\n#&gt; Residual deviance: 166.43  on 290  degrees of freedom\n#&gt; AIC: 174.43\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/13.html#confidence-interval",
    "href": "lectures/13.html#confidence-interval",
    "title": "Inference with Logistic Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/13.html#example-1",
    "href": "lectures/13.html#example-1",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.95)\n\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) -1.7353779  0.02529523\n#&gt; recur       -0.6217831 -0.20078281\n#&gt; number      -0.2421738  0.20731479\n#&gt; size        -0.5880581  0.06061498"
  },
  {
    "objectID": "lectures/13.html#confidence-interval-for-odds-ratio",
    "href": "lectures/13.html#confidence-interval-for-odds-ratio",
    "title": "Inference with Logistic Regression",
    "section": "Confidence Interval for Odds Ratio",
    "text": "Confidence Interval for Odds Ratio\n\n\nCode\nexp(confint(m1, level = 0.95))\n\n\n#&gt;                 2.5 %    97.5 %\n#&gt; (Intercept) 0.1763335 1.0256179\n#&gt; recur       0.5369861 0.8180901\n#&gt; number      0.7849197 1.2303698\n#&gt; size        0.5554048 1.0624898"
  },
  {
    "objectID": "lectures/13.html#model-inference",
    "href": "lectures/13.html#model-inference",
    "title": "Inference with Logistic Regression",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(g(\\hat Y)=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/13.html#model-inference-1",
    "href": "lectures/13.html#model-inference-1",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven 2 models:\n\\[\ng(\\hat Y) = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\ng(\\hat Y) = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/13.html#likelihood-ratio-test",
    "href": "lectures/13.html#likelihood-ratio-test",
    "title": "Inference with Logistic Regression",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nThe Likelihood Ratio Test is a test to determine whether the likelihood of observing the outcome is significantly bigger in a larger, more complicated model, than a simpler model.\nIt conducts a hypothesis tests to see if models are significantly different from each other."
  },
  {
    "objectID": "lectures/13.html#conducting-an-lrt-in-r",
    "href": "lectures/13.html#conducting-an-lrt-in-r",
    "title": "Inference with Logistic Regression",
    "section": "Conducting an LRT in R",
    "text": "Conducting an LRT in R\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial)\nxlm0 &lt;- glm(Y ~ 1, data = DATA, family = binomial)\nanova(xlm0, xlm, test = \"LRT\")"
  },
  {
    "objectID": "lectures/13.html#example-2",
    "href": "lectures/13.html#example-2",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm0 &lt;- update(m1, formula. = ~ 1)\nanova(m0, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ 1\n#&gt; Model 2: death ~ recur + number + size\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n#&gt; 1       293     189.38                         \n#&gt; 2       290     166.43  3   22.953 4.13e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/13.html#model-inference-2",
    "href": "lectures/13.html#model-inference-2",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/13.html#model-inference-3",
    "href": "lectures/13.html#model-inference-3",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/13.html#model-inference-4",
    "href": "lectures/13.html#model-inference-4",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/13.html#likelihood-ratio-test-in-r",
    "href": "lectures/13.html#likelihood-ratio-test-in-r",
    "title": "Inference with Logistic Regression",
    "section": "Likelihood Ratio Test in R",
    "text": "Likelihood Ratio Test in R\n\n\nCode\nfull &lt;- glm(Y  ~  X1 + X2 + X3 + X4, DATA, family = binomial())\nred &lt;- glm(Y ~ X1 + X2, DATA, family = binomial())\nanova(red, full, test = \"LRT\")"
  },
  {
    "objectID": "lectures/13.html#example-3",
    "href": "lectures/13.html#example-3",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ number + size + recur, bladder1, family = binomial())\nm2 &lt;- glm(death ~ recur, bladder1, family = binomial())\nanova(m2, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ recur\n#&gt; Model 2: death ~ number + size + recur\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n#&gt; 1       292     168.72                     \n#&gt; 2       290     166.43  2   2.2883   0.3185"
  },
  {
    "objectID": "lectures/2/2.html#documents",
    "href": "lectures/2/2.html#documents",
    "title": "Numbers & Data",
    "section": "Documents",
    "text": "Documents\n\nVideo_1A.ypynb"
  },
  {
    "objectID": "lectures/2/2.html#penguin",
    "href": "lectures/2/2.html#penguin",
    "title": "Numbers & Data",
    "section": "Penguin",
    "text": "Penguin\n\nPicture from: World Wildlife Fund\n\nWhat is the current knowledge of penguins?\nHow do we gain new knowledge of penguins?\nCan anything we learn about this penguin be applied to other penguins?"
  },
  {
    "objectID": "lectures/2/2.html#penguin-1",
    "href": "lectures/2/2.html#penguin-1",
    "title": "Numbers & Data",
    "section": "Penguin",
    "text": "Penguin\n\n\n\n\n\nPicture from: World Wildlife Fund\n\n\n\n\n\nHow long is its flipper?\nHow long is its beak?\nWhat is its mass?\nWhat its species?\nWhere was it found?\nWhen was it found?"
  },
  {
    "objectID": "lectures/2/2.html#penguins",
    "href": "lectures/2/2.html#penguins",
    "title": "Numbers & Data",
    "section": "Penguins",
    "text": "Penguins\n\nPicture from: Australian Antarctic Program"
  },
  {
    "objectID": "lectures/2/2.html#palmer-penguins",
    "href": "lectures/2/2.html#palmer-penguins",
    "title": "Numbers & Data",
    "section": "Palmer Penguins",
    "text": "Palmer Penguins\nPalmer Penguins is a study to determine if"
  },
  {
    "objectID": "lectures/2/2.html#observations",
    "href": "lectures/2/2.html#observations",
    "title": "Numbers & Data",
    "section": "Observations",
    "text": "Observations\nAn observation is the unit which contains information to be obtained.\n\n\nAlso known as experimental unit."
  },
  {
    "objectID": "lectures/2/2.html#data-1",
    "href": "lectures/2/2.html#data-1",
    "title": "Numbers & Data",
    "section": "Data",
    "text": "Data\nData is information from a particular experimental unit. There can be more that one piece of information per experimental unit."
  },
  {
    "objectID": "lectures/2/2.html#data-structure",
    "href": "lectures/2/2.html#data-structure",
    "title": "Numbers & Data",
    "section": "Data Structure",
    "text": "Data Structure\nThe structure of the data can be represented in various forms:\n\nA list of long numbers\nTable"
  },
  {
    "objectID": "lectures/2/2.html#tabular-form",
    "href": "lectures/2/2.html#tabular-form",
    "title": "Numbers & Data",
    "section": "Tabular Form",
    "text": "Tabular Form"
  },
  {
    "objectID": "lectures/2/2.html#measurements-1",
    "href": "lectures/2/2.html#measurements-1",
    "title": "Numbers & Data",
    "section": "Measurements",
    "text": "Measurements\nMeasurements are the processes where we represent an attribute of an experimental unit as either a number or category."
  },
  {
    "objectID": "lectures/2/2.html#quantitative-measurements",
    "href": "lectures/2/2.html#quantitative-measurements",
    "title": "Numbers & Data",
    "section": "Quantitative Measurements",
    "text": "Quantitative Measurements\nQuantitative measurements are data measurements that take a numeric form."
  },
  {
    "objectID": "lectures/2/2.html#qualitative-measurements",
    "href": "lectures/2/2.html#qualitative-measurements",
    "title": "Numbers & Data",
    "section": "Qualitative Measurements",
    "text": "Qualitative Measurements\nQualitative measurements are data measurements that take a certain category."
  },
  {
    "objectID": "lectures/2/2.html#value",
    "href": "lectures/2/2.html#value",
    "title": "Numbers & Data",
    "section": "Value",
    "text": "Value\nA value is a description (number or category) of a specific attribute of an experimental unit."
  },
  {
    "objectID": "lectures/2/2.html#variable",
    "href": "lectures/2/2.html#variable",
    "title": "Numbers & Data",
    "section": "Variable",
    "text": "Variable\nA variable is the descriptive attribute that we want to obtain from an experimental unit. In terms of a data set, the variable contains all the values of specific attribute in a sample."
  },
  {
    "objectID": "lectures/4.html#r-packages",
    "href": "lectures/4.html#r-packages",
    "title": "Numerical Data",
    "section": "R Packages",
    "text": "R Packages\n\ncsucistats\ntidyverse"
  },
  {
    "objectID": "lectures/4.html#motivating-example",
    "href": "lectures/4.html#motivating-example",
    "title": "Numerical Data",
    "section": "Motivating Example",
    "text": "Motivating Example"
  },
  {
    "objectID": "lectures/4.html#what-is-numerical-data",
    "href": "lectures/4.html#what-is-numerical-data",
    "title": "Numerical Data",
    "section": "What is numerical data?",
    "text": "What is numerical data?\n\ntrashwheel$PlasticBottles\n\n#&gt;   [1] 1450 1120 2450 2380  980 1430  910 3580 2400 1340  740  950  530  840 1130 1640 1350 1640 1730 5960 2170 1930 3200 2500 2140 1630 3640 1430  570 4800  550 2240 4220 1400 2820 1900 3650  760 1250  880 1800 1370  550\n#&gt;  [44]  640 1160 1570 1340 1740 2870 3120 2780 2240 2550 1470 1240 1150 2850  960  870 1340 2630 1850 2830 1740 2780 1260 2340 3240  940  830  960  870  920 1130 1060  740  950 2340 2660 2430 2690 1540 1240 2870 2450 1830\n#&gt;  [87] 2360 2640 2840 2640 3070 2820 2640  870 3220 1980  410  640 2730 1360  760 1240  800 2540 2130 2670 1940 2140 1960 1640 1850 2370 2530 2720 2250 3240 3530 2340 2460 1340  950 2240 3050 1630 1120 3340 3560 2460 3860\n#&gt; [130] 3230 3760 3640 4230 3590 3740 4200 3840 4350 3960 4150 4450 3460 2850 3560 4130 4350 4420 2130 2450 2300 2850 2640 3250 2530 2780 2740 2530 1876 2340 2244 2980 3460 1840 1360 1880 2460 2260 2890 2140 2570 2030 1940\n#&gt; [173] 1870 1920 1920 2230  980  650  430  390  490  210  560  470  390  460  530  460  340  360 2540 2870 2230 2340 2560 2340 2480 2540 2980 3250 3340 2850 3220 2110 3670 2890 1460 3240 3530 2760 2980 2430 1500 1270 1340\n#&gt; [216] 2780 2910 2760 2950 2550 2720 2540 2370 2250 1260  950 1020 2250 1130 1240 1950 1460 2210 2470 2150  970  840  790  750  810  790  670  710  730  820  760  670  730  750  820 1580 2130 2350 2460 1220  950  400 1050\n#&gt; [259] 1480 1300 1650 2130 2340 1890 1420 1280 1140  870  940  620 1560 1720 1890 1760  890  710  620  780  540  670  820  540  730  910  710 1750 1320 2740 1120 2100 3110 1200 1390  940 1220 1960 2150 2380 1440 1080 1840\n#&gt; [302] 2100 1500  660  590 1200 2610 1820 1080  790 1240  780  480  300  990 1110  580 2200 1900 2800  840  680  300  240 1015 2150  580 1400  800 1110 1980 1040  940  600  460  980 1880  900 1740 1140 2040 2800 1800 3300\n#&gt; [345] 1880 2000 2680  950 3700 1900 3600 2400 4400  980 1980 3600 2200 2800 4200  800 1400 1800 2210  980 2800 1900 2780 2440 3400  920 1260 1980 1000  960  640  860  590 1800 3600 2300 3900 2900 1850 3400  980  750  500\n#&gt; [388] 3200 3800 1850 2800  980 3200 2800 2640 1200  510  660  480 2900 2300 1980 1040  900 3400 1800 3000 2800  950 3800  800  760  540  340  600  480 2100 1840 1100 2650 1900  900 1880 1240 3000 2300 1400 2050 3250 2100\n#&gt; [431] 2450 4050 3800 3000 2200 3500 1800 1250 2200 1640 1080 1790 2080 1600 2200 2000 1200  980  800 1200 1800 1440 1600 2400 1900 2100 1200  720  800 2200 1600 1000 3000 2700 2300 1900 5800 1800 2200 3400 4400 3400 2500\n#&gt; [474] 1600 1200 1040 4400 4800 1800 3600 3000 4200 1400 2800 1800 3400 2600 3300 4100 1800 3500 2900  850  680 3100 1750  980 2800 1200  840 1040  500  800  640  900  660  750 3800 2900 1800 2700 4200 3000 1200  600 4400\n#&gt; [517] 3100 2800 1100 3400 2800  800 1800  540 3300 2800  900 2100 3200 1440  800  540 2900 2000  960 3200 1250 2900 2100 3600 1400 3900 3000 1800 2700 2400 2900 1400 3900 4200 2800 1900  960  640  750  500 3500 2900 2100\n#&gt; [560] 3200 1800 1500 2100 1000 2900 1600 2100 1700 2400 3600 2700 2900 1800 2200 3000 2100 1000 2700 2200 3100 3400 4000 2100 1900 2300 3200 1200 3400 2100  980  540 1100  750 1800 2300 3400 2700 3000 3800 4200 1800 2000\n#&gt; [603] 3300 4200 1200 2200  850  980  300  450   80 1800 2200 3400 3100 2600 1800 1200 2200 1400 2100 1600 2000 3200 2100 1400  880 2000 1200 1950 9540 8350 8590 7830 8210 9830 9240 9540 8230 7540 8490 9610 7960 8870 7420\n#&gt; [646] 8250 8472 7530 2340 2210 2670  657 2290 2110 7340 9230 1020  900 1140 1350 1200 1090  920 3150 6970 9100 8400 6500 4400 3600 4000 3700 4057 8800 6800 5900 7500 8000 4400 6600 3800 4800 4400 5200 6400 5900 4800 5600\n#&gt; [689] 7200 4400 5400 4800 6700 6000 6800 5950 7200 6400 5300 4900 4400 5000 4900 4800 5800 3600 4200 5000 5400 6400 4000 4300 5100 3900 2700 3000 4100 3900 4400 4900 3600 4500 5000 5400 3400 4800 3800 2900 3200 4200 2900\n#&gt; [732]   NA 3400 4100 3900 4900 3400 5100 4200 2900 3800 3200 4000 1150 2320 3450 1490  920 1010  680 1800 2700  950  103 1850 1200 1050 1860 1180 1050 1420 1800 1500 1200  860  640 1400  980 2100 1100 2400 1500 2200 1200\n#&gt; [775] 2000 1800 1000 2100 2400 2700 3000  980  240  840 1800 1000  500   12 2400 1000 1800  300 2100 2400 1200 1800 2100  280  480  150 1400  980   40  640  800 1000  810  500 1800  840  500 1500  480 1000 1200  450  280\n#&gt; [818]  150    0    0 2100 1800 1400  980 3000 2400 1100  340 1200  600 2300 1200 3000 4200 1800  980 2100 3200 1050 2700  980 1400  800  640  240 2100 2800 1600 1200 2200  980  540  320  270 1200  360  120  480    0 2400\n#&gt; [861] 3200 2500 1800  250  240  640  180  540    0    0 2800 1600  980  480  720    0  360 2100 2900 2100 1800 2900 3200 1800 1600 2800 1200 2100 2500 1800 1600 2100 1200  980 2500 1900 2100 1200  900  800  480  720 1600\n#&gt; [904] 2400  800  500  640  900 1400 2100 1900  960 2400 1800 2700 1500  930 2100 3000 1600 1200 4100 5400 4400 5000 3400 4900 2500 2700 1300  800  440 2100 3200 1100  800  540 4100 3300 1500  850  440 2800 1200 1400  880\n#&gt; [947]  750 1800  400   10 1400  600 2500 1100 2300 1000 1300  500  700   20    0 1800  680 2800 3300 2900 1800 2800 1000 2500  950  360    0 3400 2700 3200 2800 1600 2900 2000 2500 1400  800 1500 2100 1400  980 1800  500\n#&gt; [990] 1200  180    0    0"
  },
  {
    "objectID": "lectures/4.html#summary-statistics-1",
    "href": "lectures/4.html#summary-statistics-1",
    "title": "Numerical Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nSummary statistics are used to describe the distribution of data."
  },
  {
    "objectID": "lectures/4.html#central-tendency",
    "href": "lectures/4.html#central-tendency",
    "title": "Numerical Data",
    "section": "Central Tendency",
    "text": "Central Tendency\nCentral tendency is a statistical concept that refers to the central or typical value around which a set of data points tends to cluster. It is used to summarize and describe a data set by identifying a single representative value that provides insights into the data’s overall characteristics."
  },
  {
    "objectID": "lectures/4.html#variation",
    "href": "lectures/4.html#variation",
    "title": "Numerical Data",
    "section": "Variation",
    "text": "Variation\nVariation in statistics refers to the extent to which data points in a dataset deviate or differ from a central tendency measure. Understanding variation is crucial for making informed decisions, drawing meaningful conclusions, and assessing the reliability of statistical analyses."
  },
  {
    "objectID": "lectures/4.html#minimum",
    "href": "lectures/4.html#minimum",
    "title": "Numerical Data",
    "section": "Minimum",
    "text": "Minimum\nThe minimum (min) is the smallest value in the data."
  },
  {
    "objectID": "lectures/4.html#maximum",
    "href": "lectures/4.html#maximum",
    "title": "Numerical Data",
    "section": "Maximum",
    "text": "Maximum\nThe maximum (max) is the largest value in the data."
  },
  {
    "objectID": "lectures/4.html#quartiles",
    "href": "lectures/4.html#quartiles",
    "title": "Numerical Data",
    "section": "Quartiles",
    "text": "Quartiles\nQuartiles are three values (Q1, Q2, Q3) that divides the data into four subsets."
  },
  {
    "objectID": "lectures/4.html#q1",
    "href": "lectures/4.html#q1",
    "title": "Numerical Data",
    "section": "Q1",
    "text": "Q1\nQ1 is the value signifying that a quarter of the data is lower than it."
  },
  {
    "objectID": "lectures/4.html#q2---median",
    "href": "lectures/4.html#q2---median",
    "title": "Numerical Data",
    "section": "Q2 - Median",
    "text": "Q2 - Median\nQ2 is the value signifying that half of the data is below it.\n\nThe median also represents the central tendency of the data."
  },
  {
    "objectID": "lectures/4.html#q3",
    "href": "lectures/4.html#q3",
    "title": "Numerical Data",
    "section": "Q3",
    "text": "Q3\nQ3 is the value signifying that 3 quarters of the data is below it."
  },
  {
    "objectID": "lectures/4.html#interquartile-range",
    "href": "lectures/4.html#interquartile-range",
    "title": "Numerical Data",
    "section": "Interquartile Range",
    "text": "Interquartile Range\n\n\\[\nIQR = Q_3 - Q_1\n\\]"
  },
  {
    "objectID": "lectures/4.html#range",
    "href": "lectures/4.html#range",
    "title": "Numerical Data",
    "section": "Range",
    "text": "Range\n\n\\[\nR = \\mathrm{max} - \\mathrm{min}\n\\]"
  },
  {
    "objectID": "lectures/4.html#how-to-identify-the-quartiles",
    "href": "lectures/4.html#how-to-identify-the-quartiles",
    "title": "Numerical Data",
    "section": "How to identify the quartiles?",
    "text": "How to identify the quartiles?\n\nSort the data\nID Max and Min\nFind the amount of data the makes a quarter:\n\n\\(K=N/4\\)\n\nCreate 4 groups using the sorted data\n\ngroup by data size\nIf \\(K\\) has a decimal, the \\(Kth\\) value is quartile of each group."
  },
  {
    "objectID": "lectures/4.html#mean",
    "href": "lectures/4.html#mean",
    "title": "Numerical Data",
    "section": "Mean",
    "text": "Mean\nDescribe how you will find the mean of these numbers:\n\n\n#&gt; [1] 15 16 14 19 14"
  },
  {
    "objectID": "lectures/4.html#mean-1",
    "href": "lectures/4.html#mean-1",
    "title": "Numerical Data",
    "section": "Mean",
    "text": "Mean\nThe mean is another measurement for central tendency.\n\\[\n\\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i\n\\]\n\n\\(n\\): total data points\n\\(X_i\\): data points\n\\(i\\): indexing data\n\\(\\sum\\): add all from first (bottom) to last (up)"
  },
  {
    "objectID": "lectures/4.html#variance",
    "href": "lectures/4.html#variance",
    "title": "Numerical Data",
    "section": "Variance",
    "text": "Variance\nThe variance is a measurement on the average squared distance the data points are from the central tendency.\n\\[\ns^2 = \\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar X)^2\n\\]"
  },
  {
    "objectID": "lectures/4.html#standard-deviation",
    "href": "lectures/4.html#standard-deviation",
    "title": "Numerical Data",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nThe standard deviation is a measurement on the average distance the data points are from the central tendency.\n\\[\ns=\\sqrt{s^2}\n\\]"
  },
  {
    "objectID": "lectures/4.html#outliers",
    "href": "lectures/4.html#outliers",
    "title": "Numerical Data",
    "section": "Outliers",
    "text": "Outliers\nThese are data points that seem to be highly distant from all other variables."
  },
  {
    "objectID": "lectures/4.html#numerical-statistics-in-r-1",
    "href": "lectures/4.html#numerical-statistics-in-r-1",
    "title": "Numerical Data",
    "section": "Numerical Statistics in R",
    "text": "Numerical Statistics in R"
  },
  {
    "objectID": "lectures/4.html#mean-2",
    "href": "lectures/4.html#mean-2",
    "title": "Numerical Data",
    "section": "Mean",
    "text": "Mean\n\nmean(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#median",
    "href": "lectures/4.html#median",
    "title": "Numerical Data",
    "section": "Median",
    "text": "Median\n\nmedian(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#standard-deviation-1",
    "href": "lectures/4.html#standard-deviation-1",
    "title": "Numerical Data",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\nsd(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#variance-1",
    "href": "lectures/4.html#variance-1",
    "title": "Numerical Data",
    "section": "Variance",
    "text": "Variance\n\nvar(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#quartiles-1",
    "href": "lectures/4.html#quartiles-1",
    "title": "Numerical Data",
    "section": "Quartiles",
    "text": "Quartiles\n\nquantile(DATA$VAR, probs = c(0.25, 0.5, 0.75))"
  },
  {
    "objectID": "lectures/4.html#max-and-min",
    "href": "lectures/4.html#max-and-min",
    "title": "Numerical Data",
    "section": "Max and Min",
    "text": "Max and Min\n\nmax(DATA$VAR)\nmin(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#summary-statistics-2",
    "href": "lectures/4.html#summary-statistics-2",
    "title": "Numerical Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nnum_stats(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#mr.-trash-wheel",
    "href": "lectures/4.html#mr.-trash-wheel",
    "title": "Numerical Data",
    "section": "Mr. Trash Wheel",
    "text": "Mr. Trash Wheel\n\nnum_stats(trashwheel$PlasticBottles)\n\n#&gt; # A tibble: 1 × 10\n#&gt;     min   q25  mean median   q75   max    sd      var   iqr missing\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     0  988. 2219.   1900  2900  9830 1650. 2723984. 1912.       1"
  },
  {
    "objectID": "lectures/4.html#histogram",
    "href": "lectures/4.html#histogram",
    "title": "Numerical Data",
    "section": "Histogram",
    "text": "Histogram\nA histogram is a graphical representation of the distribution or frequency of data points in a dataset. It provides a visual way to understand the shape, central tendency, and spread of a dataset by dividing the data into intervals or bins and showing how many data points fall into each bin as a bar."
  },
  {
    "objectID": "lectures/4.html#histogram-r-code",
    "href": "lectures/4.html#histogram-r-code",
    "title": "Numerical Data",
    "section": "Histogram R Code",
    "text": "Histogram R Code\n\nggplot(DATA, aes(VARIABLE)) +\n  geom_histogram(bins = X)"
  },
  {
    "objectID": "lectures/4.html#histogram-1",
    "href": "lectures/4.html#histogram-1",
    "title": "Numerical Data",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\ny &lt;- rnorm(1000)\nggplot(tibble(y), aes(y)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#histogram-2",
    "href": "lectures/4.html#histogram-2",
    "title": "Numerical Data",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\ny &lt;- rgamma(1000, 2)\nggplot(tibble(y), aes(y)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#histograms",
    "href": "lectures/4.html#histograms",
    "title": "Numerical Data",
    "section": "Histograms",
    "text": "Histograms\n\n\nCode\ny &lt;- rbeta(1000, 5, 1)\nggplot(tibble(y), aes(y)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#histograms-1",
    "href": "lectures/4.html#histograms-1",
    "title": "Numerical Data",
    "section": "Histograms",
    "text": "Histograms\n\n\nCode\ny &lt;- rbinom(1000, 1, 0.4)\nz &lt;- (y == 0) * rnorm(1000, 23) + (y == 1) * rnorm(1000, 27)\nggplot(tibble(z), aes(z)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#mr.-trash-wheel-1",
    "href": "lectures/4.html#mr.-trash-wheel-1",
    "title": "Numerical Data",
    "section": "Mr. Trash Wheel",
    "text": "Mr. Trash Wheel\n\nggplot(trashwheel, aes(PlasticBottles)) +\n  geom_histogram()\n\n#&gt; Warning: Removed 1 row containing non-finite outside the scale range (`stat_bin()`)."
  },
  {
    "objectID": "lectures/4.html#box-plot",
    "href": "lectures/4.html#box-plot",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\nA box plot, also known as a box-and-whisker plot, is a graphical representation of the distribution and key statistical characteristics of a dataset. It provides a visual summary of the data’s central tendency, spread, and potential outliers."
  },
  {
    "objectID": "lectures/4.html#box-plot-1",
    "href": "lectures/4.html#box-plot-1",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot"
  },
  {
    "objectID": "lectures/4.html#box-plot-r-code",
    "href": "lectures/4.html#box-plot-r-code",
    "title": "Numerical Data",
    "section": "Box Plot R Code",
    "text": "Box Plot R Code\n\nggplot(DATA, aes(VARIABLE)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/4.html#box-plot-2",
    "href": "lectures/4.html#box-plot-2",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\n\nggplot(trashwheel, aes(PlasticBottles)) +\n  geom_boxplot() \n\n#&gt; Warning: Removed 1 row containing non-finite outside the scale range (`stat_boxplot()`)."
  },
  {
    "objectID": "lectures/4.html#box-plot-3",
    "href": "lectures/4.html#box-plot-3",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\n\nggplot(trashwheel, aes(y = PlasticBottles)) +\n  geom_boxplot() \n\n#&gt; Warning: Removed 1 row containing non-finite outside the scale range (`stat_boxplot()`)."
  },
  {
    "objectID": "lectures/4.html#dot-plots",
    "href": "lectures/4.html#dot-plots",
    "title": "Numerical Data",
    "section": "Dot Plots",
    "text": "Dot Plots\nDot Plots are similar to histograms, but they incorporate dots to count how many data points fall within bins."
  },
  {
    "objectID": "lectures/4.html#dot-plots-in-r",
    "href": "lectures/4.html#dot-plots-in-r",
    "title": "Numerical Data",
    "section": "Dot Plots in R",
    "text": "Dot Plots in R\n\nggplot(DATA, aes(VARIABLE)) +\n  geom_dotplot(binwidth = X)"
  },
  {
    "objectID": "lectures/4.html#dot-plots-1",
    "href": "lectures/4.html#dot-plots-1",
    "title": "Numerical Data",
    "section": "Dot Plots",
    "text": "Dot Plots\n\n\nCode\nggplot(trashwheel, aes(PlasticBottles)) +\n  geom_dotplot(binwidth = 100)\n\n\n#&gt; Warning: Removed 1 row containing missing values or values outside the scale range (`stat_bindot()`)."
  },
  {
    "objectID": "lectures/4.html#transformed-data-1",
    "href": "lectures/4.html#transformed-data-1",
    "title": "Numerical Data",
    "section": "Transformed Data",
    "text": "Transformed Data\nWhen working with skewed data, it may be beneficial to transform the data to identify trends.\n\n\n\\(Y = \\ln(X)\\)\n\\(Y = \\frac{1}{X}\\)\n\\(Y = \\sqrt X\\)"
  },
  {
    "objectID": "lectures/4.html#example",
    "href": "lectures/4.html#example",
    "title": "Numerical Data",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(patchwork)\ny &lt;- rlnorm(1000, mean = 0, sd = 2)\nly &lt;- log(y)\n\np1 &lt;- ggplot(tibble(y), aes(y)) +\n  geom_histogram() +\n  ggtitle(\"Skewed Data\")\np2 &lt;- ggplot(tibble(y = ly), aes(y)) +\n  geom_histogram() +\n  ggtitle(\"Log-Transformed Data\")\n\np1 + p2"
  },
  {
    "objectID": "lectures/4.html#scatter-plots-1",
    "href": "lectures/4.html#scatter-plots-1",
    "title": "Numerical Data",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nScatter plots demonstrate how two variables behave with each other. They can tell you any postive or negative trends, if they exist, with the combination of the plots."
  },
  {
    "objectID": "lectures/4.html#positive-relationship",
    "href": "lectures/4.html#positive-relationship",
    "title": "Numerical Data",
    "section": "Positive Relationship",
    "text": "Positive Relationship"
  },
  {
    "objectID": "lectures/4.html#negative-relationship",
    "href": "lectures/4.html#negative-relationship",
    "title": "Numerical Data",
    "section": "Negative Relationship",
    "text": "Negative Relationship"
  },
  {
    "objectID": "lectures/4.html#no-relationship",
    "href": "lectures/4.html#no-relationship",
    "title": "Numerical Data",
    "section": "No Relationship",
    "text": "No Relationship"
  },
  {
    "objectID": "lectures/4.html#weak-positive-or-negative-relationship",
    "href": "lectures/4.html#weak-positive-or-negative-relationship",
    "title": "Numerical Data",
    "section": "Weak Positive or Negative Relationship",
    "text": "Weak Positive or Negative Relationship"
  },
  {
    "objectID": "lectures/4.html#scatter-plots-in-r",
    "href": "lectures/4.html#scatter-plots-in-r",
    "title": "Numerical Data",
    "section": "Scatter Plots in R",
    "text": "Scatter Plots in R\n\nggplot(DATA, aes(x = VAR1, y = VAR2)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/4.html#mr.-trash-wheel-2",
    "href": "lectures/4.html#mr.-trash-wheel-2",
    "title": "Numerical Data",
    "section": "Mr. Trash Wheel",
    "text": "Mr. Trash Wheel\n\nggplot(trashwheel, aes(PlasticBottles, PlasticBags)) +\n  geom_point()\n\n#&gt; Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`)."
  },
  {
    "objectID": "lectures/4.html#transformed-data-2",
    "href": "lectures/4.html#transformed-data-2",
    "title": "Numerical Data",
    "section": "Transformed Data",
    "text": "Transformed Data\n\nggplot(trashwheel, aes(log(PlasticBottles), log(PlasticBags))) +\n  geom_point()\n\n#&gt; Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`)."
  },
  {
    "objectID": "lectures/6.html#explaining-variation",
    "href": "lectures/6.html#explaining-variation",
    "title": "Multivariable Linear Regression",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\nThis is the process where we try to reduce the variation with the use of other variables.\n\n\nCan be thought of as getting it less wrong when taking an educated guess."
  },
  {
    "objectID": "lectures/6.html#taylor-swifts-songs-danceability",
    "href": "lectures/6.html#taylor-swifts-songs-danceability",
    "title": "Multivariable Linear Regression",
    "section": "Taylor Swift’s Songs Danceability",
    "text": "Taylor Swift’s Songs Danceability\n\n\nCode\nggplot(taylor_album_songs, aes(danceability)) +\n  geom_density()\n\n\n#&gt; Warning: Removed 3 rows containing non-finite outside the scale range (`stat_density()`)."
  },
  {
    "objectID": "lectures/6.html#danceability-by-mode_name",
    "href": "lectures/6.html#danceability-by-mode_name",
    "title": "Multivariable Linear Regression",
    "section": "Danceability by mode_name",
    "text": "Danceability by mode_name\n\n\nCode\ntaylor_album_songs |&gt; \n  drop_na(mode_name) |&gt; \n  ggplot(aes(danceability)) +\n  geom_density() +\n  facet_wrap(~ mode_name)"
  },
  {
    "objectID": "lectures/6.html#danceability-by-valence",
    "href": "lectures/6.html#danceability-by-valence",
    "title": "Multivariable Linear Regression",
    "section": "Danceability by Valence",
    "text": "Danceability by Valence\n\n\nCode\nggplot(taylor_album_songs, aes(valence, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n#&gt; Warning: Removed 3 rows containing non-finite outside the scale range (`stat_smooth()`).\n\n\n#&gt; Warning: Removed 3 rows containing missing values or values outside the scale range (`geom_point()`)."
  },
  {
    "objectID": "lectures/6.html#danceability-by-energy",
    "href": "lectures/6.html#danceability-by-energy",
    "title": "Multivariable Linear Regression",
    "section": "Danceability by Energy",
    "text": "Danceability by Energy\n\n\nCode\nggplot(taylor_album_songs, aes(energy, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n#&gt; Warning: Removed 3 rows containing non-finite outside the scale range (`stat_smooth()`).\n\n\n#&gt; Warning: Removed 3 rows containing missing values or values outside the scale range (`geom_point()`)."
  },
  {
    "objectID": "lectures/6.html#modelling-danceability",
    "href": "lectures/6.html#modelling-danceability",
    "title": "Multivariable Linear Regression",
    "section": "Modelling Danceability",
    "text": "Modelling Danceability\nHow do we use all the variables to explain danceability?"
  },
  {
    "objectID": "lectures/6.html#mlr",
    "href": "lectures/6.html#mlr",
    "title": "Multivariable Linear Regression",
    "section": "MLR",
    "text": "MLR\nMultivariable Linear Regression (MLR) is used to model an outcome variable (\\(Y\\)) by multiple predictor variables (\\(X_1, X_2, \\ldots, X_p\\)).\n\nUsing MLR, you propose that the ouctome variable was constructed from a set of predictors, with their corresponding regression coefficients (\\(\\beta\\)), and a bit of error\n\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP\n\\]"
  },
  {
    "objectID": "lectures/6.html#model-data",
    "href": "lectures/6.html#model-data",
    "title": "Multivariable Linear Regression",
    "section": "Model Data",
    "text": "Model Data\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP\n\\]\n\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n\\]\n\\[\n\\varepsilon_i \\sim DGP\n\\]"
  },
  {
    "objectID": "lectures/6.html#unknown-parameters",
    "href": "lectures/6.html#unknown-parameters",
    "title": "Multivariable Linear Regression",
    "section": "Unknown Parameters",
    "text": "Unknown Parameters\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n\\]\n\n\\[\n\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p\n\\]"
  },
  {
    "objectID": "lectures/6.html#estimated-model",
    "href": "lectures/6.html#estimated-model",
    "title": "Multivariable Linear Regression",
    "section": "Estimated Model",
    "text": "Estimated Model\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n\\]\n\\[\n\\hat Y_i = \\hat\\beta_0 + \\hat\\beta_1 X_{i1} + \\hat\\beta_2 X_{i2} + \\cdots + \\hat\\beta_p X_{ip}\n\\]"
  },
  {
    "objectID": "lectures/6.html#estimating-prameters",
    "href": "lectures/6.html#estimating-prameters",
    "title": "Multivariable Linear Regression",
    "section": "Estimating Prameters",
    "text": "Estimating Prameters\n\\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p\\) are estimated by minimizing the following function:\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]"
  },
  {
    "objectID": "lectures/6.html#fitting-a-model-in-r",
    "href": "lectures/6.html#fitting-a-model-in-r",
    "title": "Multivariable Linear Regression",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\n\nCode\nlm(Y ~ X1 + X2 + ... + Xp, data = DATA)"
  },
  {
    "objectID": "lectures/6.html#modelling-danceability-1",
    "href": "lectures/6.html#modelling-danceability-1",
    "title": "Multivariable Linear Regression",
    "section": "Modelling Danceability",
    "text": "Modelling Danceability\n\n\nCode\nlm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = danceability ~ mode_name + valence + energy, data = taylor_album_songs)\n#&gt; \n#&gt; Coefficients:\n#&gt;    (Intercept)  mode_nameminor         valence          energy  \n#&gt;        0.53753         0.07322         0.17872        -0.06349"
  },
  {
    "objectID": "lectures/6.html#estimated-model-1",
    "href": "lectures/6.html#estimated-model-1",
    "title": "Multivariable Linear Regression",
    "section": "Estimated Model",
    "text": "Estimated Model\n\\[\ndance = 0.54 + 0.07\\times Minor + 0.18 \\times valence - 0.06 \\times energy\n\\]"
  },
  {
    "objectID": "lectures/6.html#hat-beta_i-representation",
    "href": "lectures/6.html#hat-beta_i-representation",
    "title": "Multivariable Linear Regression",
    "section": "\\(\\hat \\beta_i\\) Representation",
    "text": "\\(\\hat \\beta_i\\) Representation\nEach regression coefficient \\(\\beta_i\\) represents how the predictor variable changes the outcome, as it increase by 1 unit.\n\nFor categorical dummy variables, the \\(\\beta_i\\) represents how the outcome will change when the data point belongs to that value."
  },
  {
    "objectID": "lectures/6.html#hat-beta_i-interpretation",
    "href": "lectures/6.html#hat-beta_i-interpretation",
    "title": "Multivariable Linear Regression",
    "section": "\\(\\hat \\beta_i\\) Interpretation",
    "text": "\\(\\hat \\beta_i\\) Interpretation\nFor \\(hat \\beta_i\\), which is the regression coefficient (slope) of \\(X_i\\):\nAs \\(X_i\\) increases by 1 unit, the outcome (\\(Y\\)) will increase/decrease by \\(\\hat \\beta_i\\) units, adjusting for all other predictor variables.\n\nFor categorical dummy variables \\(X_i\\):\nThe outcome \\(Y_i\\) increases/decreases by \\(\\beta_i\\) units for category \\(X_i\\) compared to the reference category, adjusting for all other predictor variables."
  },
  {
    "objectID": "lectures/6.html#intepreting-minor-coefficient",
    "href": "lectures/6.html#intepreting-minor-coefficient",
    "title": "Multivariable Linear Regression",
    "section": "Intepreting Minor coefficient",
    "text": "Intepreting Minor coefficient\n\\[\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy\n\\] Minor song’s average danceability score is 0.07 units higher compared to Major song’s, adjusting for valence and energy."
  },
  {
    "objectID": "lectures/6.html#intepreting-valence-coefficient",
    "href": "lectures/6.html#intepreting-valence-coefficient",
    "title": "Multivariable Linear Regression",
    "section": "Intepreting valence coefficient",
    "text": "Intepreting valence coefficient\n\\[\ndance = 0.54 + 0.07Minor + 0.18  valence - 0.06 energy\n\\]\nAs valence increases by 1 unit, danceability increases by an average of 0.18 units, adjusting for energy and type of song."
  },
  {
    "objectID": "lectures/6.html#intepreting-energy-coefficient",
    "href": "lectures/6.html#intepreting-energy-coefficient",
    "title": "Multivariable Linear Regression",
    "section": "Intepreting energy coefficient",
    "text": "Intepreting energy coefficient\n\\[\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy\n\\]\nAs energy increases by 1 unit, danceability decreases by an average of 0.06 units, adjusting for valence and type of song."
  },
  {
    "objectID": "lectures/6.html#r2",
    "href": "lectures/6.html#r2",
    "title": "Multivariable Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nComputing \\(R^2\\) is done by determining how much the variation in the outcome is explained by model and divided by the variation of the outcome.\n\\[\nR^2 = \\frac{\\text{variation explained by model}}{\\text{variation from outcome}} \\\\\n= 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\n\\]"
  },
  {
    "objectID": "lectures/6.html#r2-1",
    "href": "lectures/6.html#r2-1",
    "title": "Multivariable Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nProblems arise when multiple predictors are added to the model. As a new predictor is added to the model, new information is added to the model which will always reduce the variation in the residuals. Therefore, the \\(R^2\\) will always increase."
  },
  {
    "objectID": "lectures/6.html#problems-with-r2-in-mlr",
    "href": "lectures/6.html#problems-with-r2-in-mlr",
    "title": "Multivariable Linear Regression",
    "section": "Problems with \\(R^2\\) in MLR",
    "text": "Problems with \\(R^2\\) in MLR\nWhen the number of variables increase, the regular \\(R²\\) will be biased in its prediction capability when new data is obtained.\n\nTherefore, statisticians uses the adjusted \\(R^2\\), that penalizes the model when more variables are added. This ensures that a variable added will have a significant effect in predicting outcomes."
  },
  {
    "objectID": "lectures/6.html#adjusted-r2-1",
    "href": "lectures/6.html#adjusted-r2-1",
    "title": "Multivariable Linear Regression",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\\[\nR_a^2 = 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\\times \\frac{n-1}{n-k-1}\n\\]\n\n\\(n\\): Number of data points\n\\(k\\): Number of predictor variables in the model"
  },
  {
    "objectID": "lectures/6.html#adjusted-r2-in-r",
    "href": "lectures/6.html#adjusted-r2-in-r",
    "title": "Multivariable Linear Regression",
    "section": "Adjusted \\(R^2\\) in R",
    "text": "Adjusted \\(R^2\\) in R\n\n\nCode\nxlm &lt;- lm(Y ~ X1 + X2 + ... + Xp, data = DATA)\nar2(xlm)"
  },
  {
    "objectID": "lectures/6.html#example",
    "href": "lectures/6.html#example",
    "title": "Multivariable Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nxlm &lt;- lm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\nar2(xlm)\n\n\n#&gt; [1] 0.08106907\n\n\n8.1% of the variation in danceability is explained by the model."
  },
  {
    "objectID": "lectures/6.html#model-selection-1",
    "href": "lectures/6.html#model-selection-1",
    "title": "Multivariable Linear Regression",
    "section": "Model Selection",
    "text": "Model Selection\nModel Selection is the process of obtaining a “final” model containing all the necessary predictors, and eliminating any that are not necessary."
  },
  {
    "objectID": "lectures/6.html#forward-selection",
    "href": "lectures/6.html#forward-selection",
    "title": "Multivariable Linear Regression",
    "section": "Forward Selection",
    "text": "Forward Selection\nBegin with the null model (\\(Y\\sim 1\\)) and add variables until a final model is chosen."
  },
  {
    "objectID": "lectures/6.html#backward-selection",
    "href": "lectures/6.html#backward-selection",
    "title": "Multivariable Linear Regression",
    "section": "Backward Selection",
    "text": "Backward Selection\nBegin with the full model, and remove variable until the final model is chosen."
  },
  {
    "objectID": "lectures/6.html#hybrid-selection",
    "href": "lectures/6.html#hybrid-selection",
    "title": "Multivariable Linear Regression",
    "section": "Hybrid Selection",
    "text": "Hybrid Selection\nA hybrid approach between the forward and backward building approach."
  },
  {
    "objectID": "lectures/6.html#about-model-selection",
    "href": "lectures/6.html#about-model-selection",
    "title": "Multivariable Linear Regression",
    "section": "About Model Selection",
    "text": "About Model Selection\nGenerally, it is not a good idea to conduct model selection. The predictor variables in your model should be guided by a literature review that illustrates important predictor variables in a model."
  },
  {
    "objectID": "lectures/8.html#sampling-distribution-1",
    "href": "lectures/8.html#sampling-distribution-1",
    "title": "Randomization Tests",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nSampling Distribution is the idea that the statistics that you generate (slopes and intercepts) have their own data generation process.\n\nIn other words, the numerical values you obtain from the lm and glm function can be different if we got a different data set.\n\n\nSome values will be more common than others. Because of this, they have their own data generating process, like the outcome of interest has it’s own data generating process."
  },
  {
    "objectID": "lectures/8.html#modelling-the-data",
    "href": "lectures/8.html#modelling-the-data",
    "title": "Randomization Tests",
    "section": "Modelling the Data",
    "text": "Modelling the Data\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\n\n\\(Y_i\\): Outcome data\n\\(X_i\\): Predictor data\n\\(\\beta_0, \\beta_1\\): parameters\n\\(\\varepsilon_i\\): error term"
  },
  {
    "objectID": "lectures/8.html#error-term",
    "href": "lectures/8.html#error-term",
    "title": "Randomization Tests",
    "section": "Error Term",
    "text": "Error Term\n\\[\n\\varepsilon_i \\sim DGP\n\\]\n\n\nThe error terms forces the outcome variable to be different from the mathematical model.\nThe numbers being generated are random and cannot be predicted."
  },
  {
    "objectID": "lectures/8.html#randomness-effect",
    "href": "lectures/8.html#randomness-effect",
    "title": "Randomization Tests",
    "section": "Randomness Effect",
    "text": "Randomness Effect"
  },
  {
    "objectID": "lectures/8.html#simulating-unicorns-1",
    "href": "lectures/8.html#simulating-unicorns-1",
    "title": "Randomization Tests",
    "section": "Simulating Unicorns",
    "text": "Simulating Unicorns\nTo better understand the variation in statistics, let’s simulate a data set of unicorn characteristics to visualize and understand the variation.\n\nWe will simulate a data set using then unicorns function and only need to specify how many unicorns you want to simulate."
  },
  {
    "objectID": "lectures/8.html#simulating-unicorn-data",
    "href": "lectures/8.html#simulating-unicorn-data",
    "title": "Randomization Tests",
    "section": "Simulating Unicorn Data",
    "text": "Simulating Unicorn Data\n\n\nCode\nunicorns(10)\n\n\n#&gt; # A tibble: 10 × 15\n#&gt;    Unicorn_ID   Age Gender      Color Type_of_Unicorn Type_of_Horn Horn_Length Horn_Strength Weight Health_Score Personality_Score Magical_Score Elusiveness_Score Gentleness_Score Nature_Score\n#&gt;         &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1          1     2 Genderfluid Pink  Rainbow         Opal                4.99          27.8  121.             6             2.59         10762.              33.5            16.6          917.\n#&gt;  2          2     4 Female      Black Rainbow         Opal                5.37          26.2  108.             2             1.64         10819.              33.1            53.4          924.\n#&gt;  3          3    15 Non-binary  Gold  Rainbow         Aquamarine          5.15          27.6  180.             8             1.53         11062.              42.3            29.9          955.\n#&gt;  4          4    11 Female      Gray  Ember           Opal                4.96          29.5  135.             9             0.754        10992.              33.3            -1.42         946.\n#&gt;  5          5    11 Agender     White Rainbow         Aquamarine          5.22          27.5  175.             2             2.09         10967.              38.4            37.0          943.\n#&gt;  6          6    10 Non-binary  Pink  Jewel           Opal                5.01          33.0  124.             4             0.681        11009.              33.8            27.2          948.\n#&gt;  7          7     8 Non-binary  White Rainbow         Aquamarine          5.04          26.5  110.             5             1.18         10897.              30.9            45.5          934.\n#&gt;  8          8    20 Non-binary  Black Jewel           Aquamarine          5.15          25.2  154.             7             4.68         11281.              35.5            26.0          983.\n#&gt;  9          9     6 Female      Gray  Ruvas           Aquamarine          4.80          27.7   80.7            5             0.974        10852.              35.0            -2.49         928.\n#&gt; 10         10    14 Male        Black Ruvas           Opal                5.03          27.2  106.             9             0.861        11102.              34.9             3.83         960."
  },
  {
    "objectID": "lectures/8.html#unicorn-data-variables",
    "href": "lectures/8.html#unicorn-data-variables",
    "title": "Randomization Tests",
    "section": "Unicorn Data Variables",
    "text": "Unicorn Data Variables\n\n\nCode\nnames(unicorns(10))\n\n\n#&gt;  [1] \"Unicorn_ID\"        \"Age\"               \"Gender\"            \"Color\"             \"Type_of_Unicorn\"   \"Type_of_Horn\"      \"Horn_Length\"       \"Horn_Strength\"     \"Weight\"            \"Health_Score\"     \n#&gt; [11] \"Personality_Score\" \"Magical_Score\"     \"Elusiveness_Score\" \"Gentleness_Score\"  \"Nature_Score\"\n\n\nWe will only look at Magical_Score and Nature_Score."
  },
  {
    "objectID": "lectures/8.html#magical-and-nature-score",
    "href": "lectures/8.html#magical-and-nature-score",
    "title": "Randomization Tests",
    "section": "Magical and Nature Score",
    "text": "Magical and Nature Score\n\\[\nMagical =  3423 + 8 \\times Nature + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim N(0, 3.24)\n\\]"
  },
  {
    "objectID": "lectures/8.html#simulating-n0-3.24",
    "href": "lectures/8.html#simulating-n0-3.24",
    "title": "Randomization Tests",
    "section": "Simulating \\(N(0, 3.24)\\)",
    "text": "Simulating \\(N(0, 3.24)\\)\n\n\nCode\nrnorm(1, 0, 1.8)\n\n\n#&gt; [1] 2.278277"
  },
  {
    "objectID": "lectures/8.html#collecting",
    "href": "lectures/8.html#collecting",
    "title": "Randomization Tests",
    "section": "Collecting",
    "text": "Collecting\n\n\nCode\nunicorns(10) |&gt; select(Nature_Score, Magical_Score)\n\n\n#&gt; # A tibble: 10 × 2\n#&gt;    Nature_Score Magical_Score\n#&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1         920.        10779.\n#&gt;  2         973.        11204.\n#&gt;  3         915.        10739.\n#&gt;  4         980.        11269.\n#&gt;  5         929.        10860.\n#&gt;  6         954.        11056.\n#&gt;  7         930.        10860.\n#&gt;  8         942.        10961.\n#&gt;  9         946.        10994.\n#&gt; 10         926.        10831."
  },
  {
    "objectID": "lectures/8.html#dgp-of-magical-score-1",
    "href": "lectures/8.html#dgp-of-magical-score-1",
    "title": "Randomization Tests",
    "section": "DGP of Magical Score 1",
    "text": "DGP of Magical Score 1\n\n\nCode\nggplot(unicorns(500), aes(Magical_Score)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/8.html#dgp-of-magical-score-2",
    "href": "lectures/8.html#dgp-of-magical-score-2",
    "title": "Randomization Tests",
    "section": "DGP of Magical Score 2",
    "text": "DGP of Magical Score 2\n\n\nCode\nggplot(unicorns(500), aes(Magical_Score)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/8.html#estimating-beta_1-via-lm",
    "href": "lectures/8.html#estimating-beta_1-via-lm",
    "title": "Randomization Tests",
    "section": "Estimating \\(\\beta_1\\) via lm",
    "text": "Estimating \\(\\beta_1\\) via lm\n\n\nCode\nu1 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u1)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3433.491         7.989"
  },
  {
    "objectID": "lectures/8.html#collecting-a-new-sample",
    "href": "lectures/8.html#collecting-a-new-sample",
    "title": "Randomization Tests",
    "section": "Collecting a new sample",
    "text": "Collecting a new sample\n\n\nCode\nu2 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u2)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u2)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3424.212         7.999"
  },
  {
    "objectID": "lectures/8.html#collecting-a-new-sample-1",
    "href": "lectures/8.html#collecting-a-new-sample-1",
    "title": "Randomization Tests",
    "section": "Collecting a new sample",
    "text": "Collecting a new sample\n\n\nCode\nu3 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u3)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u3)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3424.178         7.999"
  },
  {
    "objectID": "lectures/8.html#collecting-a-new-sample-2",
    "href": "lectures/8.html#collecting-a-new-sample-2",
    "title": "Randomization Tests",
    "section": "Collecting a new sample",
    "text": "Collecting a new sample\n\n\nCode\nu4 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u4)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u4)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3429.370         7.993"
  },
  {
    "objectID": "lectures/8.html#collecting-1000-samples",
    "href": "lectures/8.html#collecting-1000-samples",
    "title": "Randomization Tests",
    "section": "Collecting 1000 Samples",
    "text": "Collecting 1000 Samples\n\n\nCode\nbetas &lt;- replicate(1000,\n                   b(Magical_Score ~ Nature_Score, 1, unicorns(500)))\n\nbetas\n\n\n#&gt;    [1] 8.001255 7.992453 7.999343 8.005823 8.000710 7.999781 7.997726 7.999512 7.999101 8.003225 7.996390 7.993764 7.999847 8.000417 8.003056 8.001019 7.997537 8.000407 8.002184 7.999232 8.001242 8.002330 8.004709\n#&gt;   [24] 8.004690 7.995788 7.997278 7.997508 8.000461 8.004561 7.998831 8.008543 7.996063 7.999354 8.002189 8.004922 7.990975 8.002928 8.004257 8.005203 7.996880 8.002255 7.991368 7.997307 8.001478 7.994456 7.997678\n#&gt;   [47] 7.997048 8.004792 7.999704 7.997330 7.998805 8.007506 7.994847 8.002333 7.992776 7.996405 8.014476 8.009902 8.005587 7.990489 8.004463 8.002895 7.997455 7.999188 7.995108 7.997888 7.999190 8.001474 8.001455\n#&gt;   [70] 8.003676 8.003235 7.995725 8.004625 8.001586 8.000428 8.000742 8.006186 8.004568 7.999210 7.997078 8.001424 8.002736 8.000569 7.993641 7.993541 7.993028 7.993966 7.999231 8.001121 8.005623 8.014221 8.000585\n#&gt;   [93] 8.001006 7.998490 7.997128 8.002733 8.007139 7.994807 7.998411 7.995108 8.002814 7.996777 8.001536 8.002280 7.996463 7.990522 7.999892 8.000692 8.000303 7.994132 7.992591 8.004693 7.995858 8.002965 7.999006\n#&gt;  [116] 7.999816 8.005320 8.000924 8.004907 8.000558 8.001831 8.004566 7.998109 7.991353 7.998766 7.999266 7.995890 8.003041 8.000638 8.003315 7.999406 8.005077 8.001966 8.001305 7.996714 8.005907 7.997003 7.995407\n#&gt;  [139] 7.997614 8.007502 7.998757 7.998483 7.994954 8.000608 7.990699 7.998869 7.990083 8.000592 8.006462 8.002260 7.998538 8.000208 8.003851 7.999202 7.999222 8.004044 7.999884 7.995850 7.998828 8.002472 8.004701\n#&gt;  [162] 8.005915 7.995387 7.993068 7.988268 8.005387 7.996792 7.997493 7.994016 8.001878 7.998964 8.008201 7.998731 7.990647 8.001382 8.005756 8.018648 8.003967 7.993205 7.993077 7.996754 8.002307 8.000217 8.003897\n#&gt;  [185] 7.998670 7.998995 8.001025 7.995255 8.001280 7.995567 8.003152 8.002558 7.999434 7.998121 7.998213 8.001043 8.004523 7.996273 8.000599 8.000238 8.010942 7.997437 8.000389 7.996749 7.994510 7.997178 8.003518\n#&gt;  [208] 7.996621 7.998222 8.003652 8.005038 8.001474 8.001968 8.001863 7.996394 7.997817 7.994379 8.002831 8.001702 7.998730 8.003577 8.002717 7.997608 8.001038 8.000247 8.000492 7.997068 7.996173 8.002786 8.001030\n#&gt;  [231] 8.002635 8.009380 7.995761 8.002738 7.992479 7.999004 8.000989 7.999310 8.000511 8.002894 8.001906 8.006290 8.003384 7.995907 8.006741 7.996838 8.002513 8.006741 7.997175 8.005336 8.004701 8.003229 7.997711\n#&gt;  [254] 8.001788 8.001383 7.997527 8.007385 8.006241 8.004109 8.012530 7.998152 7.999467 8.000587 7.995885 7.997052 8.001188 7.994955 7.995145 8.001595 7.998851 7.999592 7.992276 7.997955 7.993697 7.997914 7.994994\n#&gt;  [277] 8.004746 8.001407 7.999802 7.998787 7.998329 8.002371 8.003589 7.995864 8.002899 7.998341 8.005140 8.003500 8.004846 8.004262 8.002107 7.998248 7.996731 7.996545 8.001471 8.002064 8.001215 7.997520 8.000356\n#&gt;  [300] 8.003130 7.992917 7.995532 8.007630 7.999062 7.997575 8.006776 8.003338 7.995350 8.004410 7.996469 8.000116 7.998926 7.993992 7.997495 8.001274 7.990026 7.996241 8.005680 8.002234 8.004088 7.992671 8.003651\n#&gt;  [323] 8.003119 7.994927 8.003292 8.002099 7.999531 7.999984 7.998282 8.006987 8.004022 8.006061 7.997439 8.003166 7.997610 7.991666 8.006528 8.003432 7.996464 8.000910 8.006753 7.999177 7.993526 7.995304 8.001827\n#&gt;  [346] 7.995558 8.000368 7.998116 7.997622 8.007479 8.005939 7.998431 8.002467 7.998867 8.002594 7.999503 8.006584 8.002195 7.999128 8.004723 7.997563 7.999405 7.996193 8.000480 7.999040 7.989779 7.997618 7.993243\n#&gt;  [369] 7.992374 8.003248 7.999763 7.994922 7.992058 7.995176 7.995228 7.997218 8.003556 8.004211 8.001695 7.994872 7.997762 8.004335 7.998548 8.005644 7.998017 8.004685 8.000491 7.997471 8.000884 8.006391 8.004629\n#&gt;  [392] 7.998375 8.004816 7.992927 8.002163 7.992457 8.002215 7.998828 8.003335 7.995448 8.001175 7.999364 8.000650 7.993445 8.007786 8.005078 8.003714 8.004187 7.993890 8.002283 7.994022 7.999269 8.002017 8.007412\n#&gt;  [415] 8.004770 7.997981 8.004327 7.998840 8.002313 7.999883 7.999515 7.995730 7.996485 7.999673 7.996710 7.999365 8.006839 8.000764 8.000416 8.002264 8.003454 8.007574 7.999767 7.999320 8.003950 7.993589 7.992462\n#&gt;  [438] 7.995199 7.998508 8.004083 7.995282 8.002728 7.999052 8.002762 8.004196 7.999523 7.993897 8.002355 8.006035 7.996700 7.998971 7.997435 8.007472 8.001774 8.002015 8.000020 7.988128 8.004899 8.005479 7.998637\n#&gt;  [461] 7.998551 7.998603 8.006008 8.000009 8.000133 8.005015 7.996266 7.996882 8.003624 8.001050 7.995501 8.003236 7.995789 7.991793 7.992897 7.999753 7.994799 7.998966 7.993526 8.005220 8.001964 8.006276 7.998629\n#&gt;  [484] 8.001441 7.999001 8.000858 8.014383 7.998001 7.999737 8.000313 8.002916 7.997470 7.997694 7.998002 8.006092 8.006434 7.988973 7.991901 8.002075 7.998648 7.994163 7.999997 7.998027 7.997132 7.999020 7.996900\n#&gt;  [507] 7.998546 7.996514 7.993650 8.002240 7.996646 7.994918 8.001627 7.993582 7.999205 7.994238 8.000125 7.999372 8.006593 8.002393 8.003716 8.008257 8.000376 8.001626 8.006167 8.001032 7.999024 7.997154 7.997655\n#&gt;  [530] 7.998353 8.002297 7.999485 8.011278 7.995409 7.993309 7.997688 8.002184 8.007058 7.993467 8.004288 8.000398 8.001501 7.995619 7.994292 7.998555 8.002718 8.000914 8.000067 7.998960 7.998534 7.999130 7.999809\n#&gt;  [553] 8.004436 8.000169 8.004503 7.998282 7.990379 7.999694 7.996538 8.003096 8.004276 8.002383 8.000014 8.003088 7.991322 8.005335 7.999611 7.998709 7.994055 7.996595 8.002474 7.991171 8.002384 8.004269 8.003841\n#&gt;  [576] 8.007428 7.998956 8.000234 8.004750 8.000089 8.003189 8.002731 7.998123 8.000439 7.996142 7.997328 8.000682 7.998839 8.000894 8.013405 8.001009 7.998222 8.002298 7.996810 8.005548 8.006936 7.999252 7.997598\n#&gt;  [599] 8.001970 8.000485 7.996522 7.999632 8.002405 8.003753 7.994922 8.004333 7.999821 8.006971 7.994230 8.003497 8.000895 7.999163 7.998341 8.000088 8.003672 8.000453 8.003247 7.999884 8.003800 8.000891 8.004621\n#&gt;  [622] 8.001404 8.003632 7.995903 8.001889 7.995085 8.003777 7.993620 7.997929 7.988339 8.000555 7.997409 7.990026 8.005505 8.001209 7.996479 8.002492 7.999873 8.000048 7.995780 8.006749 7.992300 7.999507 7.995393\n#&gt;  [645] 8.003586 7.995281 8.002921 8.002440 8.001489 7.997121 8.001360 8.001307 8.002521 8.000021 8.001179 7.996497 8.001741 7.997852 8.007067 7.999708 7.994365 7.999552 8.005871 7.997420 7.995760 7.997766 7.999617\n#&gt;  [668] 8.004437 7.994362 7.999584 8.005580 8.002497 7.995458 7.997847 7.995384 7.990549 8.004961 8.005078 7.998567 8.004815 8.006321 8.006544 8.008949 8.004946 7.995037 8.004646 7.998003 8.000635 8.000302 8.002351\n#&gt;  [691] 8.009100 8.003560 7.999272 7.998413 7.997694 8.001950 7.997775 8.000430 7.999527 8.000386 7.996954 8.003329 7.994738 8.005889 7.998380 7.999572 8.003243 8.008880 7.994569 8.000530 7.992927 8.001305 8.004806\n#&gt;  [714] 8.005112 8.001341 8.002326 7.998777 7.998029 7.991902 8.004943 8.000850 7.996307 7.996075 7.999372 8.001828 7.998587 7.999012 7.997992 7.994675 8.002565 7.999400 7.997213 7.996240 7.997413 8.006449 7.994725\n#&gt;  [737] 8.006827 8.001711 7.998191 7.998483 8.004585 7.992692 8.000826 7.998521 8.000870 7.997029 7.994092 7.999165 7.999461 8.002150 8.004002 8.001065 8.000200 8.000707 7.992955 7.995585 7.996499 8.007952 8.001151\n#&gt;  [760] 7.996643 7.997483 7.998786 8.003282 8.003088 8.000151 8.004074 7.991284 8.010860 8.000534 8.007501 8.002507 7.997697 7.998352 7.999234 7.996460 7.997274 7.998526 7.997978 8.006309 7.997780 7.996778 8.004498\n#&gt;  [783] 7.995935 8.000401 8.004363 8.002564 7.999875 7.999730 7.998181 7.993333 7.999395 8.001483 8.005137 7.995002 8.006015 8.001039 7.996869 8.008532 7.991951 7.995787 7.999984 7.999014 8.001975 7.997122 8.004272\n#&gt;  [806] 7.997841 7.997744 7.995446 8.006146 8.001328 8.000657 7.998805 7.992894 8.003562 7.992761 8.001720 8.001305 7.991201 8.004370 8.001767 7.999498 7.999386 7.996181 7.995813 8.000712 7.999691 7.992353 7.997081\n#&gt;  [829] 8.003510 8.004440 7.999823 8.006614 8.003193 7.999240 7.999531 7.997710 7.992607 7.998796 8.004112 7.999911 7.998510 7.991570 7.998481 7.993918 8.004866 7.997165 8.001597 8.001704 8.003878 8.000220 8.000844\n#&gt;  [852] 7.999865 8.001320 7.999737 7.998038 7.997607 7.995421 7.997252 8.001297 7.999014 8.001181 7.999582 8.000584 7.996053 8.000881 8.004496 7.999976 8.001812 8.002931 8.000264 8.005998 7.998305 8.002813 8.004085\n#&gt;  [875] 8.004141 8.003589 7.991012 8.004121 8.005524 8.002012 8.006316 8.001254 7.994691 8.005730 7.991580 8.004444 8.003127 8.007209 8.009768 8.001368 8.000517 8.000803 8.003360 7.994986 7.999374 8.001876 8.001043\n#&gt;  [898] 7.993863 8.009552 8.003852 8.002222 7.996909 8.009530 7.998692 7.998834 8.001472 8.000386 7.997918 8.002765 8.000431 8.005088 7.994998 8.003191 8.003736 8.000466 8.010706 8.006900 7.996472 7.999244 7.999713\n#&gt;  [921] 8.000078 7.996039 8.001705 8.007055 7.997500 8.005995 8.006221 8.000118 7.998413 8.001651 8.003078 8.003302 7.999440 7.993533 8.008660 7.995938 7.993130 7.999774 8.000575 8.000425 7.996296 8.000673 8.001646\n#&gt;  [944] 7.995788 8.000513 7.998563 7.998806 7.993536 7.998358 8.000231 7.996207 7.996439 7.999699 8.000695 8.004675 8.003500 7.999092 7.993381 8.000244 7.997153 8.000179 7.998922 7.998751 8.001345 7.997772 7.999223\n#&gt;  [967] 8.003086 8.001134 8.010650 7.995934 8.000172 8.006464 7.994085 8.007558 8.000547 8.004934 7.998890 8.001463 7.997066 8.000321 8.004631 8.008018 7.998343 7.999005 7.995193 8.000114 7.998655 7.996514 7.997445\n#&gt;  [990] 7.997490 8.004470 8.001418 8.000230 8.001431 7.992112 7.993829 8.005906 7.998882 7.996081 7.996542"
  },
  {
    "objectID": "lectures/8.html#distributions-of-hat-beta_1",
    "href": "lectures/8.html#distributions-of-hat-beta_1",
    "title": "Randomization Tests",
    "section": "Distributions of \\(\\hat \\beta_1\\)",
    "text": "Distributions of \\(\\hat \\beta_1\\)\n\n\nCode\nggplot(tibble(x = betas), aes(x = x)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/8.html#inference",
    "href": "lectures/8.html#inference",
    "title": "Randomization Tests",
    "section": "Inference",
    "text": "Inference\nIn the real world, we do not know how the model is generated; therefore, the parameters that are being estimated are unknown.\n\nWe use statistics to determine what is our best guess of what the true parameter is.\n\n\nWe can use processes known as inferential procedures to determine which values of the parameter it cannot be. This is known as inference."
  },
  {
    "objectID": "lectures/8.html#hypothesis-tests",
    "href": "lectures/8.html#hypothesis-tests",
    "title": "Randomization Tests",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/8.html#null-hypothesis-h_0",
    "href": "lectures/8.html#null-hypothesis-h_0",
    "title": "Randomization Tests",
    "section": "Null Hypothesis \\(H_0\\)",
    "text": "Null Hypothesis \\(H_0\\)\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value."
  },
  {
    "objectID": "lectures/8.html#alternative-hypothesis-h_a",
    "href": "lectures/8.html#alternative-hypothesis-h_a",
    "title": "Randomization Tests",
    "section": "Alternative Hypothesis \\(H_a\\)",
    "text": "Alternative Hypothesis \\(H_a\\)\nThe alternative hypothesis contradicts the null hypothesis."
  },
  {
    "objectID": "lectures/8.html#example-of-null-and-alternative-hypothesis",
    "href": "lectures/8.html#example-of-null-and-alternative-hypothesis",
    "title": "Randomization Tests",
    "section": "Example of Null and Alternative Hypothesis",
    "text": "Example of Null and Alternative Hypothesis\nWe want to see if \\(\\mu\\) (the true value) is different from \\(\\mu_0\\) (a number we want to test)\n\n\n\nNull Hypothesis\nAlternative Hypothesis\n\n\n\n\n\\(H_0: \\mu=\\mu_0\\)\n\\(H_a: \\mu\\ne\\mu_0\\)\n\n\n\\(H_0: \\mu\\le\\mu_0\\)\n\\(H_a: \\mu&gt;\\mu_0\\)\n\n\n\\(H_0: \\mu\\ge\\mu_0\\)\n\\(H_0: \\mu&lt;\\mu_0\\)"
  },
  {
    "objectID": "lectures/8.html#hypothesis-test",
    "href": "lectures/8.html#hypothesis-test",
    "title": "Randomization Tests",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nThe idea of hypothesis testing is to generate the sampling distribution of a statistic assuming the null hypothesis is true.\n\nCompute the statistic from your data.\n\n\nAfterwards, see where your statistic lies in comparison of your null sampling distribution."
  },
  {
    "objectID": "lectures/8.html#comparison-of-null-distribution",
    "href": "lectures/8.html#comparison-of-null-distribution",
    "title": "Randomization Tests",
    "section": "Comparison of Null Distribution",
    "text": "Comparison of Null Distribution\nIf your statistic lies near the mounds, it is believed that your data came from the null distribution.\nIf your statistic is more in the tail regions, it is believed that your data did not come from your null distribution."
  },
  {
    "objectID": "lectures/8.html#motivation",
    "href": "lectures/8.html#motivation",
    "title": "Randomization Tests",
    "section": "Motivation",
    "text": "Motivation\nThe bacteria data set contians information on whether bacteria (y: y or n) is present after utilizing treatments (ap: active or placebo).\n\nWe are interesting in determine the proportion of having bacteria present is different for those taking an “active” or “placebo”."
  },
  {
    "objectID": "lectures/8.html#crosstabulation",
    "href": "lectures/8.html#crosstabulation",
    "title": "Randomization Tests",
    "section": "Crosstabulation",
    "text": "Crosstabulation\n\n\nCode\ncat_stats(bacteria$ap, bacteria$y)"
  },
  {
    "objectID": "lectures/8.html#comparing-proportions-1",
    "href": "lectures/8.html#comparing-proportions-1",
    "title": "Randomization Tests",
    "section": "Comparing Proportions",
    "text": "Comparing Proportions\nWe are interesting in determining if different groups see different proportions of a binary outcome.\nWe compute the proportions of observing the binary outcome in Group 1 and Group 2 and see if they are fundamentally different from each other."
  },
  {
    "objectID": "lectures/8.html#by-2-cross-tabulations",
    "href": "lectures/8.html#by-2-cross-tabulations",
    "title": "Randomization Tests",
    "section": "2 by 2 Cross Tabulations",
    "text": "2 by 2 Cross Tabulations\n\n\n\nGroups\nOutcome 1\nOutcome 2\n\n\nGroup 1\n\\(p_{11}\\)\n\\(p_{21}\\)\n\n\nGroup 2\n\\(p_{12}\\)\n\\(p_{22}\\)\n\n\n\n\nWe want to compare \\(p_{11}\\) and \\(p_{12}\\), to determine if the probability of outcome 1 are the same for both groups."
  },
  {
    "objectID": "lectures/8.html#test-statistic",
    "href": "lectures/8.html#test-statistic",
    "title": "Randomization Tests",
    "section": "Test Statistic",
    "text": "Test Statistic\nWe can use both \\(p_{11}\\) and \\(p_{12}\\) to determine if there is a fundamental difference.\n\nHowever, it will be more beneficial to utilize one statistic to contruct the sampling distribution.\n\n\n\\[\nT =  p_{11} - p_{12}\n\\]"
  },
  {
    "objectID": "lectures/8.html#obtain-proportion-in-r",
    "href": "lectures/8.html#obtain-proportion-in-r",
    "title": "Randomization Tests",
    "section": "Obtain Proportion in R",
    "text": "Obtain Proportion in R\n\n\nCode\nprops(DATA$GROUP, DATA$OUTCOME, VAL)"
  },
  {
    "objectID": "lectures/8.html#obtain-difference-in-r",
    "href": "lectures/8.html#obtain-difference-in-r",
    "title": "Randomization Tests",
    "section": "Obtain Difference in R",
    "text": "Obtain Difference in R\n\n\nCode\nprops(DATA$GROUP, DATA$OUTCOME, VAL, diff = TRUE)"
  },
  {
    "objectID": "lectures/8.html#bacteria-example",
    "href": "lectures/8.html#bacteria-example",
    "title": "Randomization Tests",
    "section": "Bacteria Example",
    "text": "Bacteria Example\n\n\nCode\nprops(bacteria$ap, bacteria$y, \"y\")\n\n\n#&gt;     a     p \n#&gt; 0.750 0.875"
  },
  {
    "objectID": "lectures/8.html#bacteria-example-1",
    "href": "lectures/8.html#bacteria-example-1",
    "title": "Randomization Tests",
    "section": "Bacteria Example",
    "text": "Bacteria Example\n\n\nCode\nprops(bacteria$ap, bacteria$y, \"y\", TRUE)\n\n\n#&gt; [1] 0.125"
  },
  {
    "objectID": "lectures/8.html#hypothesis-test-1",
    "href": "lectures/8.html#hypothesis-test-1",
    "title": "Randomization Tests",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nIs 12.5% of a difference large enough to indicate that an active drug is effective against the bacteria, or seeing this can be due to random chance."
  },
  {
    "objectID": "lectures/8.html#hypotheis-test",
    "href": "lectures/8.html#hypotheis-test",
    "title": "Randomization Tests",
    "section": "Hypotheis Test",
    "text": "Hypotheis Test\nWe will test the following hypothesis:\n\\[\nH_0:\\ p_1-p_2 = 0\n\\]\n\\[\nH_a:\\ p_1 - p_2 \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/8.html#hypothesis-test-2",
    "href": "lectures/8.html#hypothesis-test-2",
    "title": "Randomization Tests",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nIn words:\n\\(H_0\\): The variables bacteria presence and active drug treatment are independent of each other.\n\\(H_a\\): The variables bacteria presence and active drug treatment are dependent of each other."
  },
  {
    "objectID": "lectures/8.html#visualizing",
    "href": "lectures/8.html#visualizing",
    "title": "Randomization Tests",
    "section": "Visualizing",
    "text": "Visualizing\n\n\nCode\nab &lt;- cat_stats(bacteria$y[bacteria$ap == \"a\"], pie = TRUE)\nabp &lt;- ggplot(ab, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) + \n  ggtitle(\"Active Treatment\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\npb &lt;- cat_stats(bacteria$y[bacteria$ap == \"p\"], pie = TRUE)\npbp &lt;- ggplot(pb, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) +\n  ggtitle(\"Placebo\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nabp + pbp + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/8.html#randomization-tests-1",
    "href": "lectures/8.html#randomization-tests-1",
    "title": "Randomization Tests",
    "section": "Randomization Tests",
    "text": "Randomization Tests\nThe idea of a randomization test is that each observed outcome variable was dictated by a mathematical model.\n\nTherefore, a randomization is most interested to determine if the mathematical model exist or completely random.\n\n\nThis is equivalent to saying the there is not pattern in the data. IE, independent variables."
  },
  {
    "objectID": "lectures/8.html#randomization-tests-2",
    "href": "lectures/8.html#randomization-tests-2",
    "title": "Randomization Tests",
    "section": "Randomization Tests",
    "text": "Randomization Tests\n\\(H_0\\): The outcome and predictor variables are independent of each other.\n\\(H_a\\): The outcome and predictor variables are dependent of each other."
  },
  {
    "objectID": "lectures/8.html#randomziation-1",
    "href": "lectures/8.html#randomziation-1",
    "title": "Randomization Tests",
    "section": "Randomziation 1",
    "text": "Randomziation 1\n\n\nCode\nab &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"a\"], pie = TRUE)\nabp &lt;- ggplot(ab, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) + \n  ggtitle(\"Active Treatment\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\npb &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"p\"], pie = TRUE)\npbp &lt;- ggplot(pb, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) +\n  ggtitle(\"Placebo\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nabp + pbp + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/8.html#randomziation-2",
    "href": "lectures/8.html#randomziation-2",
    "title": "Randomization Tests",
    "section": "Randomziation 2",
    "text": "Randomziation 2\n\n\nCode\nab &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"a\"], pie = TRUE)\nabp &lt;- ggplot(ab, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) + \n  ggtitle(\"Active Treatment\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\npb &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"p\"], pie = TRUE)\npbp &lt;- ggplot(pb, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) +\n  ggtitle(\"Placebo\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nabp + pbp + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/8.html#randomziation-3",
    "href": "lectures/8.html#randomziation-3",
    "title": "Randomization Tests",
    "section": "Randomziation 3",
    "text": "Randomziation 3\n\n\nCode\nab &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"a\"], pie = TRUE)\nabp &lt;- ggplot(ab, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) + \n  ggtitle(\"Active Treatment\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\npb &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"p\"], pie = TRUE)\npbp &lt;- ggplot(pb, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) +\n  ggtitle(\"Placebo\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nabp + pbp + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/8.html#randomziation-4",
    "href": "lectures/8.html#randomziation-4",
    "title": "Randomization Tests",
    "section": "Randomziation 4",
    "text": "Randomziation 4\n\n\nCode\nab &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"a\"], pie = TRUE)\nabp &lt;- ggplot(ab, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) + \n  ggtitle(\"Active Treatment\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\npb &lt;- cat_stats(shuffle(bacteria$y)[bacteria$ap == \"p\"], pie = TRUE)\npbp &lt;- ggplot(pb, aes(fill = Category, values = n)) +\n  geom_waffle(make_proportional = TRUE) +\n  ggtitle(\"Placebo\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nabp + pbp + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/8.html#randomization-test",
    "href": "lectures/8.html#randomization-test",
    "title": "Randomization Tests",
    "section": "Randomization Test",
    "text": "Randomization Test\nThe act of randomizing the outcome to a set of predictor variables allow you to construct the sampling of distribution of the null hypothesis.\n\n\\(H_0\\): The variables bacteria presence and active drug treatment are independent of each other. \\(p_{active}-p_{placebo} = 0\\)\n\\(H_a\\): The variables bacteria presence and active drug treatment are dependent of each other. \\(p_{active}-p_{placebo} \\neq 0\\)"
  },
  {
    "objectID": "lectures/8.html#simulating-h_0",
    "href": "lectures/8.html#simulating-h_0",
    "title": "Randomization Tests",
    "section": "Simulating \\(H_0\\)",
    "text": "Simulating \\(H_0\\)\nWe are interested in simulating the distribution for \\(p_{active}-p_{placebo}\\). We do this by randomly assigning the observed outcomes to the predictor variable. Then, we compute the difference in proportions using the props function. Afterwards, we store the value and repeat the process again.\n\nLastly, we compare where the data statistic compared to our simulated distribution."
  },
  {
    "objectID": "lectures/8.html#shuffling-data",
    "href": "lectures/8.html#shuffling-data",
    "title": "Randomization Tests",
    "section": "Shuffling Data",
    "text": "Shuffling Data\nThe shuffle function will mix up a variable in R:\n\n\nCode\nshuffle(VECTOR)"
  },
  {
    "objectID": "lectures/8.html#shuffle-data",
    "href": "lectures/8.html#shuffle-data",
    "title": "Randomization Tests",
    "section": "Shuffle Data",
    "text": "Shuffle Data\n\n\nCode\nbacteria$y\n\n\n#&gt;   [1] y y y y y y n y y y y y y y y y y y y y y y y y y y y y n n n y n y y y y y y y y y y y y y y y y y n n y y y y y y y y y n y y y y y y y n y y y y y n y y y n n n y y n y n y y y y y y y y y y y y y y n y n y n y\n#&gt; [108] y y n n n y n n y n n y y y y y y y n n y y y n y y y y y y n y y y y y y y y y y n n n y y y y y y y y y y y y y y y y y y n y n n y n y y n y y y y y y y y y y y y y y y y y y y n y y y y y y y y y y n n y y y n\n#&gt; [215] n y y y n y\n#&gt; Levels: n y\n\n\n\n\nCode\nshuffle(bacteria$y)\n\n\n#&gt;   [1] n y y n y y y y y y n y n y y y y n y n y y n y y y y y y y y n y y n y y y y n y y y y y y y y y n y n y y y n y y n y y n y y y n y n y y n y y y y y y y n y y y y y y y y y y y n y y n y y y y n y y n y y y y y\n#&gt; [108] y y y y y y y y y y y y y y y n y n y y y y y y y y n n y y y y y y y y n y y y y y y y y y y y y y y y y y n y y y y y y y y y y y y y y n y n n y n y y y n y y y n y n n y y y n n y n y y y n y y y n y y n y y y\n#&gt; [215] y y y y y y\n#&gt; Levels: n y"
  },
  {
    "objectID": "lectures/8.html#computing-proportions",
    "href": "lectures/8.html#computing-proportions",
    "title": "Randomization Tests",
    "section": "Computing Proportions",
    "text": "Computing Proportions\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] 0.1434812"
  },
  {
    "objectID": "lectures/8.html#repeat-process",
    "href": "lectures/8.html#repeat-process",
    "title": "Randomization Tests",
    "section": "Repeat Process",
    "text": "Repeat Process\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] -0.00436828\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] -0.05981183\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] -0.00436828\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] 0.05107527\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] 0.03259409\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] -0.05981183\n\n\nCode\nprops(bacteria$ap, shuffle(bacteria$y), \"y\", T)\n\n\n#&gt; [1] 0.06955645"
  },
  {
    "objectID": "lectures/8.html#the-replicate-function",
    "href": "lectures/8.html#the-replicate-function",
    "title": "Randomization Tests",
    "section": "The replicate Function",
    "text": "The replicate Function\nThe replicate function will execute a task multiple times and store the values in a vector.\n\n\nCode\nreplicate(N, FUN)\n\n\n\nN: number of simulations\nFUN: the command to be repeated"
  },
  {
    "objectID": "lectures/8.html#repeat-1000-times",
    "href": "lectures/8.html#repeat-1000-times",
    "title": "Randomization Tests",
    "section": "Repeat 1000 times",
    "text": "Repeat 1000 times\n\n\nCode\nsim_stats &lt;- replicate(1000, props(bacteria$ap, shuffle(bacteria$y), \"y\", T))\nsim_stats\n\n\n#&gt;    [1] -0.09677419 -0.05981183  0.08803763  0.03259409  0.12500000 -0.02284946  0.05107527  0.12500000  0.05107527 -0.00436828  0.03259409 -0.04133065 -0.09677419 -0.04133065 -0.07829301 -0.02284946 -0.09677419\n#&gt;   [18]  0.05107527  0.03259409  0.03259409 -0.04133065  0.08803763 -0.07829301 -0.05981183 -0.00436828 -0.00436828 -0.07829301 -0.07829301 -0.09677419 -0.00436828  0.03259409 -0.09677419  0.10651882  0.01411290\n#&gt;   [35]  0.03259409 -0.04133065 -0.00436828  0.03259409  0.01411290  0.06955645 -0.04133065  0.05107527 -0.02284946  0.06955645 -0.09677419  0.05107527 -0.04133065 -0.02284946  0.06955645  0.08803763  0.06955645\n#&gt;   [52] -0.02284946 -0.04133065  0.06955645 -0.00436828 -0.04133065  0.05107527  0.06955645 -0.05981183 -0.00436828  0.01411290  0.05107527  0.01411290  0.01411290  0.03259409 -0.07829301  0.03259409 -0.02284946\n#&gt;   [69]  0.03259409 -0.00436828  0.10651882 -0.00436828 -0.02284946  0.06955645  0.03259409  0.05107527 -0.05981183 -0.09677419  0.08803763 -0.09677419  0.03259409 -0.02284946  0.05107527  0.01411290 -0.04133065\n#&gt;   [86] -0.00436828  0.06955645  0.03259409 -0.09677419  0.03259409 -0.04133065  0.05107527  0.01411290 -0.05981183  0.05107527  0.01411290 -0.00436828 -0.02284946 -0.02284946 -0.07829301 -0.05981183  0.05107527\n#&gt;  [103]  0.03259409 -0.05981183  0.01411290 -0.04133065  0.01411290 -0.02284946  0.01411290 -0.00436828  0.03259409 -0.05981183  0.08803763  0.01411290  0.03259409 -0.00436828 -0.02284946  0.01411290  0.01411290\n#&gt;  [120] -0.00436828  0.01411290  0.06955645  0.10651882 -0.05981183  0.03259409 -0.04133065  0.05107527 -0.04133065  0.03259409  0.03259409  0.05107527  0.06955645  0.01411290  0.06955645 -0.04133065  0.03259409\n#&gt;  [137] -0.00436828 -0.00436828 -0.00436828  0.03259409  0.03259409  0.05107527  0.03259409 -0.02284946 -0.02284946 -0.02284946 -0.05981183  0.06955645 -0.00436828 -0.07829301 -0.00436828 -0.04133065  0.03259409\n#&gt;  [154]  0.08803763  0.03259409  0.08803763 -0.04133065 -0.02284946 -0.05981183  0.05107527  0.12500000 -0.00436828 -0.05981183 -0.07829301 -0.04133065 -0.04133065 -0.05981183 -0.02284946 -0.00436828 -0.02284946\n#&gt;  [171]  0.01411290 -0.04133065 -0.02284946  0.06955645 -0.04133065  0.06955645  0.03259409 -0.07829301 -0.04133065 -0.04133065  0.01411290 -0.00436828  0.01411290 -0.09677419  0.05107527  0.01411290 -0.02284946\n#&gt;  [188]  0.10651882  0.05107527  0.01411290 -0.13373656 -0.02284946 -0.13373656 -0.11525538  0.01411290  0.03259409 -0.02284946  0.10651882  0.03259409 -0.09677419  0.01411290  0.05107527  0.01411290  0.01411290\n#&gt;  [205] -0.05981183 -0.02284946 -0.02284946  0.03259409 -0.09677419  0.06955645  0.01411290 -0.02284946 -0.07829301  0.10651882  0.01411290 -0.02284946  0.06955645 -0.05981183 -0.05981183  0.01411290 -0.04133065\n#&gt;  [222]  0.01411290 -0.00436828 -0.02284946 -0.02284946 -0.09677419 -0.04133065 -0.02284946 -0.02284946 -0.04133065 -0.00436828  0.01411290 -0.05981183 -0.02284946  0.03259409 -0.05981183 -0.00436828  0.06955645\n#&gt;  [239] -0.05981183  0.01411290 -0.00436828  0.01411290  0.01411290 -0.07829301 -0.00436828 -0.00436828 -0.02284946 -0.00436828 -0.02284946  0.08803763 -0.02284946  0.05107527 -0.00436828  0.10651882  0.05107527\n#&gt;  [256] -0.00436828 -0.00436828 -0.04133065 -0.04133065  0.01411290 -0.09677419 -0.05981183 -0.02284946  0.03259409  0.03259409  0.01411290  0.05107527  0.01411290 -0.00436828 -0.07829301 -0.00436828 -0.02284946\n#&gt;  [273] -0.05981183  0.06955645  0.03259409  0.05107527 -0.13373656 -0.02284946 -0.00436828  0.03259409  0.05107527 -0.15221774  0.12500000  0.01411290  0.03259409  0.03259409 -0.09677419 -0.07829301  0.05107527\n#&gt;  [290] -0.00436828 -0.00436828 -0.04133065  0.06955645 -0.04133065  0.01411290 -0.00436828 -0.07829301  0.05107527 -0.04133065 -0.05981183  0.03259409 -0.00436828 -0.02284946 -0.00436828  0.05107527  0.06955645\n#&gt;  [307] -0.15221774 -0.04133065 -0.02284946 -0.04133065  0.03259409 -0.09677419 -0.15221774  0.05107527 -0.02284946 -0.02284946 -0.11525538 -0.04133065 -0.02284946 -0.00436828  0.06955645  0.01411290 -0.00436828\n#&gt;  [324] -0.15221774 -0.00436828 -0.00436828  0.05107527 -0.02284946 -0.00436828 -0.04133065  0.03259409  0.03259409  0.01411290  0.03259409  0.06955645  0.03259409  0.06955645  0.06955645  0.01411290  0.01411290\n#&gt;  [341]  0.03259409  0.01411290 -0.04133065 -0.02284946  0.06955645  0.05107527  0.03259409  0.03259409 -0.02284946  0.01411290 -0.04133065  0.05107527  0.05107527  0.03259409  0.03259409  0.03259409  0.05107527\n#&gt;  [358] -0.07829301  0.03259409  0.03259409 -0.02284946 -0.07829301  0.01411290  0.01411290 -0.00436828  0.06955645  0.06955645  0.03259409 -0.02284946  0.08803763 -0.02284946 -0.04133065  0.03259409 -0.11525538\n#&gt;  [375]  0.05107527 -0.00436828 -0.05981183 -0.00436828  0.01411290 -0.04133065 -0.00436828 -0.07829301 -0.07829301 -0.05981183 -0.00436828 -0.00436828 -0.00436828 -0.13373656  0.01411290  0.01411290  0.05107527\n#&gt;  [392]  0.01411290 -0.09677419  0.01411290  0.03259409  0.08803763  0.01411290  0.05107527  0.01411290 -0.02284946  0.05107527 -0.02284946 -0.05981183  0.03259409 -0.02284946 -0.07829301  0.01411290  0.10651882\n#&gt;  [409] -0.05981183  0.01411290 -0.00436828  0.10651882 -0.02284946 -0.04133065  0.03259409 -0.02284946  0.10651882 -0.02284946  0.01411290 -0.05981183 -0.00436828  0.05107527 -0.07829301 -0.04133065  0.01411290\n#&gt;  [426] -0.05981183 -0.05981183 -0.04133065  0.03259409  0.08803763  0.06955645 -0.05981183 -0.04133065 -0.04133065 -0.15221774 -0.13373656 -0.07829301 -0.09677419 -0.05981183  0.03259409 -0.04133065 -0.09677419\n#&gt;  [443] -0.04133065  0.01411290  0.08803763 -0.04133065  0.01411290  0.03259409 -0.11525538 -0.02284946  0.05107527  0.03259409 -0.04133065  0.01411290  0.05107527 -0.07829301 -0.00436828  0.01411290 -0.07829301\n#&gt;  [460] -0.00436828 -0.07829301  0.03259409  0.05107527 -0.00436828  0.06955645  0.05107527  0.03259409 -0.04133065 -0.02284946 -0.00436828  0.05107527 -0.09677419  0.03259409 -0.07829301  0.05107527 -0.09677419\n#&gt;  [477] -0.04133065  0.05107527 -0.04133065  0.03259409 -0.05981183 -0.00436828 -0.02284946 -0.13373656 -0.04133065 -0.07829301 -0.09677419  0.03259409  0.08803763  0.05107527 -0.04133065  0.08803763 -0.02284946\n#&gt;  [494]  0.01411290  0.03259409 -0.02284946  0.01411290 -0.07829301 -0.05981183  0.03259409  0.06955645  0.01411290  0.03259409 -0.09677419 -0.05981183 -0.09677419  0.01411290 -0.00436828  0.03259409  0.03259409\n#&gt;  [511]  0.01411290 -0.04133065 -0.00436828  0.05107527 -0.04133065 -0.07829301 -0.07829301  0.06955645 -0.04133065 -0.09677419 -0.00436828  0.03259409 -0.00436828  0.03259409  0.01411290  0.08803763  0.03259409\n#&gt;  [528] -0.05981183  0.01411290 -0.00436828 -0.00436828 -0.00436828 -0.00436828 -0.05981183  0.03259409 -0.05981183 -0.00436828 -0.00436828 -0.09677419 -0.00436828 -0.00436828  0.05107527  0.06955645  0.06955645\n#&gt;  [545]  0.01411290  0.03259409  0.05107527 -0.07829301  0.14348118 -0.02284946  0.06955645 -0.02284946  0.12500000  0.01411290 -0.09677419 -0.05981183  0.01411290  0.14348118  0.06955645  0.01411290 -0.02284946\n#&gt;  [562] -0.05981183 -0.04133065  0.03259409 -0.00436828  0.12500000 -0.04133065  0.03259409  0.03259409 -0.05981183  0.05107527 -0.02284946  0.01411290  0.06955645 -0.00436828  0.01411290  0.03259409 -0.13373656\n#&gt;  [579] -0.04133065 -0.04133065  0.06955645 -0.00436828 -0.02284946  0.10651882 -0.04133065 -0.04133065  0.01411290  0.10651882  0.03259409  0.05107527  0.14348118 -0.00436828  0.03259409 -0.04133065  0.05107527\n#&gt;  [596]  0.03259409  0.01411290 -0.02284946  0.05107527  0.03259409 -0.00436828 -0.04133065 -0.00436828  0.05107527 -0.09677419  0.03259409  0.01411290 -0.00436828  0.05107527  0.01411290  0.01411290  0.03259409\n#&gt;  [613] -0.04133065  0.10651882  0.05107527  0.08803763  0.03259409 -0.05981183 -0.04133065  0.01411290 -0.00436828  0.03259409 -0.00436828 -0.00436828  0.05107527 -0.11525538 -0.00436828  0.03259409 -0.04133065\n#&gt;  [630] -0.05981183 -0.00436828  0.10651882  0.05107527  0.05107527 -0.00436828 -0.00436828 -0.05981183  0.08803763 -0.00436828 -0.00436828  0.01411290  0.06955645  0.03259409 -0.00436828 -0.05981183 -0.04133065\n#&gt;  [647] -0.11525538 -0.09677419  0.03259409  0.08803763  0.01411290  0.12500000  0.01411290  0.03259409  0.01411290 -0.00436828 -0.04133065  0.05107527 -0.05981183  0.01411290 -0.07829301 -0.05981183 -0.00436828\n#&gt;  [664] -0.04133065  0.05107527  0.01411290 -0.04133065  0.06955645 -0.00436828 -0.04133065  0.10651882 -0.02284946  0.01411290 -0.04133065  0.03259409 -0.04133065 -0.04133065  0.03259409  0.08803763 -0.05981183\n#&gt;  [681] -0.04133065 -0.04133065 -0.00436828 -0.00436828  0.03259409  0.03259409 -0.07829301 -0.07829301  0.01411290 -0.02284946  0.05107527 -0.02284946 -0.07829301 -0.00436828  0.03259409  0.01411290  0.06955645\n#&gt;  [698] -0.04133065 -0.07829301 -0.02284946  0.06955645 -0.04133065  0.06955645 -0.02284946 -0.00436828  0.06955645 -0.04133065 -0.02284946 -0.04133065  0.03259409  0.08803763  0.06955645 -0.02284946 -0.04133065\n#&gt;  [715]  0.06955645 -0.02284946 -0.00436828 -0.00436828  0.03259409  0.03259409  0.05107527  0.03259409 -0.04133065  0.03259409 -0.05981183 -0.02284946 -0.00436828  0.03259409  0.03259409 -0.07829301 -0.02284946\n#&gt;  [732]  0.01411290 -0.02284946 -0.04133065  0.05107527 -0.04133065  0.08803763  0.01411290  0.08803763 -0.00436828  0.01411290  0.05107527  0.03259409 -0.00436828  0.01411290  0.03259409  0.05107527  0.06955645\n#&gt;  [749]  0.01411290  0.05107527  0.01411290 -0.02284946 -0.00436828 -0.04133065 -0.05981183  0.10651882  0.01411290 -0.00436828  0.05107527  0.05107527 -0.05981183  0.05107527  0.03259409 -0.00436828 -0.02284946\n#&gt;  [766] -0.09677419  0.03259409  0.03259409 -0.04133065 -0.00436828  0.05107527 -0.09677419 -0.04133065  0.03259409 -0.04133065 -0.02284946 -0.00436828  0.05107527  0.01411290  0.01411290 -0.00436828 -0.04133065\n#&gt;  [783]  0.06955645  0.12500000  0.05107527  0.14348118 -0.13373656  0.06955645 -0.00436828 -0.02284946  0.05107527  0.05107527 -0.04133065 -0.00436828 -0.05981183 -0.05981183 -0.00436828 -0.05981183 -0.05981183\n#&gt;  [800]  0.06955645  0.06955645 -0.00436828  0.05107527 -0.00436828 -0.02284946 -0.09677419  0.01411290  0.01411290 -0.02284946  0.01411290 -0.04133065 -0.07829301 -0.05981183  0.01411290  0.03259409 -0.00436828\n#&gt;  [817]  0.08803763 -0.09677419  0.14348118 -0.11525538  0.08803763  0.01411290 -0.07829301 -0.09677419  0.05107527 -0.04133065  0.05107527 -0.05981183  0.03259409 -0.00436828  0.06955645  0.05107527  0.01411290\n#&gt;  [834]  0.01411290  0.08803763  0.10651882  0.05107527 -0.00436828  0.03259409  0.06955645  0.01411290  0.06955645 -0.02284946  0.08803763 -0.04133065  0.01411290 -0.00436828 -0.05981183  0.12500000 -0.04133065\n#&gt;  [851]  0.05107527 -0.00436828  0.03259409  0.05107527  0.01411290 -0.05981183 -0.00436828  0.01411290 -0.05981183 -0.00436828 -0.04133065  0.03259409  0.01411290 -0.00436828  0.12500000  0.01411290 -0.04133065\n#&gt;  [868]  0.06955645 -0.02284946 -0.05981183  0.05107527  0.01411290  0.05107527 -0.05981183  0.03259409  0.06955645  0.03259409  0.06955645 -0.00436828  0.05107527  0.01411290 -0.04133065  0.01411290 -0.07829301\n#&gt;  [885] -0.04133065 -0.02284946 -0.00436828  0.01411290 -0.00436828 -0.05981183 -0.00436828  0.14348118 -0.02284946 -0.07829301 -0.02284946 -0.02284946 -0.09677419 -0.07829301 -0.00436828  0.06955645 -0.00436828\n#&gt;  [902] -0.07829301 -0.00436828 -0.04133065  0.03259409  0.01411290 -0.11525538 -0.05981183  0.03259409 -0.00436828  0.05107527 -0.00436828  0.05107527  0.01411290  0.01411290  0.03259409 -0.02284946 -0.00436828\n#&gt;  [919] -0.07829301 -0.05981183 -0.00436828 -0.02284946 -0.13373656  0.01411290 -0.05981183  0.06955645 -0.00436828  0.05107527 -0.02284946 -0.00436828 -0.02284946 -0.07829301 -0.02284946  0.01411290 -0.00436828\n#&gt;  [936]  0.03259409  0.06955645 -0.02284946 -0.05981183  0.06955645 -0.00436828  0.03259409 -0.00436828 -0.07829301  0.03259409  0.06955645  0.10651882 -0.02284946 -0.00436828 -0.04133065 -0.02284946 -0.04133065\n#&gt;  [953] -0.02284946 -0.00436828 -0.00436828  0.01411290 -0.02284946 -0.02284946  0.01411290  0.03259409  0.05107527 -0.05981183 -0.04133065  0.01411290  0.08803763 -0.07829301  0.03259409  0.05107527 -0.00436828\n#&gt;  [970]  0.01411290 -0.00436828  0.01411290  0.01411290 -0.00436828  0.03259409  0.10651882  0.05107527  0.01411290 -0.05981183  0.08803763 -0.00436828 -0.02284946  0.05107527  0.01411290 -0.04133065  0.01411290\n#&gt;  [987] -0.09677419 -0.05981183  0.05107527 -0.00436828 -0.02284946  0.05107527  0.01411290 -0.07829301 -0.02284946  0.05107527 -0.04133065 -0.02284946  0.03259409 -0.00436828"
  },
  {
    "objectID": "lectures/8.html#visualize-data",
    "href": "lectures/8.html#visualize-data",
    "title": "Randomization Tests",
    "section": "Visualize Data",
    "text": "Visualize Data\n\n\nCode\nggplot(data.frame(x = sim_stats), aes(x)) + geom_density()"
  },
  {
    "objectID": "lectures/8.html#compute-data-propoortion",
    "href": "lectures/8.html#compute-data-propoortion",
    "title": "Randomization Tests",
    "section": "Compute Data Propoortion",
    "text": "Compute Data Propoortion\n\n\nCode\ntest_stat &lt;- props(bacteria$ap, bacteria$y, \"y\", T)\ntest_stat\n\n\n#&gt; [1] 0.125"
  },
  {
    "objectID": "lectures/8.html#comparison",
    "href": "lectures/8.html#comparison",
    "title": "Randomization Tests",
    "section": "Comparison",
    "text": "Comparison\n\n\nCode\nggplot(data.frame(x = sim_stats), aes(x)) + \n  geom_density() +\n  geom_vline(xintercept = test_stat)"
  },
  {
    "objectID": "lectures/8.html#p-value-1",
    "href": "lectures/8.html#p-value-1",
    "title": "Randomization Tests",
    "section": "P-Value",
    "text": "P-Value\nThe p-value determines the probability of observing our data statistic, given that the null hypothesis is true.\n\nWhen the p-value is large, we believe there is a high probability that the null hypothesis is true.\n\n\nWhen the p-value is small, we believe that it is unlikely the the null hypothesis is true. We believe that the alternative may be a better model."
  },
  {
    "objectID": "lectures/8.html#sided-p-value",
    "href": "lectures/8.html#sided-p-value",
    "title": "Randomization Tests",
    "section": "2-Sided P-Value",
    "text": "2-Sided P-Value\nDue to randomness, there is a possibility of observing the same magnitude of \\(p_{active}-p_{placebo}\\), but in the opposite direction; therefore, we must look at both sides."
  },
  {
    "objectID": "lectures/8.html#visually-2-sided-p-value",
    "href": "lectures/8.html#visually-2-sided-p-value",
    "title": "Randomization Tests",
    "section": "Visually 2-Sided P-Value",
    "text": "Visually 2-Sided P-Value\n\n\nCode\nggplot(data.frame(x = sim_stats), aes(x)) + \n  geom_density() +\n  geom_vline(xintercept = c(-1,1) *test_stat)"
  },
  {
    "objectID": "lectures/8.html#computing-the-p-value",
    "href": "lectures/8.html#computing-the-p-value",
    "title": "Randomization Tests",
    "section": "Computing the p-value",
    "text": "Computing the p-value\nWe will count how many simulated \\(p_{active}-p_{placebo}\\)’s are more extreme than our test statistic and divided by the number of simulations plus one."
  },
  {
    "objectID": "lectures/8.html#computing-the-p-value-1",
    "href": "lectures/8.html#computing-the-p-value-1",
    "title": "Randomization Tests",
    "section": "Computing the p-value",
    "text": "Computing the p-value\n\\[\np = \\frac{m +1}{N + 1}\n\\]\n\n\\(m\\): number of more extreme values than our test statistic\n\\(N\\): number of simulations"
  },
  {
    "objectID": "lectures/8.html#computing-the-p-value-2",
    "href": "lectures/8.html#computing-the-p-value-2",
    "title": "Randomization Tests",
    "section": "Computing the P-Value",
    "text": "Computing the P-Value\n\n\nCode\n(sum(abs(SIM_VECTOR) &gt; abs(TEST_STAT)) + 1) / (N + 1)"
  },
  {
    "objectID": "lectures/8.html#computing-p-value",
    "href": "lectures/8.html#computing-p-value",
    "title": "Randomization Tests",
    "section": "Computing P-value",
    "text": "Computing P-value\n\n\nCode\nsum(abs(sim_stats) &gt; abs(test_stat)) / 1001\n\n\n#&gt; [1] 0.01998002"
  },
  {
    "objectID": "lectures/8.html#identify-your-variables",
    "href": "lectures/8.html#identify-your-variables",
    "title": "Randomization Tests",
    "section": "Identify your Variables",
    "text": "Identify your Variables\n\nWhat is your predictor variable?\nWhat is your outcome variable?"
  },
  {
    "objectID": "lectures/8.html#create-your-hypothesis",
    "href": "lectures/8.html#create-your-hypothesis",
    "title": "Randomization Tests",
    "section": "Create your Hypothesis",
    "text": "Create your Hypothesis\nChoose a hypothesis that is appropriate for your model."
  },
  {
    "objectID": "lectures/8.html#compute-a-test-statistic-for-the-data",
    "href": "lectures/8.html#compute-a-test-statistic-for-the-data",
    "title": "Randomization Tests",
    "section": "Compute a test statistic for the data",
    "text": "Compute a test statistic for the data\nIdentify what is you want to compute and compare."
  },
  {
    "objectID": "lectures/8.html#generate-a-null-distribution.",
    "href": "lectures/8.html#generate-a-null-distribution.",
    "title": "Randomization Tests",
    "section": "Generate a null distribution.",
    "text": "Generate a null distribution.\nSimulate the potential null distribution using the shuffle and replicate function."
  },
  {
    "objectID": "lectures/8.html#compute-the-p-value",
    "href": "lectures/8.html#compute-the-p-value",
    "title": "Randomization Tests",
    "section": "Compute the p-value",
    "text": "Compute the p-value\nUse the probability formula to determine the likelihood of observing your test statistic given that the null hypothesis is true."
  },
  {
    "objectID": "lectures/template.html#learning-outcomes",
    "href": "lectures/template.html#learning-outcomes",
    "title": "Welcome!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes"
  }
]