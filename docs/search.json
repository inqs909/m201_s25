[
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Categorical Data\n\n\n\n\n\nProvides an overview descriptive statistics and data visualization techniques for categorical data. \n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData Generating Process\n\n\n\n\n\nProvide a Brief Introduction of Data, Study Design, and the Data Generating Process. \n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Functions\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\nBegins the discussion on Inference. \n\n\n\n\n\nApr 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Linear Regression\n\n\n\n\n\nBegins the discussion for linear regression. \n\n\n\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Linear Regression\n\n\n\n\n\nBegins the discussion for linear regression. \n\n\n\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Logistic Regression\n\n\n\n\n\nBegins the discussion on inference for logistic and Poisson regression. \n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Logistic and Poisson Regression\n\n\n\n\n\nBegins the discussion on inference for logistic and Poisson regression. \n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariable Linear Regression\n\n\n\n\n\nContinues the discussion with multi linear regression. \n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Data\n\n\n\n\n\nProvides an overview descriptive statistics and data visualization techniques for numerical data. \n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Distribution\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\nBegins the discussion for linear regression. \n\n\n\n\n\nFeb 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\nStatistical Inference \n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nA Math? A Science? An Art? Or Something Else?\n\n\n\n\n\n\n\n\nIsaac Quintanilla Salinas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/11.html#motivating-example-1",
    "href": "lectures/11.html#motivating-example-1",
    "title": "Inference with Linear Regression",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\np1 &lt;- penguins |&gt; ggplot(aes(x=species, y = body_mass_g)) +\n  geom_jitter() + \n  geom_boxplot() + \n  labs(x = \"Species\", y = \"Body Mass\")\n  \np2 &lt;- penguins |&gt; ggplot(aes(x=flipper_length_mm, y = body_mass_g)) +\n  geom_point() + \n  labs(x = \"Flipper Length\", y = \"Body Mass\")  \n\np1 + p2"
  },
  {
    "objectID": "lectures/11.html#mathematical-models-1",
    "href": "lectures/11.html#mathematical-models-1",
    "title": "Inference with Linear Regression",
    "section": "Mathematical Models",
    "text": "Mathematical Models"
  },
  {
    "objectID": "lectures/11.html#standard-normal-distribution",
    "href": "lectures/11.html#standard-normal-distribution",
    "title": "Inference with Linear Regression",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\n\n\\[\n{\\frac{1}{\\sqrt{2 \\pi}}} e^{-\\frac{1}{2}x^2}\n\\]\n\n\n\nCode\ndata.frame(x = seq(-5,5, length.out = 100), \n           y1 = dt(seq(-5,5, length.out = 100), 1),\n           y2 = dt(seq(-5,5, length.out = 100), 10),\n           y3 = dt(seq(-5,5, length.out = 100), 30),\n           y4 = dt(seq(-5,5, length.out = 100), 100),\n           y5 = dnorm((seq(-5,5, length.out = 100)))) |&gt; \n  ggplot() +\n  # geom_line(aes(x, y1, color = \"1\")) +\n  # geom_line(aes(x, y2, color = \"10\")) +\n  # geom_line(aes(x, y3, color = \"30\")) +\n  # geom_line(aes(x, y4, color = \"100\")) +\n  geom_line(aes(x, y5)) +\n  ylab(\"y\")"
  },
  {
    "objectID": "lectures/11.html#t-distribution",
    "href": "lectures/11.html#t-distribution",
    "title": "Inference with Linear Regression",
    "section": "t Distribution",
    "text": "t Distribution\n\n\n\\[\n\\frac{\\Gamma \\left(\\frac{v+1}{2}\\right)}{\\sqrt{\\pi v}\\Gamma\\left(\\frac{v}{2}\\right)} \\left(1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\n\\]\n\n\n\nCode\ndata.frame(x = seq(-5,5, length.out = 100), \n           y1 = dt(seq(-5,5, length.out = 100), 1),\n           y2 = dt(seq(-5,5, length.out = 100), 10),\n           y3 = dt(seq(-5,5, length.out = 100), 30),\n           y4 = dt(seq(-5,5, length.out = 100), 100),\n           y5 = dnorm((seq(-5,5, length.out = 100)))) |&gt; \n  ggplot() +\n  geom_line(aes(x, y1, color = \"1\")) +\n  geom_line(aes(x, y2, color = \"10\")) +\n  geom_line(aes(x, y3, color = \"30\")) +\n  geom_line(aes(x, y4, color = \"100\")) +\n  geom_line(aes(x, y5, color = \"Normal\")) +\n  ylab(\"y\")"
  },
  {
    "objectID": "lectures/11.html#hypothesis",
    "href": "lectures/11.html#hypothesis",
    "title": "Inference with Linear Regression",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/11.html#testing-beta_j",
    "href": "lectures/11.html#testing-beta_j",
    "title": "Inference with Linear Regression",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\nT = \\frac{\\hat\\beta_j-\\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]\n\n\\(n\\): sample size\n\\(p^\\prime\\): number of \\(\\beta\\)s"
  },
  {
    "objectID": "lectures/11.html#p-value",
    "href": "lectures/11.html#p-value",
    "title": "Inference with Linear Regression",
    "section": "P-Value",
    "text": "P-Value\n\n\n\nAlternative Hypothesis\np-value\n\n\n\n\n\\(\\beta&gt;\\theta\\)\n\\(P(\\hat\\beta &gt;T)=p\\)\n\n\n\\(\\beta&lt;\\theta\\)\n\\(P(\\hat\\beta &lt; T)=p\\)\n\n\n\\(\\beta\\ne\\theta\\)\n\\(2\\times P(\\hat\\beta &gt;|T|)=p\\)"
  },
  {
    "objectID": "lectures/11.html#confidence-intervals",
    "href": "lectures/11.html#confidence-intervals",
    "title": "Inference with Linear Regression",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\n\\hat \\beta_j \\pm CV \\times se(\\hat\\beta_j)\n\\]\n\n\\(CV\\): Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\n\\(se\\): Standard Error Function"
  },
  {
    "objectID": "lectures/11.html#conducting-ht-of-beta_j",
    "href": "lectures/11.html#conducting-ht-of-beta_j",
    "title": "Inference with Linear Regression",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)"
  },
  {
    "objectID": "lectures/11.html#example",
    "href": "lectures/11.html#example",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass_g ~ species + flipper_length_mm, penguins)\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -898.8 -252.0  -24.8  229.8 1191.6 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -4013.18     586.25  -6.846 3.74e-11 ***\n#&gt; speciesChinstrap   -205.38      57.57  -3.568 0.000414 ***\n#&gt; speciesGentoo       284.52      95.43   2.981 0.003083 ** \n#&gt; flipper_length_mm    40.61       3.08  13.186  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 373.3 on 329 degrees of freedom\n#&gt; Multiple R-squared:  0.787,  Adjusted R-squared:  0.7851 \n#&gt; F-statistic: 405.3 on 3 and 329 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/11.html#confidence-interval",
    "href": "lectures/11.html#confidence-interval",
    "title": "Inference with Linear Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/11.html#example-1",
    "href": "lectures/11.html#example-1",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.90)\n\n\n#&gt;                           5 %        95 %\n#&gt; (Intercept)       -4980.19064 -3046.16714\n#&gt; speciesChinstrap   -300.33123  -110.41973\n#&gt; speciesGentoo       127.11143   441.93578\n#&gt; flipper_length_mm    35.52645    45.68588"
  },
  {
    "objectID": "lectures/11.html#model-inference",
    "href": "lectures/11.html#model-inference",
    "title": "Inference with Linear Regression",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(\\hat Y=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/11.html#model-inference-1",
    "href": "lectures/11.html#model-inference-1",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven 2 models:\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\n\\hat Y = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/11.html#anova",
    "href": "lectures/11.html#anova",
    "title": "Inference with Linear Regression",
    "section": "ANOVA",
    "text": "ANOVA"
  },
  {
    "objectID": "lectures/11.html#anova-table",
    "href": "lectures/11.html#anova-table",
    "title": "Inference with Linear Regression",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nModel\n\\(DFR=k-1\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSM}{DFR}\\)\n\\(\\hat F=\\frac{MSR}{MSE}\\)\n\n\nError\n\\(DFE=n-k\\)\n\\(SSE\\)\n\\(MSE=\\frac{SSE}{DFE}\\)\n\n\n\nTotal\n\\(TDF=n-1\\)\n\\(TSS=SSR+SSE\\)\n\n\n\n\n\n\\[\n\\hat F \\sim F(DFR, DFE)\n\\]"
  },
  {
    "objectID": "lectures/11.html#conducting-an-anova-in-r",
    "href": "lectures/11.html#conducting-an-anova-in-r",
    "title": "Inference with Linear Regression",
    "section": "Conducting an ANOVA in R",
    "text": "Conducting an ANOVA in R\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)"
  },
  {
    "objectID": "lectures/11.html#example-2",
    "href": "lectures/11.html#example-2",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -898.8 -252.0  -24.8  229.8 1191.6 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -4013.18     586.25  -6.846 3.74e-11 ***\n#&gt; speciesChinstrap   -205.38      57.57  -3.568 0.000414 ***\n#&gt; speciesGentoo       284.52      95.43   2.981 0.003083 ** \n#&gt; flipper_length_mm    40.61       3.08  13.186  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 373.3 on 329 degrees of freedom\n#&gt; Multiple R-squared:  0.787,  Adjusted R-squared:  0.7851 \n#&gt; F-statistic: 405.3 on 3 and 329 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/11.html#model-inference-2",
    "href": "lectures/11.html#model-inference-2",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/11.html#model-inference-3",
    "href": "lectures/11.html#model-inference-3",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ \\hat y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ \\hat y = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/11.html#model-inference-4",
    "href": "lectures/11.html#model-inference-4",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/11.html#test-statistic",
    "href": "lectures/11.html#test-statistic",
    "title": "Inference with Linear Regression",
    "section": "Test Statistic",
    "text": "Test Statistic\n\\[\n\\hat F = \\frac{[SSE(RED) - SSE(FULL)]/[DFE(RED)-DFE(FULL)]}{MSE(FULL)}\n\\]\n\\[\n\\hat F \\sim F[DFE(RED) - DFE(FULL), DFE(FULL)]\n\\]"
  },
  {
    "objectID": "lectures/11.html#anova-in-r",
    "href": "lectures/11.html#anova-in-r",
    "title": "Inference with Linear Regression",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\n\nCode\nfull &lt;- lm(Y  ~  X1 + X2 + X3 + X4)\nred &lt;- lm(Y ~ X1 + X2)\nanova(red, full)"
  },
  {
    "objectID": "lectures/11.html#example-3",
    "href": "lectures/11.html#example-3",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass_g ~ species + island + flipper_length_mm, penguins)\nm2 &lt;- lm(body_mass_g ~ island + flipper_length_mm, penguins)\nanova(m2, m1)\n\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: body_mass_g ~ island + flipper_length_mm\n#&gt; Model 2: body_mass_g ~ species + island + flipper_length_mm\n#&gt;   Res.Df      RSS Df Sum of Sq      F    Pr(&gt;F)    \n#&gt; 1    329 47774435                                  \n#&gt; 2    327 45552857  2   2221579 7.9738 0.0004157 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/11.html#model-assumptions-1",
    "href": "lectures/11.html#model-assumptions-1",
    "title": "Inference with Linear Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nWhen we are conducting inference linear regression, we will have to check the following conditions:\n\nLinearity\nIndependence\nNormality\nEqual Variances\nMulticollinearity (for MLR)"
  },
  {
    "objectID": "lectures/11.html#linearity",
    "href": "lectures/11.html#linearity",
    "title": "Inference with Linear Regression",
    "section": "Linearity",
    "text": "Linearity\nProbably considered the most important assumption, but there must be a linear relationship between both the outcome variable (y) and a set of predictors (\\(x_1\\), \\(x_2\\), …)."
  },
  {
    "objectID": "lectures/11.html#independence",
    "href": "lectures/11.html#independence",
    "title": "Inference with Linear Regression",
    "section": "Independence",
    "text": "Independence\nThe data points must not influence each other."
  },
  {
    "objectID": "lectures/11.html#normality",
    "href": "lectures/11.html#normality",
    "title": "Inference with Linear Regression",
    "section": "Normality",
    "text": "Normality\nThe model errors (also known as residuals) must follow a normal distribution."
  },
  {
    "objectID": "lectures/11.html#equal-variances",
    "href": "lectures/11.html#equal-variances",
    "title": "Inference with Linear Regression",
    "section": "Equal Variances",
    "text": "Equal Variances\nThe variability of the data points must be the same for all predictor values."
  },
  {
    "objectID": "lectures/11.html#residuals",
    "href": "lectures/11.html#residuals",
    "title": "Inference with Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nResiduals are the errors between the observed value and the estimated model. Common residuals include\n\nRaw Residual\nStandardized Residual\nJackknife (studentized) Residuals"
  },
  {
    "objectID": "lectures/11.html#influential-measurements",
    "href": "lectures/11.html#influential-measurements",
    "title": "Inference with Linear Regression",
    "section": "Influential Measurements",
    "text": "Influential Measurements\nInfluential measures are statistics that determine how much a data point affects the model. Common influential measures are\n\nLeverages\nCook’s Distance"
  },
  {
    "objectID": "lectures/11.html#raw-residuals",
    "href": "lectures/11.html#raw-residuals",
    "title": "Inference with Linear Regression",
    "section": "Raw Residuals",
    "text": "Raw Residuals\n\\[\n\\hat r_i = y_i - \\hat y_i\n\\]"
  },
  {
    "objectID": "lectures/11.html#residual-analysis",
    "href": "lectures/11.html#residual-analysis",
    "title": "Inference with Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to test the assumptions of linear regression."
  },
  {
    "objectID": "lectures/11.html#qq-plot",
    "href": "lectures/11.html#qq-plot",
    "title": "Inference with Linear Regression",
    "section": "QQ Plot",
    "text": "QQ Plot\nA qq (quantile-quantile) plot will plot the estimated quantiles of the residuals against the theoretical quantiles from a normal distribution function. If the points from the qq-plot lie on the \\(y=x\\) line, it is said that the residuals follow a normal distribution."
  },
  {
    "objectID": "lectures/11.html#residual-vs-fitted-plot",
    "href": "lectures/11.html#residual-vs-fitted-plot",
    "title": "Inference with Linear Regression",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\nThis plot allows you to assess the linearity, constant variance, and identify potential outliers. Create a scatter plot between the fitted values (x-axis) and the raw/standardized residuals (y-axis)."
  },
  {
    "objectID": "lectures/11.html#variance-inflation-factor",
    "href": "lectures/11.html#variance-inflation-factor",
    "title": "Inference with Linear Regression",
    "section": "Variance Inflation Factor",
    "text": "Variance Inflation Factor\nThe variance inflation factor is a measurement on how much variables are collinear with each other. A value greater than 10 is a cause for concern and action should be taken."
  },
  {
    "objectID": "lectures/11.html#residual-analysis-in-r",
    "href": "lectures/11.html#residual-analysis-in-r",
    "title": "Inference with Linear Regression",
    "section": "Residual Analysis in R",
    "text": "Residual Analysis in R\nUse the resid_df function to obtain the residuals of a model.\n\n\nCode\nrdf &lt;- resid_df(LM_OBJECT)"
  },
  {
    "objectID": "lectures/11.html#residual-vs-fitted-plot-1",
    "href": "lectures/11.html#residual-vs-fitted-plot-1",
    "title": "Inference with Linear Regression",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\n\n\nCode\nggplot(RDF, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")"
  },
  {
    "objectID": "lectures/11.html#qq-plot-1",
    "href": "lectures/11.html#qq-plot-1",
    "title": "Inference with Linear Regression",
    "section": "QQ Plot",
    "text": "QQ Plot\n\n\nCode\nggplot(RDF, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "lectures/11.html#example-4",
    "href": "lectures/11.html#example-4",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nxlm &lt;- lm(body_mass_g ~   island + species + flipper_length_mm,\n          penguins)\ndfxlm &lt;- resid_df(xlm)\n\nggplot(dfxlm, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n\nggplot(dfxlm, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line()"
  }
]