---
title: "Linear Regression"
format:
  revealjs:
    scrollable: true
    theme: [default, styles.scss]
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: notes/chalkboard_1a.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen
  - reveal-auto-agenda
editor: source
---

```{r}
#| include: false
library(tidyverse)
library(csucistats)
library(ThemePark)
library(palmerpenguins)
penguins <- penguins |> drop_na()
theme_set(theme_bw())
theme_update(theme(axis.text.x = element_text(size = 24),
                   axis.title.x = element_text(size = 30),
                   plot.title = element_text(size = 48),
                   strip.text = element_text(size = 20),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 24)))
```


# Modeling Relationships

## Explaining Variation

::: fragment
This is the process where we try to reduce the variation with the use of other variables.
:::

::: fragment
Can be thought of as getting it less wrong when taking an educated guess.
:::

## Explaining Variation

```{r}
#| echo: false
penguins |> 
  ggplot(aes(body_mass_g)) +
    geom_density() +
    theme(axis.text.x = element_text(size = 24),
                   axis.title.x = element_text(size = 30),
                   plot.title = element_text(size = 48),
                   strip.text = element_text(size = 20),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 24))

```

## Explaining Variation with One Variable

```{r}
#| echo: false
penguins |> 
  ggplot(aes(body_mass_g, fill = species)) +
    geom_density(alpha = .5) +
    theme(axis.text.x = element_text(size = 24),
          axis.title.x = element_text(size = 30),
          plot.title = element_text(size = 48),
          strip.text = element_text(size = 20),
          legend.title = element_blank(),
          legend.text = element_text(size = 24))
```


# A Simple Model

## Generated Model

$$
Y \sim DGP_1
$$

## A Simple Model

```{r}
#| echo: false
penguins |> ggplot(aes(body_mass_g)) +
  geom_histogram() +
  theme(axis.text.x = element_text(size = 24),
                   axis.title.x = element_text(size = 30),
                   plot.title = element_text(size = 48),
                   strip.text = element_text(size = 20),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 24))
```

## A Simple Model

$$
Y = \_\_\_ + error
$$

## Notation

$$
Y = \ \ \ \ \ \ \ \ \ + \varepsilon 
$$

## The Simple Generated Model

$$
Y \sim \mu + \varepsilon
$$

$$
\varepsilon \sim DGP_2
$$

::: fragment
$DGP_2$ is not the same as the $DGP_1$, it is transformed due $\beta_0$. Consider this the NULL $DGP$.
:::

## Observing Data

$$
Y = \beta_0 + \varepsilon
$$

## Estimated Line

$$
\hat Y=\hat\beta_0
$$

## Notation

::: columns
::: {.column width="50%"}
### Observed

$$
Y = \beta_0 + \varepsilon
$$
:::

::: {.column width="50%"}
### Estimated

$$
\hat Y = \hat \beta_0
$$
:::
:::

# Modelling Data

## Indexing Data

The data in a data set can be indexed by a number.

```{r}
penguins[1,-c(1:2)]
```

::: fragment
Making the variable "body_mass_g" be represented by $Y$ and "flipper_length_mm" as $X$:

$$
Y_1 = 3750 \ \ X_1=181
$$
:::

## Indexing Data

$$
Y_i, X_i
$$

## Data

With the data that we collect from a sample, we hypothesize how the data was generated.

::: fragment
Using a simple model:

$$
Y_i = \beta_0 + \varepsilon_i
$$
:::

## Estimated Value

$$
\hat Y_i = \hat \beta_0
$$

## Estimation 

To estimate $\hat \beta_0$, we minimize the follow function:

$$
\sum^n_{i=1} (Y_i-\hat Y_i)^2
$$

::: fragment
This is known as the sum squared errors, SSE 
:::



## Residuals

The residuals are known as the observed errors from the data in the model:

$$
r_i = Y_i - \hat Y_i
$$

## Estimation in R


```{r}
#| echo: true
#| eval: false

lm(Y ~ 1, data = DATA)
```


## Modeling Body Mass in Penguins

```{r}
lm(body_mass_g ~ 1, data = penguins)
```


# Linear Model

## Linear Model

The goal of Statistics is to develop models the have a better explanation of the outcome $Y$.

::: fragment
In particularly, reduce the sum of squared errors.
:::

::: fragment
By utilizing a bit more of information, $X$, we can increase the predicting capabilities of the model.
:::

::: fragment
Thus, the linear model is born.
:::

## Linear Model

$$
Y = \beta_0 + \beta_1 X + \varepsilon
$$

$$
\varepsilon \sim DGP_3
$$

## Scatter Plot

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_length_mm, body_mass_g)) + 
  geom_point() +
  theme(axis.text.x = element_text(size = 24),
                   axis.title.x = element_text(size = 30),
                   plot.title = element_text(size = 48),
                   strip.text = element_text(size = 20),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 24))
```

## Imposing a Line

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_length_mm, body_mass_g)) + 
  geom_point() +
  stat_smooth(method = "lm", se = F) +
  theme(axis.text.x = element_text(size = 24),
                   axis.title.x = element_text(size = 30),
                   plot.title = element_text(size = 48),
                   strip.text = element_text(size = 20),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 24))
```

## Modelling the Data

$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
$$

## Linear Model

$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

::: fragment
Goal is to obtain numerical values for $\hat \beta_0$ and $\hat \beta_1$ that will minimize the SSE.
:::

## SSE

$$
\sum^n_{i=1} (Y_i-\hat Y_i)^2
$$


$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

## Fitting a Model in R

```{r}
#| echo: true
#| eval: false

lm(Y ~ X, data = DATA)
```

## Example


Y: "body_mass_g"; X: "flipper_length_mm"

```{r}
lm(body_mass_g ~ flipper_length_mm, data = penguins)
```


$$
\hat Y_i = -5872.09 + 50.15 X_i
$$


## Interpretation of $\hat\beta_0$

The intercept $\hat \beta_0$ can be interpreted as the base value when $X$ is set to 0.

::: fragment
Some times the intercept can be interpretable to real world scenarios.
:::

::: fragment
Other times it cannot.
:::

## Interpreting Example

$$
\hat Y_i = -5872.09 + 50.15 X_i
$$
When flipper length is 0 mm, the body mass is -5872 grams.

## Interpretation of $\hat \beta_1$

The slope $\hat \beta_1$ indicates how will y change when x increases by 1 unit.

::: fragment
It will demonstrate if there is, on average, a positive or negative relationship based on the sign provided.
:::

## Interpreting Example

$$
\hat Y_i = -5872.09 + 50.15 X_i
$$

When flipper length increases by 1 mm, the body mass will increase by 50.15 grams.


# Categorical Variables

## Body Mass with Species

```{r}
#| code-fold: true
penguins |> 
  ggplot(aes(body_mass_g)) +
    geom_boxplot() +
    theme(axis.text.x = element_text(size = 24),
          axis.title.x = element_text(size = 30),
          plot.title = element_text(size = 48),
          strip.text = element_text(size = 20),
          legend.title = element_blank(),
          legend.text = element_text(size = 24))
```

## Body Mass with Species

```{r}
#| code-fold: true
penguins |> 
  ggplot(aes(body_mass_g, fill = species)) +
    geom_boxplot() +
    theme(axis.text.x = element_text(size = 24),
          axis.title.x = element_text(size = 30),
          plot.title = element_text(size = 48),
          strip.text = element_text(size = 20),
          legend.title = element_blank(),
          legend.text = element_text(size = 24))
```

## Group Statistics

We can use statistics to explain a continuous variable by the categories.

::: fragment
Compute statistics for each group.
:::

::: fragment

```{r}
#| eval: false
num_by_cat_stats(DATA, NUM, CAT)
```
:::


## Compute Group Statistics

```{r}
num_by_cat_stats(penguins, body_mass_g, species)
```


## Linear Models with Categorical Variables

A line is normally used to model 2 continuous variables.

::: fragment
However, the predictor variable $X$ can be restricted to a set a variables that can symbolize categories.
:::

::: fragment
A category will be used as a reference for a model.
:::


## Binary (Dummy) Variables

Binary variables are variable that can only take on the value 0 or 1.

$$
D_i = \left\{
\begin{array}{cc}
1 & Category\\
0 & Other
\end{array}
\right.
$$

## Binary (Dummy) Variables

To fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use $C-1$ dummy variables where $C$ indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables.

## Example

If we have 4 categories, we will need 3 dummy variables:

|         | Cat 1 | Cat 2 | Cat 3 | Cat 4 |
|---------|-------|-------|-------|-------|
| Dummy 1 | 1     | 0     | 0     | 0     |
| Dummy 2 | 0     | 1     | 0     | 0     |
| Dummy 2 | 0     | 0     | 1     | 0     |


## Species Dummy Variables

|         | Chinstrap | Gentoo | Adelie |
|---------|-------|-------|-------|
| $D_1$ | 1     | 0     | 0     |
| $D_2$ | 0     | 1     | 0     |

## Linear Model

$$
\hat Y_i = \hat \beta_0 + \hat\beta_1 D_{1i} + \hat\beta_2 D_{2i}
$$

::: fragment
$\hat \beta_1$ indicates how body mass changes from Adelie to Chinstrap.
:::

::: fragment
$\hat \beta_2$ indicates how body mass changes from Adelie to Gentoo.
:::

::: fragment
$\hat \beta_0$ represents the baseline level, in this case the body mass of Adelie.
:::

## Fitting a Model in R

```{r}
#| eval: false

lm(Y ~ X, data = DATA)
```

## Example

```{r}
lm(body_mass_g ~ species, penguins)
```

$$
\hat Y_i = 3706 + 26.92 D_{1i} + 1386.27 D_{2i}
$$

## Finding the Adelie MASS

$$
\hat Y_i = 3706 + 26.92 (0) + 1386.27 (0)
$$

## Finding the Chinstrap MASS

$$
\hat Y_i = 3706 + 26.92 (1) + 1386.27 (0)
$$

## Finding the Gentoo MASS

$$
\hat Y_i = 3706 + 26.92 (0) + 1386.27 (1)
$$

## Intepreting $\hat \beta_1$

On average, Chinstrap has a larger mass than Adelie by about 26.92 grams.

## Intepreting $\hat \beta_2$

On average, Gentoo has a larger mass than Adelie by about 1386.27 grams.

# Strength and Correlation

## Correlation

Correlation is a statistics that can be used to describe the strength of the relationship between 2 continuous variables.

::: fragment
$$
r = \frac{1}{n-1}\sum^n_{i=1}\frac{x_i - \bar x}{s_x}\frac{y_i - \bar y}{s_y}
$$

-   $\bar x$, $\bar y$: sample means
-   $s_x$, $s_y$: sample standard deviations

:::

::: fragment
$$
-1 \leq r \leq 1
$$
:::


## Correlations

![From IMS 2e](https://openintro-ims.netlify.app/model-slr_files/figure-html/fig-posNegCorPlots-1.png)

## Coefficient of Determination

The coefficient of determination evaluates the strength between an outcome $Y$ and the linear model, which includes $X$.

::: fragment
$$
R^2 = r^2
$$
:::

::: fragment
$$
0 \leq R^2 \leq 1
$$
:::

::: fragment
The coefficient of determination measures the total variation explained by the linear model. The closer to 1, the better the linear model.
:::

## Correlation in R

```{r}
#| eval: false
cor(DATA$Y, DATA$X)
```

## Example

```{r}
cor(penguins$body_mass_g, penguins$flipper_length_mm)
```

## Coefficient of Determination in R

```{r}
#| eval: false
xlm <- lm(Y ~ X, data = DATA)
r2(xlm)
```


## Example

```{r}
xlm <- lm(body_mass_g ~ species, penguins)
r2(xlm)
```
