---
title: "Statistical Inference"
date: 4/10/25
description: |
  Statistical Inference

format:
  revealjs:
    width: 1200
    scrollable: true
    sc-sb-title: true
    footer: <https://m201.inqs.info/lectures/10>
    theme: [default, styles.scss]
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    code-fold: true
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen
  - reveal-auto-agenda

editor: source
---

```{r}
#| include: false

library(tidyverse)
library(csucistats)
library(MASS)
Melanoma$dead <- ifelse(Melanoma$status == 1, 1, 0)

theme_set(theme_bw() + 
            theme(
              axis.text.x = element_text(size = 24),
              axis.title = element_text(size = 30),
              plot.title = element_text(size = 48),
              strip.text = element_text(size = 20),
              legend.title = element_blank(),
              legend.text = element_text(size = 24)))

```


# Statistical Inference

## What is Statistical Inference?

- Drawing conclusions about a **population** based on a **sample**
- Population = entire group; Sample = subset

::: notes
Introduce the big idea: We want to make statements about a large group (population) but can only observe a small part of it (sample). Inference bridges that gap.
:::

## Why It Matters

- Studying populations directly is hard or impossible.
- We use data from samples to make predictions and decisions.

::: notes
Explain practical challenges: cost, time, or accessibility of population data. This motivates the need for inference.
:::

## Two Main Types of Inference

1. Estimation  
2. Hypothesis Testing

::: notes
We’ll be focusing on two fundamental techniques in inference. First, estimating population values (like the mean), and second, testing claims about the population.
:::

## Estimation

- **Point Estimate**: Single best guess (e.g., $\hat \beta_1$)
- **Interval Estimate**: Range likely to contain the true value

::: notes
Point estimates are easy but not very informative. Intervals give us a sense of uncertainty, which is critical in inference.
:::

## Hypothesis Testing

- $H_0$: No effect or difference  
- $H_1$: Some effect or difference  
- We use sample data to support or reject $H_0$

::: notes
Mention that $H_0$ is the default assumption. We only reject it if the data give us strong enough evidence.
:::

## Key Concepts and Tools

- Sampling Distribution
- Central Limit Theorem
- Standard Error

::: notes
These three concepts are foundational. Understanding them helps us assess how reliable our estimates are.
:::


## Confidence Intervals

- A range where we expect the true value to fall

::: notes
Clarify interpretation: it's not about the probability the parameter is inside the interval, but about the method producing accurate intervals in the long run.
:::

## Confidence Interval Formula

$$
\bar{x} \pm z^* \cdot \frac{\sigma}{\sqrt{n}}
$$

::: notes
$z^*$ depends on the confidence level, like 1.96 for 95%. Explain how higher confidence means wider intervals.
:::


## Hypothesis Testing Steps

1. State $H_0$ and $H_1$  
2. Choose $\alpha$  
3. Compute confidence interval/p-value  
5. Make a decision

::: notes
Walk through the steps slowly with an example in mind. Emphasize that $\alpha$ is a threshold, not the actual probability of error.
:::

## p-values

- Probability of observing data as extreme as this if $H_0$ is true

::: notes
Misinterpretation of p-values is common. Emphasize: low p-value means data is unusual under $H_0$.
:::


# Power Analysis

## Errors in Inference

| | | |
|:-|:-|:-|
| Type I | Reject $H_0$ when true | False positive |
| Type II | Don’t reject $H_0$ when false | False negative |
| Power | $1 - P(\text{Type II})$ | Detecting a true effect |

::: notes
Power is often overlooked. It's about how sensitive the test is to real effects. Larger samples increase power.
:::


## Type I Error (False Positive)

- **Rejecting $H_0$ when it is actually true**
- Probability = $\alpha$ (significance level)

::: notes
Type I errors happen when we detect an effect that doesn’t really exist. This is controlled by our chosen alpha level.
:::

## Type II Error (False Negative)

- **Failing to reject $H_0$ when it is actually false**
- Probability = $\beta$
- Power = $1 - \beta$


::: notes
Type II errors are often due to small sample sizes or high variability. Power analysis helps us plan to avoid these.
:::

## Balancing Errors

- Lowering $\alpha$ reduces Type I errors, but **increases** risk of Type II errors.
- To reduce both:
  - Increase sample size
  - Use more appropriate statistical tests

::: notes
There’s a trade-off between these errors. We can’t eliminate both, but we can **manage** the risk based on the consequences of each type.
:::

## Real-World Examples

| Scenario                    | Type I Error                  | Type II Error                    |
|-----------------------------|-------------------------------|----------------------------------|
| Medical Test (e.g., cancer) | Healthy person diagnosed      | Missed diagnosis                 |
| Spam Filter                 | Good email marked as spam     | Spam not caught                  |
| Judicial Trial              | Innocent person convicted     | Guilty person acquitted          |

::: notes
Use these examples to help students internalize the concepts. Ask them to think of scenarios in their field.
:::

## What is Statistical Power?

- **Statistical Power** is the probability of correctly rejecting a false null hypothesis.
- In other words, it's the chance of **detecting a real effect** when it exists.

$$
\text{Power} = 1 - \beta
$$

::: notes
Introduce power as a key measure of a test’s sensitivity. It tells us how likely we are to catch a true effect.
:::


## Why Power Matters

- Low power → high risk of **Type II Error** (false negatives)
- High power → better chance of finding true effects
- Common threshold: **80% power**

::: notes
A test with low power might miss important effects. 80% is a widely used benchmark in research planning.
:::


## What Affects Power?

1. **Effect Size**  
   - Bigger effects are easier to detect

2. **Sample Size ($n$)**  
   - Larger samples reduce standard error

3. **Significance Level ($\alpha$)**  
   - Higher $\alpha$ increases power (but riskier!)

4. **Variability**  
   - Less noise in data = better power

::: notes
Use this to emphasize the practical levers we have. Sample size is one of the easiest things we can change to improve power.
:::

## Example: Power in a Drug Trial

- $H_0$: New drug has no effect  
- $H_1$: New drug lowers blood pressure by 5 mmHg  
- If the test has **low power**, we might fail to detect this true improvement.

::: notes
Medical examples make power feel real. Failing to detect real treatment effects could cost lives—or result in false conclusions.
:::

---

## Boosting Power

- Increase sample size ($n$)
- Use better statistical models
- Increase $\alpha$ (carefully)
- Use one-tailed test (if direction is justified)

::: notes
Show students that power is not fixed—they can plan studies to improve it. Emphasize ethical and scientific trade-offs.
:::

# Confidence Intervals

## Confidence Intervals

- A confidence interval gives a **range of plausible values** for a population parameter.
- It reflects **uncertainty** in point estimates from sample data.

::: notes
Introduce confidence intervals as the natural next step after understanding sampling variability and standard error. Emphasize that point estimates are useful, but intervals give a more complete picture.
:::

---

## Interpretation

> "We are 95% confident that the true mean lies between A and B."

- This does **not** mean there's a 95% chance the mean is in that interval.
- It means: if we repeated the sampling process many times, **95% of the intervals would contain the true value**.

::: notes
This is one of the most common misconceptions. Clarify that the confidence is in the *method*, not any one interval.
:::

---

## Formula (Known $\sigma$)

$$
\bar{x} \pm z^* \cdot \frac{\sigma}{\sqrt{n}}
$$

- $\bar{x}$ = sample mean  
- $\sigma$ = population standard deviation  
- $z^*$ = critical value (e.g., 1.96 for 95%)

::: notes
This formula is used when the population standard deviation $\sigma$ is known. It assumes a normal distribution for $\bar{x}$.
:::

---

## Formula (Unknown $\sigma$)

$$
\bar{x} \pm t^* \cdot \frac{s}{\sqrt{n}}
$$

- $s$ = sample standard deviation  
- $t^*$ = critical value from $t$-distribution ($n-1$ df)

::: notes
In real-world scenarios, $\sigma$ is rarely known. We estimate it using $s$ and use the $t$-distribution to account for extra uncertainty.
:::

---

## Choosing Confidence Levels

| Level | $z^*$ (approx.) |
|-------|-----------------|
| 90%   | 1.645           |
| 95%   | 1.960           |
| 99%   | 2.576           |

::: notes
Higher confidence means wider intervals. Discuss the trade-off: precision vs certainty.
:::

---

## Example

- $n = 100$, $\bar{x} = 68$, $\sigma = 3$
- 95% CI:

$$
68 \pm 1.96 \cdot \frac{3}{\sqrt{100}} = 68 \pm 0.588
$$

**CI: [67.412, 68.588]**

::: notes
Walk through the calculation step-by-step. Emphasize how increasing $n$ or decreasing $\sigma$ would narrow the interval.
:::


## Factors Affecting CI Width

- Sample size ($n$): larger $n$ → narrower CI  
- Standard deviation ($s$ or $\sigma$): more variability → wider CI  
- Confidence level: higher confidence → wider CI

::: notes
Use this to summarize what controls how “precise” our confidence interval is. Give examples of each.
:::


# Inference: Linear Regression

## Inference for Regression Coefficients

- $Y = \beta_0 + \beta_1 X + \varepsilon$
- We estimate $\hat{\beta}_0$ and $\hat{\beta}_1$

::: notes
Now we're applying inference in a regression context. Same principles apply—estimating, testing, confidence intervals.
:::


## Sampling Distribution of $\hat{\beta}_1$

- $\hat{\beta}_1$ is normally distributed (under assumptions)
- SE depends on spread of $x_i$

::: notes
Variance in $x_i$ helps reduce uncertainty in slope estimate. More spread = more informative data.
:::


## Confidence Interval for $\beta_1$

$$
\hat{\beta}_1 \pm t^* \cdot SE(\hat{\beta}_1)
$$

::: notes
This is like the CI for the mean, but uses the $t$-distribution because $\sigma$ is estimated.
:::


## Hypothesis Testing for $\beta_1$

- $H_0: \beta_1 = 0$  
- $t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}$

::: notes
We're testing whether the slope is zero—i.e., whether $X$ is a useful predictor for $Y$.
:::

## Interpretation

- If $p < \alpha$, conclude $\beta_1 \ne 0$
- If not, we can't say $X$ affects $Y$

::: notes
Make it clear that failing to reject $H_0$ doesn't prove it's true—just that we lack strong evidence against it.
:::


## Regression Example

- $\hat{\beta}_1 = 2.5$, $SE = 0.8$, $n = 30$
- 95% CI: $[0.86,\ 4.14]$  
- $t = 3.125$, $p < 0.01$

::: notes
Highlight how the CI and hypothesis test lead to the same conclusion: the slope is likely not zero.
:::




