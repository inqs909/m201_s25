{
  "hash": "cf9239fe131703b861ac9b7a2b646ff1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distribution Functions\"\ndate: 2/11/25\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: notes/chalkboard_1a.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n\n\n\n\n\n# Probability Theory\n\n## What is Probability?\n\n**Probability** is the measure of how likely an event is to occur. It ranges from 0 to 1:\n- $P(A) = 0$: The event $A$ will definitely not happen.\n- $P(A) = 1$: The event $A$ will definitely happen.\n- Values between 0 and 1 represent varying degrees of likelihood.\n\n## Everyday Examples\n- What’s the probability it will rain tomorrow?\n- What are the chances of rolling a 6 on a standard die?\n- How likely is it that a randomly chosen student has a GPA above 3.0?\n\n\n## Key Terms and Definitions\n\n::: panel-tabset\n\n### **Experiment**\nAn action or process that generates outcomes.  \n- Example: Rolling a die.\n\n### **Sample Space**\nThe set of all possible outcomes of an experiment.  \n- Example: For rolling a die, $S = \\{1, 2, 3, 4, 5, 6\\}$.\n\n### **Event**\nA subset of the sample space, representing outcomes of interest.  \n- Example: Rolling an even number ($A = \\{2, 4, 6\\}$).\n\n### **Prob. of an Event**\nThe proportion of times an event is expected to occur if the experiment is repeated many times.\n\n:::\n\n## The Probability Formula\n\nThe probability of an event $A$ is defined as:\n\n$$\nP(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}\n$$\n\n## Example\nIf we roll a die, what is the probability of rolling a 4?\n\n- **Favorable outcomes**: 1 (rolling a 4).\n- **Total outcomes**: 6 (since $S = \\{1, 2, 3, 4, 5, 6\\}$).\n\n$$\nP(\\text{rolling a 4}) = \\frac{1}{6} \\approx 0.167\n$$\n\n\n## Rules of Probability\n\n::: panel-tabset\n\n### Rule 1\n\n**Probability of the Sample Space**\n\nThe probability of the sample space is always 1:\n$$\nP(S) = 1\n$$\n\n### Rule 2 \n\n**Probability of Impossible Events**\n\nThe probability of an event that cannot happen is 0:\n$$\nP(\\emptyset) = 0\n$$\n\n### Rule 3\n\n**Complement Rule**\n\nThe probability of the complement of an event $A$ (not $A$) is:\n\n$$\nP(A^c) = 1 - P(A)\n$$\n- Example: If $P(\\text{rain}) = 0.3$, then $P(\\text{no rain}) = 1 - 0.3 = 0.7$.\n\n### Rule 4\n\n**Addition Rule**\n\nFor two events $A$ and $B$:\n- If $A$ and $B$ are **mutually exclusive** (cannot happen at the same time):\n$$\nP(A \\cup B) = P(A) + P(B)\n$$\n\n- If $A$ and $B$ are **not mutually exclusive**:\n\n$$\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n$$\n\n### Rule 5\n\n**Multiplication Rule**\n\nFor two events $A$ and $B$:\n- If $A$ and $B$ are **independent**:\n\n$$\nP(A \\cap B) = P(A) \\cdot P(B)\n$$\n\n:::\n\n## Applications\n\n::: panel-tabset\n\n### Drawing a Card\nIf you draw a card from a standard deck of 52 cards, what is the probability of drawing:\n\n1. A heart?  \n  $$\n   P(\\text{heart}) = \\frac{13}{52} = 0.25\n  $$\n\n2. A red card (heart or diamond)?  \n  $$\n   P(\\text{red card}) = \\frac{26}{52} = 0.5\n  $$\n\n### Tossing a Coin Twice\nWhat is the probability of getting:\n\n1. Exactly one head?  \n   Sample space: $S = \\{\\text{HH, HT, TH, TT}\\}$.  \n   Event: $A = \\{\\text{HT, TH}\\}$.  \n   $$\n   P(A) = \\frac{2}{4} = 0.5\n   $$\n\n2. At least one head?  \n   Event: $B = \\{\\text{HH, HT, TH}\\}$.  \n   $$\n   P(B) = \\frac{3}{4} = 0.75\n   $$\n\n### Traffic Lights\nA commuter encounters three traffic lights, each with a 70% chance of being green. Assuming independence, what is the probability that all three lights are green?\n\n$$\nP(\\text{all green}) = 0.7 \\cdot 0.7 \\cdot 0.7 = 0.343\n$$\n\n:::\n\n# Distribution Functions\n\n## Distribution Functions\n\nA **distribution function** describes the probabilities of a random variable across its possible values. It answers questions like:\n\n- How likely is a random variable to take on a specific value or fall within a range?\n- What is the overall \"shape\" of the distribution of outcomes?\n\n## Types of Distribution Functions\n\n::: panel-tabset\n\n### **Discrete Distribution**\n\nUsed for **discrete random variables**, which can take on countable values (e.g., integers). Examples include:\n\n- Number of heads in coin tosses.\n- Number of customers in a store.\n\n### **Continuous Distribution**\nUsed for **continuous random variables**, which can take on any value within a range (e.g., real numbers). Examples include:\n\n- Heights of people.\n- Time it takes to complete a task.\n\n:::\n\n## Cumulative Distribution Function (CDF)\n\nThe **Cumulative Distribution Function (CDF)** describes the probability that a random variable $X$ is less than or equal to a certain value $x$:\n$$\nF_X(x) = P(X \\leq x)\n$$\n\n## Properties of the CDF\n\n1. **Range**: The CDF is always between 0 and 1:\n   $$\n   0 \\leq F_X(x) \\leq 1\n   $$\n2. **Non-Decreasing**: The CDF never decreases as $x$ increases.\n3. **Limits**:\n   - $\\lim_{x \\to -\\infty} F_X(x) = 0$\n   - $\\lim_{x \\to \\infty} F_X(x) = 1$\n\n## CDF Example\n\nFor a die roll (discrete case):\n\n- $F_X(2) = P(X \\leq 2) = P(X = 1) + P(X = 2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}$.\n\nFor a normal distribution (continuous case):\n\n- Use the CDF to find probabilities, typically provided via tables or software.\n\n\n## Probability Density Function (PDF)\n\nThe **Probability Density Function (PDF)** is used for **continuous random variables** and describes the likelihood of the variable falling within a small interval.\n\n$$\nP(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx\n$$\n\n## Properties of the PDF:\n\n1. **Non-Negative**:\n   $$\n   f_X(x) \\geq 0 \\quad \\forall x\n   $$\n\n2. **Total Area Under Curve**:\n   $$\n   \\int_{-\\infty}^\\infty f_X(x) \\, dx = 1\n   $$\n\n## PDF Examples\n\nThe PDF of the normal distribution is:\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\nHere, $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n\n\n\n## Probability Mass Function (PMF)\n\nThe **Probability Mass Function (PMF)** is used for **discrete random variables** and gives the probability of each possible value:\n$$\nP(X = x) = p_X(x)\n$$\n\n## Properties of the PMF\n1. **Non-Negative**:\n   $$\n   p_X(x) \\geq 0 \\quad \\forall x\n   $$\n\n2. **Sum of Probabilities**:\n   $$\n   \\sum_x p_X(x) = 1\n   $$\n\n## PMF Example\n\nFor a fair die, the PMF is:\n$$\np_X(x) = \\frac{1}{6}, \\quad x \\in \\{1, 2, 3, 4, 5, 6\\}\n$$\n\n\n## Applications\n\n::: panel-tabset\n\n### PMF of a Coin Toss\nFor a fair coin tossed once:\n- $ X = 0 $: Tails, $ P(X = 0) = 0.5 $\n- $ X = 1 $: Heads, $ P(X = 1) = 0.5 $\n\nThe PMF is:\n$$\np_X(x) = \n\\begin{cases} \n0.5 & x = 0 \\text{ or } x = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\n\n### CDF of an Exponential Distribution\nAn exponential random variable with rate $ \\lambda > 0 $ has a CDF:\n$$\nF_X(x) = \n\\begin{cases} \n1 - e^{-\\lambda x}, & x \\geq 0 \\\\\n0, & x < 0\n\\end{cases}\n$$\n\nThis can be used to model waiting times between events.\n\n### Applications in Real Life\n1. **PMF**:\n   - Number of emails received per hour.\n2. **PDF**:\n   - Heights of students in a class.\n3. **CDF**:\n   - The likelihood of completing a task within a certain time frame.\n\n:::\n\n\n# Normal Distribution\n\n## Normal Distribution\n\nThe **Normal Distribution** is a probability distribution that is symmetric, with most of the data points clustering around the mean. \n\n- It’s **bell-shaped** and is defined mathematically by two parameters:\n  - **Mean ($\\mu$)**: The center or peak of the distribution.\n  - **Standard Deviation ($\\sigma$)**: Controls the spread of the distribution.\n\n\n::: fragment\n::: callout-important\nFor any normally distributed data, the highest probability density is at the mean, and as you move away from the mean, the probability density gradually decreases.\n:::\n:::\n\n## Properties\n\n\n1. **Symmetry**: It is perfectly symmetric about the mean, meaning the left side is a mirror image of the right.\n2. **Unimodal**: There is a single peak at the mean.\n3. **Mean, Median, and Mode are Equal**: In a normal distribution, these three measures of central tendency are located at the same point.\n4. **68-95-99.7 Rule** (Empirical Rule): \n   - About **68%** of data falls within **one standard deviation** of the mean.\n   - About **95%** of data falls within **two standard deviations**.\n   - About **99.7%** of data falls within **three standard deviations**.\n\n::: notes\n\nThis rule helps us understand how data is distributed in a normal curve and provides a quick way to estimate probabilities for normally distributed data.\n\n:::\n\n## Normal Distribution\n\n## The Standard Normal Distribution\n\nThe **Standard Normal Distribution** is a special type of normal distribution with a mean of 0 and a standard deviation of 1. It's often used as a reference to convert any normal distribution to a standard form.\n\n## Z-Scores\nA **Z-score** (or standard score) tells us how many standard deviations an individual data point is from the mean. It’s calculated as:\n\n$$\nZ = \\frac{X - \\mu}{\\sigma}\n$$\n\n- If $Z$ is positive, the data point is above the mean.\n- If $Z$ is negative, the data point is below the mean.\n- Using Z-scores, we can compare values across different normal distributions or find the probability associated with a particular score.\n\n\n## Why the Normal Distribution is Important\n\n\n1. **It Describes Many Natural Phenomena**: Heights, weights, test scores, measurement errors, and countless other variables follow a normal distribution, especially when influenced by many small, random factors.\n2. **Predictive Power**: With normally distributed data, we can make predictions and infer probabilities, thanks to the 68-95-99.7 rule.\n3. **Central Limit Theorem**: The normal distribution is foundational to the Central Limit Theorem, which tells us that, regardless of the original data distribution, the sampling distribution of the sample mean will approach a normal distribution as sample size increases.\n4. **Ease of Use in Statistical Methods**: Many statistical tests and methods assume normality, allowing for simplified calculations and reliable inferences.\n\n\n## Applications of the Normal Distribution\n\n::: panel-tabset\n\n### Standardized Testing\nScores on standardized tests, such as IQ tests or the SAT, are often designed to follow a normal distribution. By knowing a student’s score in terms of Z-scores, we can determine their percentile or how they compare to other test-takers.\n\n### Finance\nIn finance, stock returns and other economic factors are often modeled with a normal distribution to estimate risk, forecast trends, and make informed investment decisions.\n\n:::\n\n\n# Empirical Rule\n\n\n## What is the Empirical Rule?\n\nThe **Empirical Rule** provides a way to understand the spread of data in a normal distribution by describing how data points cluster around the mean. According to this rule:\n\n1. Approximately **68%** of data points fall within **one standard deviation** of the mean.\n2. Approximately **95%** of data points fall within **two standard deviations** of the mean.\n3. Approximately **99.7%** of data points fall within **three standard deviations** of the mean.\n\nThe empirical rule is very helpful because, with just the mean and standard deviation, we can quickly estimate how data is distributed within a normal curve.\n\n\n## Applying the Empirical Rule to the Normal Distribution\n\nLet’s define the key terms and apply the empirical rule to the normal distribution.\n\n- **Mean (μ)**: This is the central point of the normal distribution where the data clusters around.\n- **Standard Deviation (σ)**: This is a measure of how spread out the data points are from the mean.\n\nIn a normal distribution:\n- **68% of data** lies between $(\\mu - \\sigma)$ and $(\\mu + \\sigma)$.\n- **95% of data** lies between $(\\mu - 2\\sigma)$ and $(\\mu + 2\\sigma)$.\n- **99.7% of data** lies between $(\\mu - 3\\sigma)$ and $(\\mu + 3\\sigma)$.\n\nThese intervals allow us to estimate probabilities for data within each range without needing to calculate exact probabilities.\n\n\n## Visualizing the Empirical Rule\n\nTo better understand the empirical rule, imagine a symmetric, bell-shaped normal curve. Here’s how it would look based on the empirical rule:\n\n1. The **68% region** represents the middle of the curve, starting one standard deviation left of the mean and ending one standard deviation right.\n2. The **95% region** stretches further out, covering almost the entire curve except for the outer tails.\n3. The **99.7% region** includes nearly all data points, covering the entire curve except for a tiny fraction at each extreme.\n\nThis visualization shows how the data is most concentrated around the mean, with less data appearing as we move further away.\n\n\n## Using the Empirical Rule for Probabilities\n\nThe empirical rule helps us answer questions like:\n\n- **What percentage of data points fall within a certain range?**\n- **How unusual is a data point located far from the mean?**\n\nFor example:\n- If we know that a data point lies more than two standard deviations away from the mean, we know it’s in the outer 5% of the distribution, making it relatively rare.\n- Using the rule, we can estimate that around 95% of values should lie within two standard deviations of the mean. If we observe data points outside of this range, we might consider them outliers.\n\n\n## Examples\n\n::: panel-tabset\n\n### Exam Scores\nSuppose exam scores are normally distributed with a mean of 70 and a standard deviation of 10.\n- **68% of students** scored between **60 and 80** (70 ± 10).\n- **95% of students** scored between **50 and 90** (70 ± 20).\n- **99.7% of students** scored between **40 and 100** (70 ± 30).\n\n### Heights of Adults\nAssume that adult heights follow a normal distribution with a mean height of 170 cm and a standard deviation of 8 cm.\n- **68% of adults** have heights between **162 cm and 178 cm** (170 ± 8).\n- **95% of adults** have heights between **154 cm and 186 cm** (170 ± 16).\n- **99.7% of adults** have heights between **146 cm and 194 cm** (170 ± 24).\n\nUsing the empirical rule, we can see how most adult heights fall within predictable ranges around the mean.\n\n:::\n\n# Binomial Distribution\n\n## Binomial Distribution\n\nThe **Binomial Distribution** is a probability distribution that models the number of successes in a fixed number of trials, where each trial has:\n- **Two possible outcomes**: typically called \"success\" and \"failure.\"\n- A constant probability of success, $ p $, on each trial.\n\n### Real-World Examples:\n- Tossing a coin $ n $ times and counting how many heads you get.\n- Rolling a die $ n $ times and counting how many times you roll a 6.\n- Administering a medical treatment to $ n $ patients and recording how many recover.\n\nThe binomial distribution answers questions like:\n- \"What is the probability of getting exactly 3 heads in 5 coin tosses?\"\n- \"What is the likelihood of at least 4 successes in 10 trials?\"\n\n\n## Conditions for a Binomial Experiment\n\nA binomial experiment must satisfy these four conditions:\n\n1. **Fixed Number of Trials** ($ n $):\n   - The experiment consists of a set number of trials.\n\n2. **Two Possible Outcomes**:\n   - Each trial results in either a success (e.g., heads) or a failure (e.g., tails).\n\n3. **Constant Probability of Success** ($ p $):\n   - The probability of success remains the same for each trial.\n\n4. **Independence**:\n   - The outcome of one trial does not affect the outcomes of other trials.\n\nIf these conditions are met, the binomial distribution is the right model to use.\n\n\n## The Binomial Probability Formula\n\nThe probability of observing exactly $ k $ successes in $ n $ trials is given by the **binomial probability formula**:\n\n$$\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n$$\n\nWhere:\n- $ P(X = k) $: Probability of exactly $ k $ successes.\n- $ n $: Number of trials.\n- $ k $: Number of successes.\n- $ p $: Probability of success on a single trial.\n- $ 1-p $: Probability of failure.\n- $ \\binom{n}{k} $: Binomial coefficient, which represents the number of ways to choose $ k $ successes from $ n $ trials. It’s calculated as:\n\n$$\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n$$\n\n\n### Example Calculation\nSuppose you flip a fair coin 5 times ($ n = 5, p = 0.5 $) and want to know the probability of getting exactly 3 heads ($ k = 3 $).\n\nUsing the formula:\n\n$$\nP(X = 3) = \\binom{5}{3} (0.5)^3 (1-0.5)^{5-3}\n$$\n\n1. Calculate $ \\binom{5}{3} $:\n$$\n\\binom{5}{3} = \\frac{5!}{3!(5-3)!} = \\frac{120}{6 \\cdot 2} = 10\n$$\n\n2. Plug into the formula:\n$$\nP(X = 3) = 10 (0.5)^3 (0.5)^2 = 10 (0.125)(0.25) = 0.3125\n$$\nExamples and \nSo, the probability of getting exactly 3 heads is **0.3125**, or 31.25%.\n\n## Key Properties of the Binomial Distribution\n\n### Mean and Variance\nFor a binomial random variable $ X $:\n- **Mean (Expected Value)**: $ \\mu = n \\cdot p $\n- **Variance**: $ \\sigma^2 = n \\cdot p \\cdot (1-p) $\n- **Standard Deviation**: $ \\sigma = \\sqrt{n \\cdot p \\cdot (1-p)} $\n\n### Shape of the Distribution\n- If $ p = 0.5 $, the distribution is symmetric.\n- If $ p > 0.5 $, the distribution is skewed left.\n- If $ p < 0.5 $, the distribution is skewed right.\n\n\n## Applications\n\n::: panel-tabset\n\n### Quality Control\nA factory produces lightbulbs, and 95% of them meet quality standards. If you randomly test 10 bulbs, what is the probability that exactly 8 bulbs pass the test?\n\nHere:\n- $ n = 10 $, $ p = 0.95 $, $ k = 8 $.\n\nUsing the formula, calculate:\n$$\nP(X = 8) = \\binom{10}{8} (0.95)^8 (0.05)^2\n$$\n\n### Sports\nA basketball player has a 60% chance of making a free throw. If they take 5 shots, what is the probability of making at least 3 shots?\n\nHere:\n- $ n = 5 $, $ p = 0.6 $.\n- We calculate $ P(X \\geq 3) = P(X = 3) + P(X = 4) + P(X = 5) $.\n\n### Clinical Trials\nIn a clinical trial, a new drug has a 70% success rate. If 15 patients are treated, what is the probability that exactly 10 respond positively to the treatment?\n\n:::\n\n# Poisson Distribution\n\n\n## Poisson Distribution\n\nThe **Poisson Distribution** is a discrete probability distribution that models the number of events occurring within a fixed interval. These events must happen independently and at a constant average rate. \n\n### Real-World Examples:\n- The number of emails you receive in an hour.\n- The number of cars passing through a toll booth in 10 minutes.\n- The number of defects in a square meter of fabric.\n\nThe Poisson distribution helps us answer questions like:\n- \"What is the probability of receiving 5 emails in the next hour?\"\n- \"How likely is it to have 2 defects in a single square meter?\"\n\n\n## Conditions for Using the Poisson Distribution\n\nTo use the Poisson distribution, the following conditions must be met:\n\n1. **Events Occur Randomly**:\n   - The events are random and unpredictable.\n\n2. **Independence**:\n   - The occurrence of one event does not affect the probability of another event occurring.\n\n3. **Constant Average Rate** ($ \\lambda $):\n   - The average number of events ($ \\lambda $) over a fixed interval remains constant.\n\n4. **Non-Overlapping Intervals**:\n   - Events in one interval do not influence events in another.\n\n\n## The Poisson Probability Formula\n\nThe probability of observing exactly $ k $ events in a fixed interval is given by the **Poisson probability formula**:\n\n$$\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n$$\n\nWhere:\n- $ P(X = k) $: Probability of $ k $ events occurring.\n- $ \\lambda $: Average number of events in the interval.\n- $ e $: Euler's number ($ \\approx 2.718 $).\n- $ k! $: Factorial of $ k $, calculated as $ k \\times (k-1) \\times \\dots \\times 1 $.\n\n### Example Calculation\nSuppose a call center receives an average of 10 calls per hour ($ \\lambda = 10 $). What is the probability of receiving exactly 7 calls in the next hour ($ k = 7 $)?\n\nUsing the formula:\n\n$$\nP(X = 7) = \\frac{10^7 e^{-10}}{7!}\n$$\n\n1. Calculate $ e^{-10} \\approx 0.0000454 $.\n2. Compute $ 10^7 = 10,000,000 $.\n3. Find $ 7! = 7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1 = 5040 $.\n4. Plug in the values:\n$$\nP(X = 7) = \\frac{10,000,000 \\cdot 0.0000454}{5040} \\approx 0.0902\n$$\n\nSo, the probability of receiving exactly 7 calls is approximately **9.02%**.\n\n\n## Poisson Distribution Properties\n\n### Mean and Variance\nFor a Poisson random variable $ X $:\n- **Mean**: $ \\mu = \\lambda $\n- **Variance**: $ \\sigma^2 = \\lambda $\n- **Standard Deviation**: $ \\sigma = \\sqrt{\\lambda} $\n\n### Shape of the Distribution\n- When $ \\lambda $ is small, the distribution is highly skewed to the right.\n- As $ \\lambda $ increases, the distribution becomes more symmetric and resembles a normal distribution.\n\n\n## Applications\n\n::: panel-tabset\n\n### Traffic Flow\nA toll booth observes an average of 3 cars passing through every 5 minutes ($ \\lambda = 3 $). What is the probability of seeing exactly 5 cars in the next 5 minutes?\n\nUsing the formula:\n$$\nP(X = 5) = \\frac{3^5 e^{-3}}{5!} = \\frac{243 \\cdot 0.0498}{120} \\approx 0.1008\n$$\n\nSo, the probability is approximately **10.08%**.\n\n\n### Defective Products\nA factory produces an average of 2 defective items per day ($ \\lambda = 2 $). What is the probability of finding no defective items in a day ($ k = 0 $)?\n\n$$\nP(X = 0) = \\frac{2^0 e^{-2}}{0!} = e^{-2} \\approx 0.1353\n$$\n\nSo, there’s a **13.53% chance** of finding no defective items.\n\n\n\n### Hospital Arrivals\nOn average, 4 patients arrive at an emergency room every hour ($ \\lambda = 4 $). What is the probability of seeing more than 6 patients in an hour?\n\nThis is a cumulative probability:\n$$\nP(X > 6) = 1 - P(X \\leq 6)\n$$\nYou would calculate $ P(X \\leq 6) $ by summing probabilities for $ P(X = 0) $ through $ P(X = 6) $ using the formula.\n\n:::",
    "supporting": [
      "5_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}