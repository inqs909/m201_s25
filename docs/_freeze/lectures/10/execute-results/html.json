{
  "hash": "6678447d4d00a058179b5773b5d993ab",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Inference\"\ndate: 4/10/25\ndescription: |\n  Statistical Inference\n\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    sc-sb-title: true\n    footer: <https://m201.inqs.info/lectures/10>\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n\n\n\n\n# Statistical Inference\n\n## What is Statistical Inference?\n\n- Drawing conclusions about a **population** based on a **sample**\n- Population = entire group; Sample = subset\n\n::: notes\nIntroduce the big idea: We want to make statements about a large group (population) but can only observe a small part of it (sample). Inference bridges that gap.\n:::\n\n## Why It Matters\n\n- Studying populations directly is hard or impossible.\n- We use data from samples to make predictions and decisions.\n\n::: notes\nExplain practical challenges: cost, time, or accessibility of population data. This motivates the need for inference.\n:::\n\n## Two Main Types of Inference\n\n1. Estimation  \n2. Hypothesis Testing\n\n::: notes\nWe’ll be focusing on two fundamental techniques in inference. First, estimating population values (like the mean), and second, testing claims about the population.\n:::\n\n## Estimation\n\n- **Point Estimate**: Single best guess (e.g., $\\hat \\beta_1$)\n- **Interval Estimate**: Range likely to contain the true value\n\n::: notes\nPoint estimates are easy but not very informative. Intervals give us a sense of uncertainty, which is critical in inference.\n:::\n\n## Hypothesis Testing\n\n- $H_0$: No effect or difference  \n- $H_1$: Some effect or difference  \n- We use sample data to support or reject $H_0$\n\n::: notes\nMention that $H_0$ is the default assumption. We only reject it if the data give us strong enough evidence.\n:::\n\n## Key Concepts and Tools\n\n- Sampling Distribution\n- Central Limit Theorem\n- Standard Error\n\n::: notes\nThese three concepts are foundational. Understanding them helps us assess how reliable our estimates are.\n:::\n\n\n## Confidence Intervals\n\n- A range where we expect the true value to fall\n\n::: notes\nClarify interpretation: it's not about the probability the parameter is inside the interval, but about the method producing accurate intervals in the long run.\n:::\n\n## Confidence Interval Formula\n\n$$\n\\bar{x} \\pm z^* \\cdot \\frac{\\sigma}{\\sqrt{n}}\n$$\n\n::: notes\n$z^*$ depends on the confidence level, like 1.96 for 95%. Explain how higher confidence means wider intervals.\n:::\n\n\n## Hypothesis Testing Steps\n\n1. State $H_0$ and $H_1$  \n2. Choose $\\alpha$  \n3. Compute confidence interval/p-value  \n5. Make a decision\n\n::: notes\nWalk through the steps slowly with an example in mind. Emphasize that $\\alpha$ is a threshold, not the actual probability of error.\n:::\n\n## p-values\n\n- Probability of observing data as extreme as this if $H_0$ is true\n\n::: notes\nMisinterpretation of p-values is common. Emphasize: low p-value means data is unusual under $H_0$.\n:::\n\n\n# Power Analysis\n\n## Errors in Inference\n\n| | | |\n|:-|:-|:-|\n| Type I | Reject $H_0$ when true | False positive |\n| Type II | Don’t reject $H_0$ when false | False negative |\n| Power | $1 - P(\\text{Type II})$ | Detecting a true effect |\n\n::: notes\nPower is often overlooked. It's about how sensitive the test is to real effects. Larger samples increase power.\n:::\n\n\n## Type I Error (False Positive)\n\n- **Rejecting $H_0$ when it is actually true**\n- Probability = $\\alpha$ (significance level)\n\n::: notes\nType I errors happen when we detect an effect that doesn’t really exist. This is controlled by our chosen alpha level.\n:::\n\n## Type II Error (False Negative)\n\n- **Failing to reject $H_0$ when it is actually false**\n- Probability = $\\beta$\n- Power = $1 - \\beta$\n\n\n::: notes\nType II errors are often due to small sample sizes or high variability. Power analysis helps us plan to avoid these.\n:::\n\n## Balancing Errors\n\n- Lowering $\\alpha$ reduces Type I errors, but **increases** risk of Type II errors.\n- To reduce both:\n  - Increase sample size\n  - Use more appropriate statistical tests\n\n::: notes\nThere’s a trade-off between these errors. We can’t eliminate both, but we can **manage** the risk based on the consequences of each type.\n:::\n\n## Real-World Examples\n\n| Scenario                    | Type I Error                  | Type II Error                    |\n|-----------------------------|-------------------------------|----------------------------------|\n| Medical Test (e.g., cancer) | Healthy person diagnosed      | Missed diagnosis                 |\n| Spam Filter                 | Good email marked as spam     | Spam not caught                  |\n| Judicial Trial              | Innocent person convicted     | Guilty person acquitted          |\n\n::: notes\nUse these examples to help students internalize the concepts. Ask them to think of scenarios in their field.\n:::\n\n## What is Statistical Power?\n\n- **Statistical Power** is the probability of correctly rejecting a false null hypothesis.\n- In other words, it's the chance of **detecting a real effect** when it exists.\n\n$$\n\\text{Power} = 1 - \\beta\n$$\n\n::: notes\nIntroduce power as a key measure of a test’s sensitivity. It tells us how likely we are to catch a true effect.\n:::\n\n\n## Why Power Matters\n\n- Low power → high risk of **Type II Error** (false negatives)\n- High power → better chance of finding true effects\n- Common threshold: **80% power**\n\n::: notes\nA test with low power might miss important effects. 80% is a widely used benchmark in research planning.\n:::\n\n\n## What Affects Power?\n\n1. **Effect Size**  \n   - Bigger effects are easier to detect\n\n2. **Sample Size ($n$)**  \n   - Larger samples reduce standard error\n\n3. **Significance Level ($\\alpha$)**  \n   - Higher $\\alpha$ increases power (but riskier!)\n\n4. **Variability**  \n   - Less noise in data = better power\n\n::: notes\nUse this to emphasize the practical levers we have. Sample size is one of the easiest things we can change to improve power.\n:::\n\n## Example: Power in a Drug Trial\n\n- $H_0$: New drug has no effect  \n- $H_1$: New drug lowers blood pressure by 5 mmHg  \n- If the test has **low power**, we might fail to detect this true improvement.\n\n::: notes\nMedical examples make power feel real. Failing to detect real treatment effects could cost lives—or result in false conclusions.\n:::\n\n---\n\n## Boosting Power\n\n- Increase sample size ($n$)\n- Use better statistical models\n- Increase $\\alpha$ (carefully)\n- Use one-tailed test (if direction is justified)\n\n::: notes\nShow students that power is not fixed—they can plan studies to improve it. Emphasize ethical and scientific trade-offs.\n:::\n\n# Confidence Intervals\n\n## Confidence Intervals\n\n- A confidence interval gives a **range of plausible values** for a population parameter.\n- It reflects **uncertainty** in point estimates from sample data.\n\n::: notes\nIntroduce confidence intervals as the natural next step after understanding sampling variability and standard error. Emphasize that point estimates are useful, but intervals give a more complete picture.\n:::\n\n---\n\n## Interpretation\n\n> \"We are 95% confident that the true mean lies between A and B.\"\n\n- This does **not** mean there's a 95% chance the mean is in that interval.\n- It means: if we repeated the sampling process many times, **95% of the intervals would contain the true value**.\n\n::: notes\nThis is one of the most common misconceptions. Clarify that the confidence is in the *method*, not any one interval.\n:::\n\n---\n\n## Formula (Known $\\sigma$)\n\n$$\n\\bar{x} \\pm z^* \\cdot \\frac{\\sigma}{\\sqrt{n}}\n$$\n\n- $\\bar{x}$ = sample mean  \n- $\\sigma$ = population standard deviation  \n- $z^*$ = critical value (e.g., 1.96 for 95%)\n\n::: notes\nThis formula is used when the population standard deviation $\\sigma$ is known. It assumes a normal distribution for $\\bar{x}$.\n:::\n\n---\n\n## Formula (Unknown $\\sigma$)\n\n$$\n\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\n$$\n\n- $s$ = sample standard deviation  \n- $t^*$ = critical value from $t$-distribution ($n-1$ df)\n\n::: notes\nIn real-world scenarios, $\\sigma$ is rarely known. We estimate it using $s$ and use the $t$-distribution to account for extra uncertainty.\n:::\n\n---\n\n## Choosing Confidence Levels\n\n| Level | $z^*$ (approx.) |\n|-------|-----------------|\n| 90%   | 1.645           |\n| 95%   | 1.960           |\n| 99%   | 2.576           |\n\n::: notes\nHigher confidence means wider intervals. Discuss the trade-off: precision vs certainty.\n:::\n\n---\n\n## Example\n\n- $n = 100$, $\\bar{x} = 68$, $\\sigma = 3$\n- 95% CI:\n\n$$\n68 \\pm 1.96 \\cdot \\frac{3}{\\sqrt{100}} = 68 \\pm 0.588\n$$\n\n**CI: [67.412, 68.588]**\n\n::: notes\nWalk through the calculation step-by-step. Emphasize how increasing $n$ or decreasing $\\sigma$ would narrow the interval.\n:::\n\n\n## Factors Affecting CI Width\n\n- Sample size ($n$): larger $n$ → narrower CI  \n- Standard deviation ($s$ or $\\sigma$): more variability → wider CI  \n- Confidence level: higher confidence → wider CI\n\n::: notes\nUse this to summarize what controls how “precise” our confidence interval is. Give examples of each.\n:::\n\n\n# Inference: Linear Regression\n\n## Inference for Regression Coefficients\n\n- $Y = \\beta_0 + \\beta_1 X + \\varepsilon$\n- We estimate $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$\n\n::: notes\nNow we're applying inference in a regression context. Same principles apply—estimating, testing, confidence intervals.\n:::\n\n\n## Sampling Distribution of $\\hat{\\beta}_1$\n\n- $\\hat{\\beta}_1$ is normally distributed (under assumptions)\n- SE depends on spread of $x_i$\n\n::: notes\nVariance in $x_i$ helps reduce uncertainty in slope estimate. More spread = more informative data.\n:::\n\n\n## Confidence Interval for $\\beta_1$\n\n$$\n\\hat{\\beta}_1 \\pm t^* \\cdot SE(\\hat{\\beta}_1)\n$$\n\n::: notes\nThis is like the CI for the mean, but uses the $t$-distribution because $\\sigma$ is estimated.\n:::\n\n\n## Hypothesis Testing for $\\beta_1$\n\n- $H_0: \\beta_1 = 0$  \n- $t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}$\n\n::: notes\nWe're testing whether the slope is zero—i.e., whether $X$ is a useful predictor for $Y$.\n:::\n\n## Interpretation\n\n- If $p < \\alpha$, conclude $\\beta_1 \\ne 0$\n- If not, we can't say $X$ affects $Y$\n\n::: notes\nMake it clear that failing to reject $H_0$ doesn't prove it's true—just that we lack strong evidence against it.\n:::\n\n\n## Regression Example\n\n- $\\hat{\\beta}_1 = 2.5$, $SE = 0.8$, $n = 30$\n- 95% CI: $[0.86,\\ 4.14]$  \n- $t = 3.125$, $p < 0.01$\n\n::: notes\nHighlight how the CI and hypothesis test lead to the same conclusion: the slope is likely not zero.\n:::\n\n\n\n\n",
    "supporting": [
      "10_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}