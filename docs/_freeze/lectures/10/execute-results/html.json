{
  "hash": "41d20a8f9f998dc368389884e509e1bc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mathematical Models\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: notes/chalkboard_1a.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\neditor: source\n---\n\n\n\n\n\n\n\n\n\n# Motivation\n\n## Motivation\n\nThe `bacteria` data set contians information on whether bacteria (`y`: y or n) is present after utilizing treatments (`ap`: **a**ctive or **p**lacebo).\n\n::: fragment\nWe are interesting in determine the proportion of having bacteria present is different for those taking an \"active\" or \"placebo\".\n:::\n\n## Comparing Proportions\n\nWe are interesting in determining if different groups see different proportions of a binary outcome.\n\nWe compute the proportions of observing the binary outcome in Group 1 and Group 2 and see if they are fundamentally different from each other.\n\n## 2 by 2 Cross Tabulations\n\n|         |           |           |\n|---------|-----------|-----------|\n| Groups  | Outcome 1 | Outcome 2 |\n| Group 1 | $p_{11}$  | $p_{21}$  |\n| Group 2 | $p_{12}$  | $p_{22}$  |\n\n\n::: fragment\nWe want to compare $p_{11}$ and $p_{12}$, to determine if the probability of outcome 1 are the same for both groups.\n\n:::\n\n## Test Statistic\n\nWe can use both $p_{11}$ and $p_{12}$ to determine if there is a fundamental difference.\n\n::: fragment\nHowever, it will be more beneficial to utilize one statistic to contruct the sampling distribution.\n:::\n\n::: fragment\n$$\nT =  \\hat p_{11} - \\hat p_{12}\n$$\n:::\n\n## Obtain Proportions in R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nprops_df(DATA, GROUP, OUTCOME, VAL)\n```\n:::\n\n\n\n\n\n## Obtain Difference in R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nprops_df(DATA, GROUP, OUTCOME, VAL, diff = TRUE)\n```\n:::\n\n\n\n\n\n## Bacteria Example\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nprops_df(bacteria, ap, y, \"y\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    p1    p2 \n#> 0.750 0.875\n```\n\n\n:::\n:::\n\n\n\n\n\n## Bacteria Example\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nprops_df(bacteria, ap, y, \"y\", TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.125\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hypotheis Test\n\nWe will test the following hypothesis:\n\n$$\nH_0:\\ \\Delta =  p_1-p_2 = 0\n$$\n\n$$\nH_a:\\ \\Delta = p_1 - p_2 \\neq 0\n$$\n\n# Normal Distribution\n\n## Normal Distribution\n\nThe **Normal Distribution** is a probability distribution that is symmetric, with most of the data points clustering around the mean. \n\n- It’s **bell-shaped** and is defined mathematically by two parameters:\n  - **Mean ($\\mu$)**: The center or peak of the distribution.\n  - **Standard Deviation ($\\sigma$)**: Controls the spread of the distribution.\n\n\n## Normal Distribution\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- seq(-4, 4, length.out = 1000)\ndata.frame(x = x, y = dnorm(x)) |> \n  ggplot(aes(x, y)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](10_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Properties of the Normal Distribution\n\n::: incremental\n\n1. **Symmetry**: It is perfectly symmetric about the mean, meaning the left side is a mirror image of the right.\n2. **Unimodal**: There is a single peak at the mean.\n3. **Mean, Median, and Mode are Equal**: In a normal distribution, these three measures of central tendency are located at the same point.\n4. **68-95-99.7 Rule** (Empirical Rule)\n \n:::\n\n## Standard Normal Distribution\nThe **Standard Normal Distribution** is a special type of normal distribution with a mean of 0 and a standard deviation of 1. It's often used as a reference to convert any normal distribution to a standard form.\n\n## Z-Scores\nA **Z-score** (or standard score) tells us how many standard deviations an individual data point is from the mean. It’s calculated as: $Z = \\frac{X - \\mu}{\\sigma}$\n\n- If $Z$ is positive, the data point is above the mean.\n- If $Z$ is negative, the data point is below the mean.\n- Using Z-scores, we can compare values across different normal distributions or find the probability associated with a particular score.\n\n\n\n## Empirical Rule\n\nThe **Empirical Rule** provides a way to understand the spread of data in a normal distribution by describing how data points cluster around the mean. According to this rule:\n\n1. Approximately **68%** of data points fall within **one standard deviation** of the mean.\n2. Approximately **95%** of data points fall within **two standard deviations** of the mean.\n3. Approximately **99.7%** of data points fall within **three standard deviations** of the mean.\n\n\n\n## Empirical Rule and Normal Distribution\n\nIn a normal distribution:\n\n- **68% of data** lies between $(\\mu - \\sigma)\\) and \\((\\mu + \\sigma)$.\n\n- **95% of data** lies between $(\\mu - 2\\sigma)\\) and \\((\\mu + 2\\sigma)$.\n\n- **99.7% of data** lies between $(\\mu - 3\\sigma)\\) and \\((\\mu + 3\\sigma)$.\n\nThese intervals allow us to estimate probabilities for data within each range without needing to calculate exact probabilities.\n\n\n## Visualizing the Empirical Rule\n\n1. The **68% region** represents the middle of the curve, starting one standard deviation left of the mean and ending one standard deviation right.\n2. The **95% region** stretches further out, covering almost the entire curve except for the outer tails.\n3. The **99.7% region** includes nearly all data points, covering the entire curve except for a tiny fraction at each extreme.\n\n## Empirical Rule\n\n![](img/fig-er6895997-1.png)\n\n# Central Limit Theorem\n\n## Central Limit Theorem\nWhat does the Central Limit Theorem (CLT) actually tell us?\n\n::: fragment\n\nThe CLT states that:\n\n::: incremental\n\n1. **If you take a sufficiently large number of samples from any population, the distribution of the sample means will be approximately normal.**\n2. This approximation holds **no matter the shape of the original population distribution** (it could be normal, skewed, bimodal, etc.).\n3. The mean of this sampling distribution will be equal to the **population mean**.\n4. The standard deviation of this sampling distribution (often called the **standard error**) will be the population standard deviation divided by the square root of the sample size, \\( \\sigma_{\\text{sample mean}} = \\frac{\\sigma}{\\sqrt{n}} \\).\n\n:::\n\n:::\n\n\n## Why the Central Limit Theorem is Important\n\n\nThe CLT is powerful for several reasons:\n\n::: incremental\n\n1. **Predicting Sample Outcomes:** Since we know the distribution of sample means will be approximately normal, we can make predictions about future samples.\n2. **Confidence Intervals and Hypothesis Testing:** The normal distribution of sample means allows us to estimate population parameters (like the mean) with confidence and test hypotheses even if the population itself isn’t normally distributed.\n3. **Practical Applications in Various Fields:** From quality control in manufacturing to political polling and medicine, the CLT lets us make inferences about population characteristics based on sample data.\n\n:::\n\n# Inference\n\n## Using Mathematical Models\n\nThe inferential procedures, such as computing the p-value or confidence interval, can be constructed with mathematical models.\n\n## Advantages\n\n::: incremental\n\n-   Mathematical Models have been widely studied and implemented in several software packages.\n-   These are not computationally intensive\n-   Can work with smaller data set and provide more reliable results\n\n:::\n\n## Disadvantages\n\n::: incremental\n-   If the mathematical model is wrong, results may be invalid.\n-   Difficult to know what are the true mathematical models.\n:::\n\n# Inference in R\n\n## Inference in R\n\n$$\nH_0:\\ \\Delta =  p_1-p_2 = 0\n$$\n\n$$\nH_a:\\ \\Delta = p_1 - p_2 \\neq 0\n$$\n\n## Proportions Test\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nprop.test(x = c(CAT_1, CAT_2), n = c(N_1, N_2))\n```\n:::\n\n\n\n\n\n\n## Descriptives\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncat_stats(bacteria$ap, bacteria$y)\n```\n:::\n\n\n\n\n\n## Proportions Test in R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nprop.test(x = c(93, 84), n = c(124, 96))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \t2-sample test for equality of proportions with continuity correction\n#> \n#> data:  c(93, 84) out of c(124, 96)\n#> X-squared = 4.6109, df = 1, p-value = 0.03177\n#> alternative hypothesis: two.sided\n#> 95 percent confidence interval:\n#>  -0.23516294 -0.01483706\n#> sample estimates:\n#> prop 1 prop 2 \n#>  0.750  0.875\n```\n\n\n:::\n:::\n",
    "supporting": [
      "10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}